{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ByungjunKim/WikisourceParsing/blob/main/wikisource_tutorial_colab_tested.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_header"
      },
      "source": [
        "# ğŸ“š ìœ„í‚¤ë¬¸í—Œ íŒŒì„œ íŠœí† ë¦¬ì–¼\n",
        "\n",
        "**í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œì—ì„œ ì™„ì „í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ì„¸ìš”!**\n",
        "\n",
        "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
        "- ìœ„í‚¤ë¬¸í—Œ XML ë¤í”„ì—ì„œ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œí•˜ê¸°\n",
        "- APIë¥¼ í™œìš©í•´ ëˆ„ë½ëœ ë¶„ë¥˜ ì •ë³´ ë³´ê°•í•˜ê¸°\n",
        "- ìœ„í‚¤ë°ì´í„°ì™€ ì—°ë™í•´ ì—°ë„ ì •ë³´ ì¶”ì¶œí•˜ê¸°\n",
        "- ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¹”ë”í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±í•˜ê¸°\n",
        "\n",
        "## ğŸ“– ì˜ˆì œ: ì• êµ­ê°€ ì™„ì „ ë¶„ì„\n",
        "- **ë¬¸ì œ**: XML ë¤í”„ì—ì„œëŠ” 2ê°œ ë¶„ë¥˜ë§Œ ë‚˜ì˜¤ëŠ”ë°, ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ëŠ” 4ê°œê°€ ìˆìŒ\n",
        "- **í•´ê²°**: API ë³´ê°•ìœ¼ë¡œ '1941ë…„ ì‘í’ˆ', 'PD-old-50' ë“± ëˆ„ë½ëœ ë¶„ë¥˜ê¹Œì§€ ì™„ì „ ì¶”ì¶œ\n",
        "- **ê²°ê³¼**: ë¶„ë¥˜ì—ì„œ ì—°ë„ ì •ë³´(1941ë…„)ê¹Œì§€ ì •í™•í•˜ê²Œ ì¶”ì¶œ ì„±ê³µ!\n",
        "\n",
        "---\n",
        "**âœ… ë…¸íŠ¸ë¶ í…ŒìŠ¤íŠ¸ ì™„ë£Œ**: 2025-09-11 10:21:57\n",
        "- ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì„±ê³µ\n",
        "- API í•¨ìˆ˜ë“¤ ì •ìƒ ì‘ë™\n",
        "- ë¤í”„ íŒŒì¼ í™•ì¸ ì™„ë£Œ (147.8MB)\n",
        "- ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ ì¤€ë¹„ (11ê°œ ì½”ì–´)\n",
        "\n",
        "**ğŸš€ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## ğŸ”§ í™˜ê²½ ì„¤ì •\n",
        "\n",
        "ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ì„í¬íŠ¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_libraries"
      },
      "outputs": [],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install mwxml requests tqdm pandas\n",
        "\n",
        "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import mwxml\n",
        "import bz2\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(f\"ğŸ–¥ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´: {mp.cpu_count()}ê°œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "\n",
        "í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œ ë¤í”„ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_dump"
      },
      "outputs": [],
      "source": [
        "# ìœ„í‚¤ë¬¸í—Œ ë¤í”„ ë‹¤ìš´ë¡œë“œ\n",
        "dump_url = \"https://dumps.wikimedia.org/kowikisource/20250901/kowikisource-20250901-pages-articles.xml.bz2\"\n",
        "dump_file = \"kowikisource-20250901-pages-articles.xml.bz2\"\n",
        "\n",
        "if not os.path.exists(dump_file):\n",
        "    print(\"ğŸ“¥ ìœ„í‚¤ë¬¸í—Œ ë¤í”„ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 150MB, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
        "    !wget -O {dump_file} {dump_url}\n",
        "    print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"âœ… ë¤í”„ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤!\")\n",
        "\n",
        "# íŒŒì¼ í¬ê¸° í™•ì¸\n",
        "file_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "print(f\"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_functions_section"
      },
      "source": [
        "## ğŸŒ API ë³´ê°• í•¨ìˆ˜ë“¤\n",
        "\n",
        "ìœ„í‚¤ë¬¸í—Œ APIì™€ ìœ„í‚¤ë°ì´í„° APIë¥¼ ì‚¬ìš©í•´ì„œ ëˆ„ë½ëœ ì •ë³´ë¥¼ ë³´ê°•í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_functions"
      },
      "outputs": [],
      "source": [
        "def get_categories_from_api(page_title):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤ë¬¸í—Œ APIì—ì„œ í˜ì´ì§€ì˜ ëª¨ë“  ë¶„ë¥˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤\n",
        "\n",
        "    ì´ í•¨ìˆ˜ê°€ ì¤‘ìš”í•œ ì´ìœ :\n",
        "    - XML ë¤í”„ì—ëŠ” ê¸°ë³¸ ë¶„ë¥˜ë§Œ ìˆìŒ\n",
        "    - í…œí”Œë¦¿ì—ì„œ ìë™ ìƒì„±ë˜ëŠ” ë¶„ë¥˜ëŠ” APIë¡œë§Œ í™•ì¸ ê°€ëŠ¥\n",
        "    - ì˜ˆ: '1941ë…„ ì‘í’ˆ', 'PD-old-50' ë“±\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'categories',\n",
        "            'cllimit': 'max'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return []\n",
        "\n",
        "        categories = pages[page_id].get('categories', [])\n",
        "        category_names = []\n",
        "\n",
        "        for cat in categories:\n",
        "            cat_title = cat['title']\n",
        "            if cat_title.startswith('ë¶„ë¥˜:'):\n",
        "                category_names.append(cat_title[3:])  # 'ë¶„ë¥˜:' ì œê±°\n",
        "\n",
        "        return category_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"API ë¶„ë¥˜ ì¡°íšŒ ì˜¤ë¥˜ ({page_title}): {e}\")\n",
        "        return []\n",
        "\n",
        "def get_year_from_wikidata(page_title):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤ë°ì´í„°ì—ì„œ ì‘í’ˆì˜ ë°œí‘œ ì—°ë„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤\n",
        "\n",
        "    ìœ„í‚¤ë°ì´í„° ì—°ë™ì˜ ì¥ì :\n",
        "    - êµ¬ì¡°í™”ëœ ì—°ë„ ì •ë³´ ì œê³µ\n",
        "    - ì—¬ëŸ¬ ì–¸ì–´íŒì—ì„œ ê³µìœ ë˜ëŠ” ì •í™•í•œ ë°ì´í„°\n",
        "    - ë¶„ë¥˜ ì •ë³´ì™€ êµì°¨ ê²€ì¦ ê°€ëŠ¥\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1ë‹¨ê³„: ìœ„í‚¤ë¬¸í—Œ í˜ì´ì§€ì—ì„œ ìœ„í‚¤ë°ì´í„° ID ê°€ì ¸ì˜¤ê¸°\n",
        "        wikisource_api = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'pageprops'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(wikisource_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return None\n",
        "\n",
        "        wikidata_id = pages[page_id].get('pageprops', {}).get('wikibase_item')\n",
        "\n",
        "        if not wikidata_id:\n",
        "            return None\n",
        "\n",
        "        # 2ë‹¨ê³„: ìœ„í‚¤ë°ì´í„°ì—ì„œ ë°œí‘œì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "        wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'format': 'json',\n",
        "            'ids': wikidata_id,\n",
        "            'props': 'claims'\n",
        "        }\n",
        "\n",
        "        response = requests.get(wikidata_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        entity = data.get('entities', {}).get(wikidata_id, {})\n",
        "        claims = entity.get('claims', {})\n",
        "\n",
        "        # ë°œí‘œ ê´€ë ¨ ì†ì„±ë“¤ í™•ì¸\n",
        "        date_properties = ['P577', 'P571', 'P585']  # ë°œí‘œì¼, ì‹œì‘ì¼, íŠ¹ì •ì‹œì \n",
        "\n",
        "        for prop in date_properties:\n",
        "            if prop in claims:\n",
        "                for claim in claims[prop]:\n",
        "                    try:\n",
        "                        time_value = claim['mainsnak']['datavalue']['value']['time']\n",
        "                        # +1941-00-00T00:00:00Z í˜•íƒœì—ì„œ ì—°ë„ ì¶”ì¶œ\n",
        "                        year = int(time_value[1:5])\n",
        "                        if 1800 <= year <= 2030:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„\n",
        "                            return year\n",
        "                    except (KeyError, ValueError):\n",
        "                        continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ìœ„í‚¤ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({page_title}): {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… API ë³´ê°• í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parsing_functions_section"
      },
      "source": [
        "## ğŸ” í…ìŠ¤íŠ¸ íŒŒì‹± í•¨ìˆ˜ë“¤\n",
        "\n",
        "ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ê¹”ë”í•œ ë³¸ë¬¸ì„ ë§Œë“œëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "parsing_functions"
      },
      "outputs": [],
      "source": [
        "def extract_metadata(text):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
        "\n",
        "    ì¶”ì¶œí•˜ëŠ” ì •ë³´:\n",
        "    - ì €ì: [[ì €ì:ì´ë¦„]] íŒ¨í„´ì—ì„œ\n",
        "    - ë¶„ë¥˜: [[ë¶„ë¥˜:ì´ë¦„]] íŒ¨í„´ì—ì„œ\n",
        "    - ì‘ê³¡ê°€: 'ì‘ê³¡' í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ\n",
        "    - ë¼ì´ì„ ìŠ¤: {{PD-*}} í…œí”Œë¦¿ì—ì„œ\n",
        "    - ì–¸ì–´: 'í•œì', 'í•œê¸€' í‚¤ì›Œë“œì—ì„œ\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            'authors': [],\n",
        "            'categories': [],\n",
        "            'composer': None,\n",
        "            'translator': None,\n",
        "            'year': None,\n",
        "            'license': None,\n",
        "            'language': None\n",
        "        }\n",
        "\n",
        "    # ì €ì ì •ë³´ ì¶”ì¶œ\n",
        "    authors = []\n",
        "    author_patterns = [\n",
        "        r'\\[\\[ì €ì:([^|\\]]+)',  # [[ì €ì:ì´ë¦„]] ë˜ëŠ” [[ì €ì:ì´ë¦„|í‘œì‹œëª…]]\n",
        "        r'\\|\\s*author\\s*=\\s*\\[\\[ì €ì:([^|\\]]+)',  # author= ë§¤ê°œë³€ìˆ˜\n",
        "    ]\n",
        "\n",
        "    for pattern in author_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        authors.extend(matches)\n",
        "\n",
        "    # ë¶„ë¥˜ ì •ë³´ ì¶”ì¶œ\n",
        "    categories = re.findall(r'\\[\\[ë¶„ë¥˜:([^\\]]+)\\]\\]', text)\n",
        "\n",
        "    # ì‘ê³¡ê°€ ì •ë³´ ì¶”ì¶œ\n",
        "    composer = None\n",
        "    composer_patterns = [\n",
        "        r'([^.]+)\\s*ì‘ê³¡',\n",
        "        r'ì‘ê³¡ê°€?\\s*[:=]\\s*([^.\\n]+)'\n",
        "    ]\n",
        "    for pattern in composer_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            composer = matches[0].strip()\n",
        "            break\n",
        "\n",
        "    # ì—°ë„ ì •ë³´ ì¶”ì¶œ\n",
        "    year_matches = re.findall(r'(\\d{4})ë…„', text)\n",
        "    year = None\n",
        "    if year_matches:\n",
        "        year = max(set(year_matches), key=year_matches.count)\n",
        "\n",
        "    # ë¼ì´ì„ ìŠ¤ ì •ë³´ ì¶”ì¶œ\n",
        "    license_info = None\n",
        "    license_patterns = [\n",
        "        r'\\{\\{(PD-[^}]+)\\}\\}',\n",
        "        r'\\{\\{(CC-[^}]+)\\}\\}'\n",
        "    ]\n",
        "    for pattern in license_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            license_info = matches[0]\n",
        "            break\n",
        "\n",
        "    # ì–¸ì–´ ì •ë³´ ì¶”ì¶œ\n",
        "    language = None\n",
        "    if 'í•œì' in text and 'í•œê¸€' in text:\n",
        "        language = 'í•œì+í•œê¸€'\n",
        "    elif 'í•œì' in text:\n",
        "        language = 'í•œì'\n",
        "    elif 'í•œê¸€' in text:\n",
        "        language = 'í•œê¸€'\n",
        "\n",
        "    # ì¤‘ë³µ ì œê±°\n",
        "    authors = list(set([a.strip() for a in authors if a.strip()]))\n",
        "    categories = list(set([c.strip() for c in categories if c.strip()]))\n",
        "\n",
        "    return {\n",
        "        'authors': authors,\n",
        "        'categories': categories,\n",
        "        'composer': composer,\n",
        "        'translator': None,  # ë‚˜ì¤‘ì— í™•ì¥ ê°€ëŠ¥\n",
        "        'year': year,\n",
        "        'license': license_info,\n",
        "        'language': language\n",
        "    }\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë§ˆí¬ì—…ì„ ì œê±°í•˜ê³  ê¹”ë”í•œ ë³¸ë¬¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
        "\n",
        "    ì œê±°í•˜ëŠ” ê²ƒë“¤:\n",
        "    - í…œí”Œë¦¿: {{...}}\n",
        "    - HTML íƒœê·¸: <div>, <br> ë“±\n",
        "    - ìœ„í‚¤ ë§í¬: [[...]]\n",
        "    - ë§ˆí¬ì—…: ''', ''\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # í…œí”Œë¦¿ ì œê±°\n",
        "    cleaned = re.sub(r'\\{\\{[^{}]*\\}\\}', '', text)\n",
        "\n",
        "    # ë§í¬ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
        "    cleaned = re.sub(r'\\[\\[[^|\\]]*\\|([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "\n",
        "    # HTML íƒœê·¸ ì œê±°\n",
        "    cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
        "\n",
        "    # ìœ„í‚¤ ë§ˆí¬ì—… ì œê±°\n",
        "    cleaned = re.sub(r\"'''([^']+)'''\", r'\\1', cleaned)  # êµµì€ ê¸€ì”¨\n",
        "    cleaned = re.sub(r\"''([^']+)''\", r'\\1', cleaned)   # ê¸°ìš¸ì„\n",
        "\n",
        "    # ì„¹ì…˜ í—¤ë” ì œê±°\n",
        "    cleaned = re.sub(r'^=+\\s*([^=]+)\\s*=+$', r'\\1', cleaned, flags=re.MULTILINE)\n",
        "\n",
        "    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_urls(title, authors):\n",
        "    \"\"\"\n",
        "    í˜ì´ì§€ì™€ ì €ìì˜ URLì„ ìƒì„±í•©ë‹ˆë‹¤\n",
        "    \"\"\"\n",
        "    base_url = \"https://ko.wikisource.org/wiki/\"\n",
        "\n",
        "    # í˜ì´ì§€ URL\n",
        "    page_url = base_url + urllib.parse.quote(title.replace(' ', '_'))\n",
        "\n",
        "    # ì €ì URLë“¤\n",
        "    author_links = []\n",
        "    for author in authors:\n",
        "        author_url = base_url + urllib.parse.quote(f\"ì €ì:{author}\".replace(' ', '_'))\n",
        "        author_links.append({\n",
        "            'name': author,\n",
        "            'url': author_url\n",
        "        })\n",
        "\n",
        "    return page_url, author_links\n",
        "\n",
        "print(\"âœ… í…ìŠ¤íŠ¸ íŒŒì‹± í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhancement_section"
      },
      "source": [
        "## ğŸš€ API ë³´ê°• í•¨ìˆ˜\n",
        "\n",
        "ë¤í”„ì—ì„œ íŒŒì‹±í•œ ë°ì´í„°ë¥¼ APIë¡œ ë³´ê°•í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enhancement_function"
      },
      "outputs": [],
      "source": [
        "def enhance_with_api(page_data):\n",
        "    \"\"\"\n",
        "    í˜ì´ì§€ ë°ì´í„°ë¥¼ API ì •ë³´ë¡œ ë³´ê°•í•©ë‹ˆë‹¤\n",
        "\n",
        "    ë³´ê°• ê³¼ì •:\n",
        "    1. APIì—ì„œ ì™„ì „í•œ ë¶„ë¥˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    2. ìœ„í‚¤ë°ì´í„°ì—ì„œ ì—°ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    3. ë¶„ë¥˜ì—ì„œ ì—°ë„ ì¶”ì¶œí•˜ê¸°\n",
        "    4. ìµœì¢… ì—°ë„ ê²°ì • (ë¶„ë¥˜ > ìœ„í‚¤ë°ì´í„° > ë¤í”„)\n",
        "    \"\"\"\n",
        "    title = page_data['title']\n",
        "\n",
        "    # APIì—ì„œ ì™„ì „í•œ ë¶„ë¥˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    api_categories = get_categories_from_api(title)\n",
        "\n",
        "    # ìœ„í‚¤ë°ì´í„°ì—ì„œ ì—°ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    wikidata_year = get_year_from_wikidata(title)\n",
        "\n",
        "    # ê¸°ì¡´ ë°ì´í„° ë³µì‚¬\n",
        "    enhanced = page_data.copy()\n",
        "\n",
        "    # ë¶„ë¥˜ ì •ë³´ ë³‘í•© (ì¤‘ë³µ ì œê±°)\n",
        "    all_categories = list(set(page_data.get('categories', []) + api_categories))\n",
        "    enhanced['categories'] = all_categories\n",
        "    enhanced['api_categories'] = api_categories\n",
        "\n",
        "    # ë¶„ë¥˜ì—ì„œ ì—°ë„ ì¶”ì¶œ\n",
        "    year_from_categories = None\n",
        "    year_categories = [cat for cat in all_categories if 'ë…„' in cat and 'ì‘í’ˆ' in cat]\n",
        "    if year_categories:\n",
        "        for cat in year_categories:\n",
        "            year_match = re.search(r'(\\d{4})ë…„', cat)\n",
        "            if year_match:\n",
        "                year_from_categories = int(year_match.group(1))\n",
        "                break\n",
        "\n",
        "    # ìµœì¢… ì—°ë„ ê²°ì • (ìš°ì„ ìˆœìœ„: ë¶„ë¥˜ > ìœ„í‚¤ë°ì´í„° > ë¤í”„)\n",
        "    enhanced['year'] = (\n",
        "        year_from_categories or\n",
        "        wikidata_year or\n",
        "        page_data.get('year')\n",
        "    )\n",
        "\n",
        "    # ë³´ê°• ì •ë³´ ì¶”ê°€\n",
        "    enhanced['year_from_categories'] = year_from_categories\n",
        "    enhanced['year_from_wikidata'] = wikidata_year\n",
        "\n",
        "    return enhanced\n",
        "\n",
        "print(\"âœ… API ë³´ê°• í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_parser_section"
      },
      "source": [
        "## âš¡ ë©”ì¸ íŒŒì„œ (ë©€í‹°í”„ë¡œì„¸ì‹±)\n",
        "\n",
        "ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•´ì„œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ íŒŒì„œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_parser"
      },
      "outputs": [],
      "source": [
        "def process_pages_batch(pages_batch):\n",
        "    \"\"\"\n",
        "    í˜ì´ì§€ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜\n",
        "    (ë©€í‹°í”„ë¡œì„¸ì‹±ì—ì„œ ê° ì½”ì–´ê°€ ì‹¤í–‰)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for page_data in pages_batch:\n",
        "        try:\n",
        "            page_id, title, namespace, redirect, revisions = page_data\n",
        "\n",
        "            if not revisions:\n",
        "                continue\n",
        "\n",
        "            # ìµœì‹  ë¦¬ë¹„ì „ ì‚¬ìš©\n",
        "            revision = revisions[0]\n",
        "            revision_id, timestamp, username, comment, text, size = revision\n",
        "\n",
        "            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
        "            metadata = extract_metadata(text)\n",
        "\n",
        "            # ë³¸ë¬¸ ì •ë¦¬\n",
        "            clean_content = clean_text(text)\n",
        "\n",
        "            # URL ìƒì„±\n",
        "            page_url, author_links = generate_urls(title, metadata['authors'])\n",
        "\n",
        "            # í˜ì´ì§€ ë°ì´í„° êµ¬ì„±\n",
        "            page_result = {\n",
        "                'page_id': page_id,\n",
        "                'title': title,\n",
        "                'url': page_url,\n",
        "                'namespace': namespace,\n",
        "                'redirect': redirect,\n",
        "\n",
        "                # í•„ìˆ˜ ë©”íƒ€ë°ì´í„°\n",
        "                'authors': metadata['authors'],\n",
        "                'author_links': author_links,\n",
        "                'categories': metadata['categories'],\n",
        "                'content': clean_content,\n",
        "                'raw_content': text,\n",
        "\n",
        "                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°\n",
        "                'composer': metadata['composer'],\n",
        "                'translator': metadata['translator'],\n",
        "                'year': metadata['year'],\n",
        "                'license': metadata['license'],\n",
        "                'language': metadata['language'],\n",
        "\n",
        "                # ë¦¬ë¹„ì „ ì •ë³´\n",
        "                'revision_id': revision_id,\n",
        "                'last_modified': str(timestamp) if timestamp else None,\n",
        "                'last_contributor': username,\n",
        "                'size': len(text) if text else 0,\n",
        "                'content_size': len(clean_content)\n",
        "            }\n",
        "\n",
        "            results.append(page_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def parse_wikisource(dump_file, limit=None, enable_api=False, batch_size=50):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤ë¬¸í—Œ ë¤í”„ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤\n",
        "\n",
        "    Args:\n",
        "        dump_file: ë¤í”„ íŒŒì¼ ê²½ë¡œ\n",
        "        limit: ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)\n",
        "        enable_api: API ë³´ê°• ì‚¬ìš© ì—¬ë¶€\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "\n",
        "    Returns:\n",
        "        list: íŒŒì‹±ëœ í˜ì´ì§€ ë°ì´í„°\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # CPU ì½”ì–´ ìˆ˜ (Colabì—ì„œ ìµœëŒ€ í™œìš©)\n",
        "    max_workers = mp.cpu_count()\n",
        "\n",
        "    print(f\"ğŸš€ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\")\n",
        "    print(f\"  ğŸ“Š CPU ì½”ì–´: {max_workers}ê°œ (ìµœëŒ€ í™œìš©)\")\n",
        "    print(f\"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "    print(f\"  ğŸŒ API ë³´ê°•: {'ì‚¬ìš©' if enable_api else 'ì‚¬ìš© ì•ˆí•¨'}\")\n",
        "\n",
        "    # 1ë‹¨ê³„: ë¤í”„ì—ì„œ ë°ì´í„° ìˆ˜ì§‘\n",
        "    print(\"\\nğŸ“¥ 1ë‹¨ê³„: ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\")\n",
        "    pages_batches = []\n",
        "    current_batch = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"í˜ì´ì§€ ìˆ˜ì§‘\"):\n",
        "            if limit and total_pages >= limit:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # ë¦¬ë¹„ì „ ë°ì´í„° ìˆ˜ì§‘\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break  # ìµœì‹  ë¦¬ë¹„ì „ë§Œ\n",
        "\n",
        "                page_data = (\n",
        "                    page.id,\n",
        "                    page.title,\n",
        "                    page.namespace,\n",
        "                    str(page.redirect.title) if page.redirect else None,\n",
        "                    revisions\n",
        "                )\n",
        "                current_batch.append(page_data)\n",
        "\n",
        "                if len(current_batch) >= batch_size:\n",
        "                    pages_batches.append(current_batch)\n",
        "                    current_batch = []\n",
        "\n",
        "                total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if current_batch:\n",
        "            pages_batches.append(current_batch)\n",
        "\n",
        "    print(f\"âœ… {total_pages}ê°œ í˜ì´ì§€ë¥¼ {len(pages_batches)}ê°œ ë°°ì¹˜ë¡œ ìˆ˜ì§‘\")\n",
        "\n",
        "    # 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬\n",
        "    print(\"\\nâš¡ 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬\")\n",
        "    results = []\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in pages_batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"ë°°ì¹˜ ì²˜ë¦¬\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                results.extend(batch_results)\n",
        "            except Exception as e:\n",
        "                print(f\"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "    # 3ë‹¨ê³„: API ë³´ê°• (ì„ íƒì )\n",
        "    if enable_api and results:\n",
        "        print(\"\\nğŸŒ 3ë‹¨ê³„: API ë³´ê°• ì²˜ë¦¬\")\n",
        "        enhanced_results = []\n",
        "\n",
        "        for page_data in tqdm(results, desc=\"API ë³´ê°•\"):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "                time.sleep(0.1)  # API ì œí•œ ê³ ë ¤\n",
        "            except Exception as e:\n",
        "                print(f\"API ë³´ê°• ì˜¤ë¥˜ ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        results = enhanced_results\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nğŸ‰ íŒŒì‹± ì™„ë£Œ!\")\n",
        "    print(f\"  â±ï¸  ì´ ì‹œê°„: {total_time:.1f}ì´ˆ\")\n",
        "    print(f\"  ğŸ“Š ì²˜ë¦¬ ì†ë„: {len(results)/total_time:.1f} í˜ì´ì§€/ì´ˆ\")\n",
        "    print(f\"  ğŸ“„ ì´ í˜ì´ì§€: {len(results)}ê°œ\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… ë©”ì¸ íŒŒì„œê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_section"
      },
      "source": [
        "## ğŸ¯ ì‹¤ìŠµ 1: ì• êµ­ê°€ ë¶„ì„ (ê¸°ë³¸ íŒŒì‹±)\n",
        "\n",
        "ë¨¼ì € XML ë¤í”„ë§Œìœ¼ë¡œ ì• êµ­ê°€ë¥¼ íŒŒì‹±í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo_basic"
      },
      "outputs": [],
      "source": [
        "# ì• êµ­ê°€ë§Œ íŒŒì‹± (API ë³´ê°• ì—†ìŒ)\n",
        "print(\"ğŸ“š ì‹¤ìŠµ 1: ê¸°ë³¸ XML ë¤í”„ íŒŒì‹±\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "basic_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=1,  # ì²« ë²ˆì§¸ í˜ì´ì§€ (ì• êµ­ê°€)ë§Œ\n",
        "    enable_api=False,  # API ë³´ê°• ì•ˆí•¨\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "if basic_results:\n",
        "    aegukga_basic = basic_results[0]\n",
        "\n",
        "    print(f\"\\nğŸ“„ ì œëª©: {aegukga_basic['title']}\")\n",
        "    print(f\"ğŸ‘¤ ì €ì: {aegukga_basic['authors']}\")\n",
        "    print(f\"ğŸ“‚ ë¶„ë¥˜: {aegukga_basic['categories']}\")\n",
        "    print(f\"ğŸµ ì‘ê³¡ê°€: {aegukga_basic.get('composer', 'N/A')}\")\n",
        "    print(f\"ğŸ“… ì—°ë„: {aegukga_basic.get('year', 'N/A')}\")\n",
        "    print(f\"ğŸ“œ ë¼ì´ì„ ìŠ¤: {aegukga_basic.get('license', 'N/A')}\")\n",
        "    print(f\"ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {aegukga_basic['content_size']} ë¬¸ì\")\n",
        "\n",
        "    print(f\"\\nğŸ“– ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "    print(f\"{aegukga_basic['content'][:200]}...\")\n",
        "\n",
        "    print(f\"\\nğŸ” ë¬¸ì œì  ë°œê²¬:\")\n",
        "    print(f\"  âŒ ë¶„ë¥˜ê°€ {len(aegukga_basic['categories'])}ê°œë§Œ ë‚˜ì˜´ (ì‹¤ì œë¡œëŠ” 4ê°œ)\")\n",
        "    print(f\"  âŒ '1941ë…„ ì‘í’ˆ' ë¶„ë¥˜ê°€ ëˆ„ë½ë¨\")\n",
        "    print(f\"  âŒ 'PD-old-50' ë¶„ë¥˜ê°€ ëˆ„ë½ë¨\")\n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_api_section"
      },
      "source": [
        "## ğŸŒŸ ì‹¤ìŠµ 2: ì• êµ­ê°€ ë¶„ì„ (API ë³´ê°•)\n",
        "\n",
        "ì´ì œ API ë³´ê°•ì„ ì‚¬ìš©í•´ì„œ ì™„ì „í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "demo_api"
      },
      "outputs": [],
      "source": [
        "# ì• êµ­ê°€ API ë³´ê°• íŒŒì‹±\n",
        "print(\"ğŸŒŸ ì‹¤ìŠµ 2: API ë³´ê°• íŒŒì‹±\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "api_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=1,  # ì²« ë²ˆì§¸ í˜ì´ì§€ (ì• êµ­ê°€)ë§Œ\n",
        "    enable_api=True,  # API ë³´ê°• ì‚¬ìš©!\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "if api_results:\n",
        "    aegukga_api = api_results[0]\n",
        "\n",
        "    print(f\"\\nğŸ“„ ì œëª©: {aegukga_api['title']}\")\n",
        "    print(f\"ğŸ‘¤ ì €ì: {aegukga_api['authors']}\")\n",
        "\n",
        "    # ë¶„ë¥˜ ë¹„êµ\n",
        "    dump_cats = [cat for cat in aegukga_api['categories'] if cat not in aegukga_api.get('api_categories', [])]\n",
        "    api_cats = aegukga_api.get('api_categories', [])\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ë¶„ë¥˜ ì •ë³´:\")\n",
        "    print(f\"  ë¤í”„ì—ì„œ: {dump_cats}\")\n",
        "    print(f\"  APIì—ì„œ: {api_cats}\")\n",
        "    print(f\"  ì „ì²´: {aegukga_api['categories']}\")\n",
        "\n",
        "    print(f\"\\nğŸµ ì‘ê³¡ê°€: {aegukga_api.get('composer', 'N/A')}\")\n",
        "\n",
        "    # ì—°ë„ ì •ë³´ ìƒì„¸\n",
        "    print(f\"\\nğŸ“… ì—°ë„ ì •ë³´:\")\n",
        "    print(f\"  ìµœì¢…: {aegukga_api.get('year', 'N/A')}\")\n",
        "    print(f\"  ë¶„ë¥˜ì—ì„œ: {aegukga_api.get('year_from_categories', 'N/A')}\")\n",
        "    print(f\"  ìœ„í‚¤ë°ì´í„°ì—ì„œ: {aegukga_api.get('year_from_wikidata', 'N/A')}\")\n",
        "\n",
        "    print(f\"\\nğŸ“œ ë¼ì´ì„ ìŠ¤: {aegukga_api.get('license', 'N/A')}\")\n",
        "    print(f\"ğŸŒ ì–¸ì–´: {aegukga_api.get('language', 'N/A')}\")\n",
        "    print(f\"ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {aegukga_api['content_size']} ë¬¸ì\")\n",
        "\n",
        "    # ì„±ê³µ í™•ì¸\n",
        "    success_checks = [\n",
        "        ('âœ…' if '1941ë…„ ì‘í’ˆ' in api_cats else 'âŒ', '1941ë…„ ì‘í’ˆ ë¶„ë¥˜'),\n",
        "        ('âœ…' if 'PD-old-50' in api_cats else 'âŒ', 'PD-old-50 ë¶„ë¥˜'),\n",
        "        ('âœ…' if aegukga_api.get('year') == 1941 else 'âŒ', '1941ë…„ ì—°ë„ ì¶”ì¶œ'),\n",
        "        ('âœ…' if len(api_cats) >= 4 else 'âŒ', '4ê°œ ë¶„ë¥˜ ì™„ì „ ì¶”ì¶œ')\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ¯ ëª©í‘œ ë‹¬ì„±:\")\n",
        "    for status, description in success_checks:\n",
        "        print(f\"  {status} {description}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison_section"
      },
      "source": [
        "## ğŸ“Š ë¹„êµ ë¶„ì„\n",
        "\n",
        "ê¸°ë³¸ íŒŒì‹±ê³¼ API ë³´ê°• íŒŒì‹±ì˜ ì°¨ì´ì ì„ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comparison"
      },
      "outputs": [],
      "source": [
        "print(\"ğŸ“Š ê¸°ë³¸ íŒŒì‹± vs API ë³´ê°• íŒŒì‹± ë¹„êµ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if basic_results and api_results:\n",
        "    basic = basic_results[0]\n",
        "    enhanced = api_results[0]\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ë¶„ë¥˜ ì •ë³´:\")\n",
        "    print(f\"  ê¸°ë³¸ íŒŒì‹±: {len(basic['categories'])}ê°œ â†’ {basic['categories']}\")\n",
        "    print(f\"  API ë³´ê°•: {len(enhanced['categories'])}ê°œ â†’ {enhanced['categories']}\")\n",
        "    print(f\"  ê°œì„ : +{len(enhanced['categories']) - len(basic['categories'])}ê°œ ë¶„ë¥˜ ì¶”ê°€\")\n",
        "\n",
        "    print(f\"\\nğŸ“… ì—°ë„ ì •ë³´:\")\n",
        "    print(f\"  ê¸°ë³¸ íŒŒì‹±: {basic.get('year', 'None')}\")\n",
        "    print(f\"  API ë³´ê°•: {enhanced.get('year', 'None')} (ë¶„ë¥˜: {enhanced.get('year_from_categories')}, ìœ„í‚¤ë°ì´í„°: {enhanced.get('year_from_wikidata')})\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ í•µì‹¬ ì„±ê³¼:\")\n",
        "    missing_categories = set(enhanced['categories']) - set(basic['categories'])\n",
        "    if missing_categories:\n",
        "        print(f\"  âœ… ì¶”ê°€ëœ ë¶„ë¥˜: {list(missing_categories)}\")\n",
        "\n",
        "    if enhanced.get('year') and not basic.get('year'):\n",
        "        print(f\"  âœ… ì—°ë„ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {enhanced.get('year')}ë…„\")\n",
        "\n",
        "    if enhanced.get('year_from_categories') == enhanced.get('year_from_wikidata'):\n",
        "        print(f\"  âœ… ë¶„ë¥˜ì™€ ìœ„í‚¤ë°ì´í„° ì—°ë„ ì¼ì¹˜: {enhanced.get('year')}ë…„\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:\")\n",
        "print(f\"  1. XML ë¤í”„ë§Œìœ¼ë¡œëŠ” í…œí”Œë¦¿ ê¸°ë°˜ ë¶„ë¥˜ë¥¼ ë†“ì¹  ìˆ˜ ìˆìŒ\")\n",
        "print(f\"  2. API ë³´ê°•ìœ¼ë¡œ ì›¹ì‚¬ì´íŠ¸ì™€ ë™ì¼í•œ ì™„ì „í•œ ì •ë³´ ì¶”ì¶œ ê°€ëŠ¥\")\n",
        "print(f\"  3. ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì—°ë„ ì •ë³´ë¥¼ êµì°¨ ê²€ì¦í•˜ì—¬ ì •í™•ì„± í–¥ìƒ\")\n",
        "print(f\"  4. êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„°ë¡œ í›„ì† ë¶„ì„ ì‘ì—… ìš©ì´\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bulk_processing_section"
      },
      "source": [
        "## ğŸ”„ ì‹¤ìŠµ 3: ëŒ€ëŸ‰ ì²˜ë¦¬ (ì„ íƒì )\n",
        "\n",
        "ë” ë§ì€ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•´ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bulk_processing"
      },
      "outputs": [],
      "source": [
        "# ğŸš€ ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰\n",
        "print(\"ğŸš€ ì‹¤ìŠµ 3: ëŒ€ëŸ‰ ì²˜ë¦¬\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ì¡°ì •)\n",
        "BULK_LIMIT = 50  # 50ê°œ í˜ì´ì§€ ì²˜ë¦¬\n",
        "API_ENHANCEMENT = True  # API ë³´ê°• ì‚¬ìš© ì—¬ë¶€\n",
        "\n",
        "print(f\"ğŸ“Š ì„¤ì •:\")\n",
        "print(f\"  ì²˜ë¦¬ í˜ì´ì§€: {BULK_LIMIT}ê°œ\")\n",
        "print(f\"  API ë³´ê°•: {'ì‚¬ìš©' if API_ENHANCEMENT else 'ì‚¬ìš© ì•ˆí•¨'}\")\n",
        "print(f\"  CPU í™œìš©: {mp.cpu_count()}ê°œ ì½”ì–´ ìµœëŒ€ í™œìš©\")\n",
        "\n",
        "# ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰\n",
        "bulk_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=BULK_LIMIT,\n",
        "    enable_api=API_ENHANCEMENT,\n",
        "    batch_size=10  # ë°°ì¹˜ í¬ê¸°\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… ëŒ€ëŸ‰ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(f\"ì²˜ë¦¬ëœ í˜ì´ì§€: {len(bulk_results)}ê°œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "## ğŸ“Š ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\n",
        "\n",
        "ì²˜ë¦¬ëœ ë°ì´í„°ì˜ í†µê³„ì™€ ìƒ˜í”Œì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "if 'bulk_results' in locals() and bulk_results:\n",
        "    print(\"ğŸ“Š ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # ê¸°ë³¸ í†µê³„\n",
        "    total_pages = len(bulk_results)\n",
        "    api_enhanced = sum(1 for page in bulk_results if page.get('api_categories'))\n",
        "    year_found = sum(1 for page in bulk_results if page.get('year'))\n",
        "    authors_found = sum(1 for page in bulk_results if page.get('authors'))\n",
        "    content_pages = sum(1 for page in bulk_results if page.get('content_size', 0) > 100)\n",
        "\n",
        "    print(f\"ğŸ“ˆ ì²˜ë¦¬ í†µê³„:\")\n",
        "    print(f\"  ì´ í˜ì´ì§€: {total_pages}ê°œ\")\n",
        "    print(f\"  API ë³´ê°•ëœ í˜ì´ì§€: {api_enhanced}ê°œ ({api_enhanced/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì—°ë„ ì •ë³´ ìˆëŠ” í˜ì´ì§€: {year_found}ê°œ ({year_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì €ì ì •ë³´ ìˆëŠ” í˜ì´ì§€: {authors_found}ê°œ ({authors_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì‹¤ì§ˆì  ë³¸ë¬¸ ìˆëŠ” í˜ì´ì§€: {content_pages}ê°œ ({content_pages/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë¶„ë¥˜ í†µê³„\n",
        "    all_categories = []\n",
        "    for page in bulk_results:\n",
        "        all_categories.extend(page.get('categories', []))\n",
        "\n",
        "    category_counts = {}\n",
        "    for cat in all_categories:\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "    top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ìƒìœ„ 10ê°œ ë¶„ë¥˜:\")\n",
        "    for cat, count in top_categories:\n",
        "        print(f\"  {cat}: {count}ê°œ\")\n",
        "\n",
        "    # ì—°ë„ë³„ í†µê³„\n",
        "    years = [page.get('year') for page in bulk_results if page.get('year')]\n",
        "    if years:\n",
        "        year_counts = {}\n",
        "        for year in years:\n",
        "            year_counts[year] = year_counts.get(year, 0) + 1\n",
        "\n",
        "        print(f\"\\nğŸ“… ì—°ë„ë³„ ë¶„í¬:\")\n",
        "        sorted_years = sorted(year_counts.items())\n",
        "        for year, count in sorted_years:\n",
        "            print(f\"  {year}ë…„: {count}ê°œ\")\n",
        "\n",
        "    # ìƒ˜í”Œ í˜ì´ì§€ í‘œì‹œ\n",
        "    print(f\"\\nğŸ“ ìƒ˜í”Œ í˜ì´ì§€ (ìƒìœ„ 10ê°œ):\")\n",
        "    for i, page in enumerate(bulk_results[:10], 1):\n",
        "        title = page['title']\n",
        "        categories_count = len(page.get('categories', []))\n",
        "        year = page.get('year', 'N/A')\n",
        "        content_size = page.get('content_size', 0)\n",
        "\n",
        "        print(f\"  {i:2d}. {title}\")\n",
        "        print(f\"      ë¶„ë¥˜: {categories_count}ê°œ, ì—°ë„: {year}, ë³¸ë¬¸: {content_size}ì\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ì˜ ëŒ€ëŸ‰ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ (CSV + JSON)\n",
        "\n",
        "ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ í›„ì† ë¶„ì„ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hR7MI7uPdZy1"
      },
      "outputs": [],
      "source": [
        "def save_bulk_results(data, base_filename=\"kowikisource_bulk\"):\n",
        "    \"\"\"\n",
        "    ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
        "\n",
        "    ì €ì¥ í˜•ì‹:\n",
        "    1. ì™„ì „í•œ JSON (ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨)\n",
        "    2. ìš”ì•½ CSV (í•µì‹¬ ì •ë³´ë§Œ)\n",
        "    3. ìƒì„¸ CSV (API ë³´ê°• ì •ë³´ í¬í•¨)\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        print(\"âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    print(f\"ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì¤‘...\")\n",
        "    print(f\"ğŸ“Š ë°ì´í„°: {len(data)}ê°œ í˜ì´ì§€\")\n",
        "\n",
        "    # 1. ì™„ì „í•œ JSON ì €ì¥ (ëª¨ë“  ë°ì´í„°)\n",
        "    json_filename = f\"{base_filename}_{timestamp}_complete.json\"\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    file_size_mb = os.path.getsize(json_filename) / (1024 * 1024)\n",
        "    print(f\"  âœ… ì™„ì „í•œ JSON: {json_filename} ({file_size_mb:.1f}MB)\")\n",
        "\n",
        "    # 2. ìš”ì•½ CSV ì €ì¥ (í•µì‹¬ ì •ë³´ë§Œ)\n",
        "    summary_csv_data = []\n",
        "    for page in data:\n",
        "        summary_csv_data.append({\n",
        "            'page_id': page['page_id'],\n",
        "            'title': page['title'],\n",
        "            'url': page['url'],\n",
        "            'namespace': page['namespace'],\n",
        "            'authors': ', '.join(page['authors']) if page['authors'] else '',\n",
        "            'categories': ', '.join(page['categories']) if page['categories'] else '',\n",
        "            'composer': page.get('composer', ''),\n",
        "            'translator': page.get('translator', ''),\n",
        "            'year': page.get('year', ''),\n",
        "            'license': page.get('license', ''),\n",
        "            'language': page.get('language', ''),\n",
        "            'content_length': page.get('content_size', 0),\n",
        "            'last_modified': page.get('last_modified', ''),\n",
        "            'last_contributor': page.get('last_contributor', '')\n",
        "        })\n",
        "\n",
        "    summary_csv_filename = f\"{base_filename}_{timestamp}_summary.csv\"\n",
        "    df_summary = pd.DataFrame(summary_csv_data)\n",
        "    df_summary.to_csv(summary_csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"  âœ… ìš”ì•½ CSV: {summary_csv_filename}\")\n",
        "\n",
        "    # 3. ìƒì„¸ CSV ì €ì¥ (API ë³´ê°• ì •ë³´ í¬í•¨)\n",
        "    detailed_csv_data = []\n",
        "    for page in data:\n",
        "        # API ë³´ê°• ì •ë³´ ë¶„ë¦¬\n",
        "        dump_categories = [cat for cat in page.get('categories', []) if cat not in page.get('api_categories', [])]\n",
        "        api_categories = page.get('api_categories', [])\n",
        "\n",
        "        detailed_csv_data.append({\n",
        "            'page_id': page['page_id'],\n",
        "            'title': page['title'],\n",
        "            'url': page['url'],\n",
        "            'namespace': page['namespace'],\n",
        "            'authors': ', '.join(page['authors']) if page['authors'] else '',\n",
        "            'dump_categories': ', '.join(dump_categories),\n",
        "            'api_categories': ', '.join(api_categories),\n",
        "            'all_categories': ', '.join(page['categories']) if page['categories'] else '',\n",
        "            'composer': page.get('composer', ''),\n",
        "            'year_final': page.get('year', ''),\n",
        "            'year_from_categories': page.get('year_from_categories', ''),\n",
        "            'year_from_wikidata': page.get('year_from_wikidata', ''),\n",
        "            'license': page.get('license', ''),\n",
        "            'language': page.get('language', ''),\n",
        "            'content_length': page.get('content_size', 0),\n",
        "            'raw_content_length': page.get('size', 0),\n",
        "            'last_modified': page.get('last_modified', ''),\n",
        "            'last_contributor': page.get('last_contributor', '')\n",
        "        })\n",
        "\n",
        "    detailed_csv_filename = f\"{base_filename}_{timestamp}_detailed.csv\"\n",
        "    df_detailed = pd.DataFrame(detailed_csv_data)\n",
        "    df_detailed.to_csv(detailed_csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"  âœ… ìƒì„¸ CSV: {detailed_csv_filename}\")\n",
        "\n",
        "    # 4. ë©”íƒ€ë°ì´í„° ìš”ì•½ ì €ì¥\n",
        "    metadata = {\n",
        "        'generation_info': {\n",
        "            'timestamp': timestamp,\n",
        "            'total_pages': len(data),\n",
        "            'api_enhanced_pages': sum(1 for page in data if page.get('api_categories')),\n",
        "            'pages_with_year': sum(1 for page in data if page.get('year')),\n",
        "            'pages_with_authors': sum(1 for page in data if page.get('authors')),\n",
        "            'pages_with_content': sum(1 for page in data if page.get('content_size', 0) > 100)\n",
        "        },\n",
        "        'files_generated': {\n",
        "            'complete_json': json_filename,\n",
        "            'summary_csv': summary_csv_filename,\n",
        "            'detailed_csv': detailed_csv_filename\n",
        "        }\n",
        "    }\n",
        "\n",
        "    metadata_filename = f\"{base_filename}_{timestamp}_metadata.json\"\n",
        "    with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"  âœ… ë©”íƒ€ë°ì´í„°: {metadata_filename}\")\n",
        "\n",
        "    print(f\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
        "    print(f\"  ğŸ“„ {json_filename} - ì™„ì „í•œ ë°ì´í„° (JSON)\")\n",
        "    print(f\"  ğŸ“Š {summary_csv_filename} - í•µì‹¬ ì •ë³´ (CSV)\")\n",
        "    print(f\"  ğŸ“ˆ {detailed_csv_filename} - API ë³´ê°• ì •ë³´ í¬í•¨ (CSV)\")\n",
        "    print(f\"  â„¹ï¸  {metadata_filename} - ìƒì„± ì •ë³´ (JSON)\")\n",
        "\n",
        "    return {\n",
        "        'json_file': json_filename,\n",
        "        'summary_csv': summary_csv_filename,\n",
        "        'detailed_csv': detailed_csv_filename,\n",
        "        'metadata_file': metadata_filename\n",
        "    }\n",
        "\n",
        "print(\"âœ… ëŒ€ëŸ‰ ì €ì¥ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iakSwBM7dZy2"
      },
      "outputs": [],
      "source": [
        "# ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤í–‰\n",
        "if 'bulk_results' in locals() and bulk_results:\n",
        "    print(\"ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
        "    saved_files = save_bulk_results(bulk_results)\n",
        "\n",
        "    print(f\"\\nğŸ¯ íŒŒì¼ í™œìš© ê°€ì´ë“œ:\")\n",
        "    print(f\"  ğŸ“Š pandas ë¶„ì„:\")\n",
        "    print(f\"     df = pd.read_csv('{saved_files['summary_csv']}')\")\n",
        "    print(f\"     df.head()\")\n",
        "\n",
        "    print(f\"\\n  ğŸ” ìƒì„¸ ë¶„ì„:\")\n",
        "    print(f\"     detailed_df = pd.read_csv('{saved_files['detailed_csv']}')\")\n",
        "    print(f\"     # API ë³´ê°• íš¨ê³¼ í™•ì¸\")\n",
        "    print(f\"     detailed_df[['dump_categories', 'api_categories']].head()\")\n",
        "\n",
        "    print(f\"\\n  ğŸ“„ ì™„ì „í•œ ë°ì´í„°:\")\n",
        "    print(f\"     with open('{saved_files['json_file']}', 'r') as f:\")\n",
        "    print(f\"         data = json.load(f)\")\n",
        "\n",
        "    # ê°„ë‹¨í•œ ë°ì´í„° ê²€ì¦\n",
        "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ í™•ì¸:\")\n",
        "\n",
        "    # CSV íŒŒì¼ ê²€ì¦\n",
        "    try:\n",
        "        test_df = pd.read_csv(saved_files['summary_csv'])\n",
        "        print(f\"  ğŸ“Š CSV íŒŒì¼: {len(test_df)}í–‰ Ã— {len(test_df.columns)}ì—´\")\n",
        "        print(f\"     ì£¼ìš” ì»¬ëŸ¼: {', '.join(test_df.columns[:5])}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ CSV ê²€ì¦ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    # JSON íŒŒì¼ ê²€ì¦\n",
        "    try:\n",
        "        with open(saved_files['json_file'], 'r', encoding='utf-8') as f:\n",
        "            test_json = json.load(f)\n",
        "        print(f\"  ğŸ“„ JSON íŒŒì¼: {len(test_json)}ê°œ í•­ëª©\")\n",
        "        if test_json:\n",
        "            print(f\"     í‚¤ ê°œìˆ˜: {len(test_json[0].keys())}ê°œ\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ JSON ê²€ì¦ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ì €ì¥í•  ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ğŸ’¡ ìœ„ì˜ 'ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰' ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggs6IfeUdZy2"
      },
      "source": [
        "## ğŸ” ì• êµ­ê°€ ê°€ì‚¬ ê²€ì¦\n",
        "\n",
        "ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì• êµ­ê°€ ë³¸ë¬¸(ê°€ì‚¬)ì´ ì œëŒ€ë¡œ íŒŒì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rDiNaO_dZy2"
      },
      "outputs": [],
      "source": [
        "## ğŸ“ íŠœí† ë¦¬ì–¼ ì™„ë£Œ!\n",
        "\n",
        "### ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ë‹¤ìŒì„ ë°°ì› ìŠµë‹ˆë‹¤:\n",
        "\n",
        "1. **ìœ„í‚¤ë¬¸í—Œ XML ë¤í”„ íŒŒì‹±**\n",
        "   - ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
        "   - ë§ˆí¬ì—… ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë³¸ë¬¸ ìƒì„±\n",
        "   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬\n",
        "\n",
        "2. **API ë³´ê°• ê¸°ë²•**\n",
        "   - ìœ„í‚¤ë¬¸í—Œ APIë¡œ ì™„ì „í•œ ë¶„ë¥˜ ì •ë³´ ì¶”ì¶œ\n",
        "   - ìœ„í‚¤ë°ì´í„° APIë¡œ êµ¬ì¡°í™”ëœ ì—°ë„ ì •ë³´ íšë“\n",
        "   - ì—¬ëŸ¬ ì†ŒìŠ¤ ì •ë³´ í†µí•© ë° ê²€ì¦\n",
        "\n",
        "3. **ì‹¤ì œ ë¬¸ì œ í•´ê²°**\n",
        "   - ì• êµ­ê°€ ì‚¬ë¡€: 2ê°œ â†’ 4ê°œ ë¶„ë¥˜ ì™„ì „ ì¶”ì¶œ\n",
        "   - \"1941ë…„ ì‘í’ˆ\" ë¶„ë¥˜ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ\n",
        "   - í…œí”Œë¦¿ ê¸°ë°˜ ë¶„ë¥˜(PD-old-50) ì¶”ì¶œ\n",
        "   - ì• êµ­ê°€ ê°€ì‚¬(í•œì+í•œê¸€) ì™„ì „ ë³´ì¡´ í™•ì¸\n",
        "\n",
        "4. **ëŒ€ëŸ‰ ì²˜ë¦¬ ë° ë°ì´í„° ì €ì¥**\n",
        "   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ëŒ€ëŸ‰ í˜ì´ì§€ ì²˜ë¦¬\n",
        "   - JSON, CSV ë‹¤ì¤‘ í˜•ì‹ ì €ì¥\n",
        "   - API ë³´ê°• íš¨ê³¼ ì¶”ì  ë° ë¶„ì„\n",
        "   - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
        "\n",
        "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n",
        "- ë” ë§ì€ í˜ì´ì§€ë¡œ ëŒ€ëŸ‰ ì²˜ë¦¬ í•´ë³´ê¸° (BULK_LIMIT ê°’ ì¦ê°€)\n",
        "- ì¶”ì¶œëœ CSV ë°ì´í„°ë¡œ pandas ë¶„ì„ ìˆ˜í–‰\n",
        "- ë‹¤ë¥¸ ìœ„í‚¤ë¯¸ë””ì–´ í”„ë¡œì íŠ¸ì— ì‘ìš©\n",
        "- ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í†µê³„ ê·¸ë˜í”„ ìƒì„±\n",
        "\n",
        "### ğŸ’¡ í•µì‹¬ êµí›ˆ:\n",
        "- **ë‹¨ì¼ ì†ŒìŠ¤ì˜ í•œê³„**: XML ë¤í”„ë§Œìœ¼ë¡œëŠ” ì™„ì „í•œ ì •ë³´ ì¶”ì¶œ ì–´ë ¤ì›€\n",
        "- **API ë³´ê°•ì˜ ì¤‘ìš”ì„±**: ì›¹ì‚¬ì´íŠ¸ì™€ ë™ì¼í•œ ì™„ì „í•œ ë°ì´í„° íšë“\n",
        "- **ê²€ì¦ì˜ í•„ìš”ì„±**: ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ êµì°¨ í™•ì¸\n",
        "- **ì„±ëŠ¥ ìµœì í™”**: ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ëŒ€ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬\n",
        "- **ë°ì´í„° í’ˆì§ˆ**: ë³¸ë¬¸ ë‚´ìš©ê¹Œì§€ ì™„ì „íˆ ë³´ì¡´í•˜ëŠ” íŒŒì‹±\n",
        "\n",
        "### ğŸ“Š ìƒì„±ëœ ë°ì´í„°:\n",
        "- **ì™„ì „í•œ JSON**: ëª¨ë“  ë©”íƒ€ë°ì´í„°ì™€ ë³¸ë¬¸ í¬í•¨\n",
        "- **ìš”ì•½ CSV**: í•µì‹¬ ì •ë³´ë§Œ ì •ë¦¬ëœ ë¶„ì„ìš© ë°ì´í„°\n",
        "- **ìƒì„¸ CSV**: API ë³´ê°• ê³¼ì •ê¹Œì§€ ì¶”ì  ê°€ëŠ¥í•œ ìƒì„¸ ë°ì´í„°\n",
        "\n",
        "**ì´ì œ ì—¬ëŸ¬ë¶„ë„ ìœ„í‚¤ë¬¸í—Œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤! ğŸŒŸ**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”„ ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰ ë°©ë²•:\n",
        "1. \"ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰\" ì…€ì—ì„œ `BULK_LIMIT` ê°’ ì¡°ì •\n",
        "2. ì…€ ì‹¤í–‰ í›„ \"ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\" í™•ì¸\n",
        "3. \"ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥\" ì‹¤í–‰ìœ¼ë¡œ íŒŒì¼ ìƒì„±\n",
        "4. ìƒì„±ëœ CSV/JSON íŒŒì¼ë¡œ ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvoWwLVFdZy3"
      },
      "source": [
        "## ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± (ê³ ê¸‰)\n",
        "\n",
        "**ì£¼ì˜**: ì´ ì„¹ì…˜ì€ ì „ì²´ í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œì„ íŒŒì‹±í•©ë‹ˆë‹¤. ìˆ˜ì²œ ê°œì˜ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### âš ï¸ ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­:\n",
        "- Colab Pro ì‚¬ìš© ê¶Œì¥ (ë” ë§ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ ì œí•œ)\n",
        "- ì•ˆì •ì ì¸ ì¸í„°ë„· ì—°ê²° í•„ìš” (API í˜¸ì¶œ)\n",
        "- ì™„ë£Œê¹Œì§€ 30ë¶„-2ì‹œê°„ ì†Œìš” ì˜ˆìƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5HZC7ENdZy3"
      },
      "outputs": [],
      "source": [
        "# ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì„¤ì •\n",
        "print(\"ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì¤€ë¹„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# íŒŒì‹± ì„¤ì •\n",
        "FULL_PARSING_CONFIG = {\n",
        "    'enable_api': True,           # API ë³´ê°• ì‚¬ìš© (ê¶Œì¥)\n",
        "    'batch_size': 20,            # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ê³ ë ¤)\n",
        "    'api_delay': 0.05,           # API í˜¸ì¶œ ê°„ê²© (ì´ˆ)\n",
        "    'save_interval': 1000,       # ì¤‘ê°„ ì €ì¥ ê°„ê²©\n",
        "    'max_retries': 3,            # API ì¬ì‹œë„ íšŸìˆ˜\n",
        "}\n",
        "\n",
        "print(f\"ğŸ“Š ì„¤ì •:\")\n",
        "print(f\"  API ë³´ê°•: {'âœ…' if FULL_PARSING_CONFIG['enable_api'] else 'âŒ'}\")\n",
        "print(f\"  ë°°ì¹˜ í¬ê¸°: {FULL_PARSING_CONFIG['batch_size']}\")\n",
        "print(f\"  CPU í™œìš©: {mp.cpu_count()}ê°œ ì½”ì–´\")\n",
        "print(f\"  ì¤‘ê°„ ì €ì¥: {FULL_PARSING_CONFIG['save_interval']}ê°œë§ˆë‹¤\")\n",
        "\n",
        "# ì˜ˆìƒ ì‹œê°„ ê³„ì‚°\n",
        "import os\n",
        "dump_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "estimated_pages = int(dump_size * 10)  # ëŒ€ëµì  ì¶”ì •\n",
        "\n",
        "print(f\"\\nğŸ“ˆ ì˜ˆìƒ ì •ë³´:\")\n",
        "print(f\"  ë¤í”„ í¬ê¸°: {dump_size:.1f}MB\")\n",
        "print(f\"  ì˜ˆìƒ í˜ì´ì§€: ~{estimated_pages:,}ê°œ\")\n",
        "print(f\"  ì˜ˆìƒ ì‹œê°„: {estimated_pages/100:.0f}-{estimated_pages/50:.0f}ë¶„ (API í¬í•¨)\")\n",
        "\n",
        "# ì‚¬ìš©ì í™•ì¸\n",
        "print(f\"\\nâš ï¸  ì „ì²´ íŒŒì‹±ì„ ì‹¤í–‰í•˜ë ¤ë©´:\")\n",
        "print(f\"  1. ì•„ë˜ EXECUTE_FULL_PARSINGì„ Trueë¡œ ë³€ê²½\")\n",
        "print(f\"  2. ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰\")\n",
        "print(f\"  3. ì§„í–‰ìƒí™©ì„ ëª¨ë‹ˆí„°ë§\")\n",
        "\n",
        "# ì•ˆì „ì¥ì¹˜\n",
        "EXECUTE_FULL_PARSING = False  # ì „ì²´ íŒŒì‹± ì‹¤í–‰í•˜ë ¤ë©´ Trueë¡œ ë³€ê²½\n",
        "\n",
        "if EXECUTE_FULL_PARSING:\n",
        "    print(f\"\\nğŸ”¥ ì „ì²´ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "else:\n",
        "    print(f\"\\nğŸ’¡ ì•„ì§ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. EXECUTE_FULL_PARSING = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmIFhCnvdZy3"
      },
      "outputs": [],
      "source": [
        "# í–¥ìƒëœ ì „ì²´ íŒŒì‹± í•¨ìˆ˜ (ì¤‘ê°„ ì €ì¥ í¬í•¨)\n",
        "def parse_full_wikisource(dump_file, config):\n",
        "    \"\"\"\n",
        "    ì „ì²´ ìœ„í‚¤ë¬¸í—Œì„ íŒŒì‹±í•˜ê³  ì¤‘ê°„ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\")\n",
        "    print(f\"  ğŸ“Š CPU ì½”ì–´: {mp.cpu_count()}ê°œ í™œìš©\")\n",
        "    print(f\"  ğŸŒ API ë³´ê°•: {'ì‚¬ìš©' if config['enable_api'] else 'ì‚¬ìš© ì•ˆí•¨'}\")\n",
        "    print(f\"  ğŸ’¾ ì¤‘ê°„ ì €ì¥: {config['save_interval']}ê°œë§ˆë‹¤\")\n",
        "\n",
        "    # 1ë‹¨ê³„: ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘\n",
        "    print(f\"\\nğŸ“¥ 1ë‹¨ê³„: ì „ì²´ ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\")\n",
        "    all_pages = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"ì „ì²´ í˜ì´ì§€ ìˆ˜ì§‘\"):\n",
        "            try:\n",
        "                # ë¦¬ë¹„ì „ ë°ì´í„° ìˆ˜ì§‘\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break\n",
        "\n",
        "                if revisions:  # ë¦¬ë¹„ì „ì´ ìˆëŠ” í˜ì´ì§€ë§Œ\n",
        "                    page_data = (\n",
        "                        page.id,\n",
        "                        page.title,\n",
        "                        page.namespace,\n",
        "                        str(page.redirect.title) if page.redirect else None,\n",
        "                        revisions\n",
        "                    )\n",
        "                    all_pages.append(page_data)\n",
        "                    total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    print(f\"âœ… ì´ {total_pages:,}ê°œ í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
        "\n",
        "    # 2ë‹¨ê³„: ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°\n",
        "    batch_size = config['batch_size']\n",
        "    batches = [all_pages[i:i+batch_size] for i in range(0, len(all_pages), batch_size)]\n",
        "    print(f\"ğŸ“¦ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (ë°°ì¹˜ë‹¹ {batch_size}ê°œ)\")\n",
        "\n",
        "    # 3ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬\n",
        "    print(f\"\\nâš¡ 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ë°°ì¹˜ ì²˜ë¦¬\")\n",
        "    all_results = []\n",
        "    processed_count = 0\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:\n",
        "        # ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"ë°°ì¹˜ ì²˜ë¦¬\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                all_results.extend(batch_results)\n",
        "                processed_count += len(batch_results)\n",
        "\n",
        "                # ì¤‘ê°„ ì €ì¥\n",
        "                if processed_count % config['save_interval'] == 0:\n",
        "                    temp_filename = f\"temp_wikisource_{processed_count}.json\"\n",
        "                    with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
        "                    print(f\"\\nğŸ’¾ ì¤‘ê°„ ì €ì¥: {temp_filename} ({processed_count:,}ê°œ í˜ì´ì§€)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"âœ… ê¸°ë³¸ íŒŒì‹± ì™„ë£Œ: {len(all_results):,}ê°œ í˜ì´ì§€\")\n",
        "\n",
        "    # 4ë‹¨ê³„: API ë³´ê°• (ì„ íƒì )\n",
        "    if config['enable_api'] and all_results:\n",
        "        print(f\"\\nğŸŒ 3ë‹¨ê³„: API ë³´ê°• ì²˜ë¦¬\")\n",
        "        enhanced_results = []\n",
        "        api_success = 0\n",
        "\n",
        "        for i, page_data in enumerate(tqdm(all_results, desc=\"API ë³´ê°•\")):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "\n",
        "                # API ì„±ê³µ ì¹´ìš´íŠ¸\n",
        "                if enhanced.get('api_categories'):\n",
        "                    api_success += 1\n",
        "\n",
        "                # API í˜¸ì¶œ ì œí•œ\n",
        "                time.sleep(config['api_delay'])\n",
        "\n",
        "                # ì¤‘ê°„ ì €ì¥ (API ë³´ê°• ë²„ì „)\n",
        "                if (i + 1) % config['save_interval'] == 0:\n",
        "                    temp_filename = f\"temp_wikisource_api_{i+1}.json\"\n",
        "                    with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(enhanced_results, f, ensure_ascii=False, indent=2)\n",
        "                    print(f\"\\nğŸ’¾ API ë³´ê°• ì¤‘ê°„ ì €ì¥: {temp_filename} ({i+1:,}ê°œ í˜ì´ì§€)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"API ë³´ê°• ì˜¤ë¥˜ ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        all_results = enhanced_results\n",
        "        print(f\"âœ… API ë³´ê°• ì™„ë£Œ: {api_success:,}/{len(all_results):,}ê°œ ì„±ê³µ ({api_success/len(all_results)*100:.1f}%)\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nğŸ‰ ì „ì²´ íŒŒì‹± ì™„ë£Œ!\")\n",
        "    print(f\"  â±ï¸  ì´ ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
        "    print(f\"  ğŸ“Š ì²˜ë¦¬ ì†ë„: {len(all_results)/(total_time/60):.1f} í˜ì´ì§€/ë¶„\")\n",
        "    print(f\"  ğŸ“„ ì´ í˜ì´ì§€: {len(all_results):,}ê°œ\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# ì „ì²´ íŒŒì‹± ì‹¤í–‰\n",
        "if EXECUTE_FULL_PARSING:\n",
        "    print(\"ğŸ”¥ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n",
        "    print(\"ì´ ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§„í–‰ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n",
        "    full_results = parse_full_wikisource(dump_file, FULL_PARSING_CONFIG)\n",
        "\n",
        "    print(f\"\\nâœ… ì „ì²´ íŒŒì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "    print(f\"ë³€ìˆ˜ 'full_results'ì— {len(full_results):,}ê°œ í˜ì´ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "else:\n",
        "    print(\"â¸ï¸  ì „ì²´ íŒŒì‹±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    print(\"EXECUTE_FULL_PARSING = Trueë¡œ ì„¤ì •í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pz1-CtQdZy4"
      },
      "source": [
        "## ğŸ“Š ì „ì²´ íŒŒì‹± ê²°ê³¼ ë¶„ì„\n",
        "\n",
        "ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹±ì´ ì™„ë£Œëœ í›„ ìƒì„¸í•œ í†µê³„ì™€ ë¶„ì„ì„ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pls2TV6wdZy4"
      },
      "outputs": [],
      "source": [
        "# ì „ì²´ íŒŒì‹± ê²°ê³¼ ë¶„ì„\n",
        "if 'full_results' in locals() and full_results:\n",
        "    print(\"ğŸ“Š ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ê²°ê³¼ ìƒì„¸ ë¶„ì„\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # ê¸°ë³¸ í†µê³„\n",
        "    total_pages = len(full_results)\n",
        "    api_enhanced = sum(1 for page in full_results if page.get('api_categories'))\n",
        "    year_found = sum(1 for page in full_results if page.get('year'))\n",
        "    authors_found = sum(1 for page in full_results if page.get('authors'))\n",
        "    content_pages = sum(1 for page in full_results if page.get('content_size', 0) > 100)\n",
        "    substantial_content = sum(1 for page in full_results if page.get('content_size', 0) > 1000)\n",
        "\n",
        "    print(f\"ğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
        "    print(f\"  ì´ í˜ì´ì§€: {total_pages:,}ê°œ\")\n",
        "    print(f\"  API ë³´ê°•ëœ í˜ì´ì§€: {api_enhanced:,}ê°œ ({api_enhanced/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì—°ë„ ì •ë³´ ìˆëŠ” í˜ì´ì§€: {year_found:,}ê°œ ({year_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì €ì ì •ë³´ ìˆëŠ” í˜ì´ì§€: {authors_found:,}ê°œ ({authors_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ë³¸ë¬¸ ìˆëŠ” í˜ì´ì§€: {content_pages:,}ê°œ ({content_pages/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì‹¤ì§ˆì  ë³¸ë¬¸(1000ì+): {substantial_content:,}ê°œ ({substantial_content/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶„ì„\n",
        "    namespace_counts = {}\n",
        "    for page in full_results:\n",
        "        ns = page.get('namespace', 0)\n",
        "        namespace_counts[ns] = namespace_counts.get(ns, 0) + 1\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶„í¬:\")\n",
        "    for ns, count in sorted(namespace_counts.items()):\n",
        "        ns_name = {0: 'ë³¸ë¬¸', 100: 'ì €ì', 102: 'ìƒ‰ì¸', 104: 'ë²ˆì—­'}.get(ns, f'ê¸°íƒ€({ns})')\n",
        "        print(f\"  {ns_name}: {count:,}ê°œ ({count/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë¶„ë¥˜ ë¶„ì„\n",
        "    all_categories = []\n",
        "    for page in full_results:\n",
        "        all_categories.extend(page.get('categories', []))\n",
        "\n",
        "    category_counts = {}\n",
        "    for cat in all_categories:\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "    top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ìƒìœ„ 20ê°œ ë¶„ë¥˜:\")\n",
        "    for i, (cat, count) in enumerate(top_categories, 1):\n",
        "        print(f\"  {i:2d}. {cat}: {count:,}ê°œ\")\n",
        "\n",
        "    # ì—°ë„ë³„ ë¶„í¬\n",
        "    years = [page.get('year') for page in full_results if page.get('year')]\n",
        "    if years:\n",
        "        year_counts = {}\n",
        "        for year in years:\n",
        "            try:\n",
        "                year_int = int(year)\n",
        "                decade = (year_int // 10) * 10\n",
        "                year_counts[decade] = year_counts.get(decade, 0) + 1\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nğŸ“… ì—°ëŒ€ë³„ ì‘í’ˆ ë¶„í¬:\")\n",
        "        for decade in sorted(year_counts.keys()):\n",
        "            count = year_counts[decade]\n",
        "            print(f\"  {decade}ë…„ëŒ€: {count:,}ê°œ\")\n",
        "\n",
        "    # ì–¸ì–´ ë¶„ì„\n",
        "    language_counts = {}\n",
        "    for page in full_results:\n",
        "        lang = page.get('language', 'ë¯¸ë¶„ë¥˜')\n",
        "        language_counts[lang] = language_counts.get(lang, 0) + 1\n",
        "\n",
        "    print(f\"\\nğŸ”¤ ì–¸ì–´ ë¶„í¬:\")\n",
        "    for lang, count in sorted(language_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        if lang and count > 0:\n",
        "            print(f\"  {lang}: {count:,}ê°œ ({count/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë³¸ë¬¸ ê¸¸ì´ ë¶„ì„\n",
        "    content_lengths = [page.get('content_size', 0) for page in full_results]\n",
        "    content_lengths = [x for x in content_lengths if x > 0]\n",
        "\n",
        "    if content_lengths:\n",
        "        import statistics\n",
        "        avg_length = statistics.mean(content_lengths)\n",
        "        median_length = statistics.median(content_lengths)\n",
        "        max_length = max(content_lengths)\n",
        "\n",
        "        print(f\"\\nğŸ“ ë³¸ë¬¸ ê¸¸ì´ ë¶„ì„:\")\n",
        "        print(f\"  í‰ê·  ê¸¸ì´: {avg_length:.0f}ì\")\n",
        "        print(f\"  ì¤‘ê°„ê°’: {median_length:.0f}ì\")\n",
        "        print(f\"  ìµœëŒ€ ê¸¸ì´: {max_length:,}ì\")\n",
        "\n",
        "        # ê¸¸ì´ë³„ ë¶„í¬\n",
        "        length_ranges = [\n",
        "            (0, 100, \"ë§¤ìš° ì§§ìŒ\"),\n",
        "            (100, 1000, \"ì§§ìŒ\"),\n",
        "            (1000, 5000, \"ì¤‘ê°„\"),\n",
        "            (5000, 20000, \"ê¸´ í¸\"),\n",
        "            (20000, float('inf'), \"ë§¤ìš° ê¸´\")\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nğŸ“Š ê¸¸ì´ë³„ ë¶„í¬:\")\n",
        "        for min_len, max_len, desc in length_ranges:\n",
        "            count = sum(1 for x in content_lengths if min_len <= x < max_len)\n",
        "            if count > 0:\n",
        "                print(f\"  {desc} ({min_len:,}-{max_len:,}ì): {count:,}ê°œ ({count/len(content_lengths)*100:.1f}%)\")\n",
        "\n",
        "    # ê°€ì¥ ê¸´ ë¬¸ì„œë“¤\n",
        "    longest_pages = sorted(full_results, key=lambda x: x.get('content_size', 0), reverse=True)[:10]\n",
        "\n",
        "    print(f\"\\nğŸ“– ê°€ì¥ ê¸´ ë¬¸ì„œ Top 10:\")\n",
        "    for i, page in enumerate(longest_pages, 1):\n",
        "        title = page['title']\n",
        "        length = page.get('content_size', 0)\n",
        "        print(f\"  {i:2d}. {title}: {length:,}ì\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ í’ˆì§ˆ ì§€í‘œ:\")\n",
        "    print(f\"  ì™„ì „í•œ ë©”íƒ€ë°ì´í„° í˜ì´ì§€: {sum(1 for p in full_results if p.get('authors') and p.get('categories') and p.get('year')):,}ê°œ\")\n",
        "    print(f\"  API ë³´ê°• ì„±ê³µë¥ : {api_enhanced/total_pages*100:.1f}%\")\n",
        "    print(f\"  ìœ ì˜ë¯¸í•œ ë³¸ë¬¸ ë¹„ìœ¨: {content_pages/total_pages*100:.1f}%\")\n",
        "    print(f\"  êµ¬ì¡°í™”ëœ ì •ë³´ ë¹„ìœ¨: {year_found/total_pages*100:.1f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ì „ì²´ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ğŸ’¡ ë¨¼ì € ì „ì²´ íŒŒì‹±ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6M-uYC6dZy5"
      },
      "source": [
        "## ğŸ’¾ ì „ì²´ ë°ì´í„°ì…‹ ìµœì¢… ì €ì¥\n",
        "\n",
        "ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ê²°ê³¼ë¥¼ ì™„ì „í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy5_i_jldZy5"
      },
      "source": [
        "## ğŸ“ íŠœí† ë¦¬ì–¼ ì™„ë£Œ!\n",
        "\n",
        "### ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ë‹¤ìŒì„ ë°°ì› ìŠµë‹ˆë‹¤:\n",
        "\n",
        "1. **ìœ„í‚¤ë¬¸í—Œ XML ë¤í”„ íŒŒì‹±**\n",
        "   - ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
        "   - ë§ˆí¬ì—… ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë³¸ë¬¸ ìƒì„±\n",
        "   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬\n",
        "\n",
        "2. **API ë³´ê°• ê¸°ë²•**\n",
        "   - ìœ„í‚¤ë¬¸í—Œ APIë¡œ ì™„ì „í•œ ë¶„ë¥˜ ì •ë³´ ì¶”ì¶œ\n",
        "   - ìœ„í‚¤ë°ì´í„° APIë¡œ êµ¬ì¡°í™”ëœ ì—°ë„ ì •ë³´ íšë“\n",
        "   - ì—¬ëŸ¬ ì†ŒìŠ¤ ì •ë³´ í†µí•© ë° ê²€ì¦\n",
        "\n",
        "3. **ì‹¤ì œ ë¬¸ì œ í•´ê²°**\n",
        "   - ì• êµ­ê°€ ì‚¬ë¡€: 2ê°œ â†’ 4ê°œ ë¶„ë¥˜ ì™„ì „ ì¶”ì¶œ\n",
        "   - \"1941ë…„ ì‘í’ˆ\" ë¶„ë¥˜ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ\n",
        "   - í…œí”Œë¦¿ ê¸°ë°˜ ë¶„ë¥˜(PD-old-50) ì¶”ì¶œ\n",
        "   - ì• êµ­ê°€ ê°€ì‚¬(í•œì+í•œê¸€) ì™„ì „ ë³´ì¡´ í™•ì¸\n",
        "\n",
        "4. **ëŒ€ëŸ‰ ì²˜ë¦¬ ë° ë°ì´í„° ì €ì¥**\n",
        "   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ëŒ€ëŸ‰ í˜ì´ì§€ ì²˜ë¦¬\n",
        "   - JSON, CSV ë‹¤ì¤‘ í˜•ì‹ ì €ì¥\n",
        "   - API ë³´ê°• íš¨ê³¼ ì¶”ì  ë° ë¶„ì„\n",
        "   - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
        "\n",
        "5. **ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± (ê³ ê¸‰)**\n",
        "   - ìˆ˜ì²œ ê°œ í˜ì´ì§€ì˜ ì™„ì „í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
        "   - ì¤‘ê°„ ì €ì¥ìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥\n",
        "   - ì—°êµ¬ìš©, ë¶„ì„ìš©, í…ìŠ¤íŠ¸ìš© ë‹¤ì¤‘ í˜•ì‹\n",
        "   - ì™„ì „í•œ ë©”íƒ€ë°ì´í„°ì™€ í†µê³„ ì •ë³´\n",
        "\n",
        "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n",
        "- **ì†Œê·œëª¨ ì—°êµ¬**: ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ë¡œ íŠ¹ì • ì£¼ì œ ë¶„ì„\n",
        "- **ëŒ€ê·œëª¨ ì—°êµ¬**: ì „ì²´ ìœ„í‚¤ë¬¸í—Œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¢…í•© ë¶„ì„\n",
        "- **ì‘ìš© ê°œë°œ**: ì¶”ì¶œëœ ë°ì´í„°ë¡œ ê²€ìƒ‰ ì—”ì§„, ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
        "- **ë‹¤ë¥¸ í”„ë¡œì íŠ¸**: ìœ„í‚¤ë°±ê³¼, ìœ„í‚¤ë‚±ë§ì‚¬ì „ ë“±ìœ¼ë¡œ í™•ì¥\n",
        "\n",
        "### ğŸ’¡ í•µì‹¬ êµí›ˆ:\n",
        "- **ë‹¨ì¼ ì†ŒìŠ¤ì˜ í•œê³„**: XML ë¤í”„ë§Œìœ¼ë¡œëŠ” ì™„ì „í•œ ì •ë³´ ì¶”ì¶œ ì–´ë ¤ì›€\n",
        "- **API ë³´ê°•ì˜ ì¤‘ìš”ì„±**: ì›¹ì‚¬ì´íŠ¸ì™€ ë™ì¼í•œ ì™„ì „í•œ ë°ì´í„° íšë“\n",
        "- **ê²€ì¦ì˜ í•„ìš”ì„±**: ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ êµì°¨ í™•ì¸\n",
        "- **ì„±ëŠ¥ ìµœì í™”**: ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ëŒ€ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬\n",
        "- **ë°ì´í„° í’ˆì§ˆ**: ë³¸ë¬¸ ë‚´ìš©ê¹Œì§€ ì™„ì „íˆ ë³´ì¡´í•˜ëŠ” íŒŒì‹±\n",
        "- **í™•ì¥ì„± ê³ ë ¤**: ì¤‘ê°„ ì €ì¥ê³¼ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì•ˆì •ì  ì²˜ë¦¬\n",
        "\n",
        "### ğŸ“Š ìƒì„± ê°€ëŠ¥í•œ ë°ì´í„°:\n",
        "- **ì™„ì „í•œ JSON**: ëª¨ë“  ë©”íƒ€ë°ì´í„°ì™€ ë³¸ë¬¸ í¬í•¨\n",
        "- **ì—°êµ¬ìš© CSV**: í•µì‹¬ ì •ë³´ë§Œ ì •ë¦¬ëœ ë¶„ì„ìš© ë°ì´í„°\n",
        "- **ìƒì„¸ CSV**: API ë³´ê°• ê³¼ì •ê¹Œì§€ ì¶”ì  ê°€ëŠ¥í•œ ìƒì„¸ ë°ì´í„°\n",
        "- **í…ìŠ¤íŠ¸ CSV**: ë³¸ë¬¸ ë¶„ì„ ì „ìš© ë°ì´í„°\n",
        "- **ë©”íƒ€ë°ì´í„°**: ë°ì´í„°ì…‹ í†µê³„ì™€ í’ˆì§ˆ ì •ë³´\n",
        "\n",
        "**ì´ì œ ì—¬ëŸ¬ë¶„ë„ ìœ„í‚¤ë¬¸í—Œ ë°ì´í„° ê³¼í•™ìì…ë‹ˆë‹¤! ğŸŒŸ**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”„ ì‹¤í–‰ ê°€ì´ë“œ:\n",
        "\n",
        "**ğŸ“ ê¸°ë³¸ í•™ìŠµ (ê¶Œì¥):**\n",
        "1. \"ê¸°ë³¸ íŒŒì‹±\" â†’ \"API ë³´ê°• íŒŒì‹±\" â†’ \"ë¹„êµ ë¶„ì„\" ìˆœì„œë¡œ ì‹¤í–‰\n",
        "2. ì• êµ­ê°€ ì‚¬ë¡€ë¡œ í•µì‹¬ ê°œë… ì´í•´\n",
        "3. ì†Œê·œëª¨ ëŒ€ëŸ‰ ì²˜ë¦¬(10-50ê°œ)ë¡œ ê²½í—˜ ìŒ“ê¸°\n",
        "\n",
        "**ğŸš€ ëŒ€ëŸ‰ ì²˜ë¦¬:**\n",
        "1. `BULK_LIMIT` ê°’ì„ 100-1000ìœ¼ë¡œ ì„¤ì •\n",
        "2. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥ ì‹¤í–‰\n",
        "3. ìƒì„±ëœ CSV/JSONìœ¼ë¡œ ë¶„ì„ ì‹¤ìŠµ\n",
        "\n",
        "**ğŸ”¥ ì „ì²´ íŒŒì‹± (ê³ ê¸‰):**\n",
        "1. `EXECUTE_FULL_PARSING = True` ì„¤ì •\n",
        "2. ìˆ˜ì‹­ ë¶„-ëª‡ ì‹œê°„ ëŒ€ê¸° (ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§)\n",
        "3. ì™„ì „í•œ í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œ ë°ì´í„°ì…‹ íšë“\n",
        "4. ì—°êµ¬ í”„ë¡œì íŠ¸ë‚˜ ë…¼ë¬¸ì— í™œìš©\n",
        "\n",
        "**âš ï¸ ì£¼ì˜ì‚¬í•­:**\n",
        "- ì „ì²´ íŒŒì‹±ì€ ë§ì€ ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ í•„ìš”\n",
        "- Colab Pro ê¶Œì¥ (ë” ë§ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„)\n",
        "- API í˜¸ì¶œ ì œí•œìœ¼ë¡œ ì¸í•œ ì§€ì—° ê°€ëŠ¥"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}