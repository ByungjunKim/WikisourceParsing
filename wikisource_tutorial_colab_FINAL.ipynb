{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ByungjunKim/WikisourceParsing/blob/main/wikisource_tutorial_colab_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_header"
      },
      "source": [
        "# ğŸ“š ìœ„í‚¤ë¬¸í—Œ íŒŒì„œ íŠœí† ë¦¬ì–¼\n",
        "\n",
        "**í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œì—ì„œ ì™„ì „í•œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ë°°ì›Œë³´ì„¸ìš”!**\n",
        "\n",
        "## ğŸ¯ í•™ìŠµ ëª©í‘œ\n",
        "- ìœ„í‚¤ë¬¸í—Œ XML ë¤í”„ì—ì„œ í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œí•˜ê¸°\n",
        "- APIë¥¼ í™œìš©í•´ ëˆ„ë½ëœ ë¶„ë¥˜ ì •ë³´ ë³´ê°•í•˜ê¸°\n",
        "- ìœ„í‚¤ë°ì´í„°ì™€ ì—°ë™í•´ ì—°ë„ ì •ë³´ ì¶”ì¶œí•˜ê¸°\n",
        "- ì‹¤ì œ ì‚¬ìš© ê°€ëŠ¥í•œ ê¹”ë”í•œ í…ìŠ¤íŠ¸ ë°ì´í„° ìƒì„±í•˜ê¸°\n",
        "\n",
        "## ğŸ“– ì˜ˆì œ: ì• êµ­ê°€ ì™„ì „ ë¶„ì„\n",
        "- **ë¬¸ì œ**: XML ë¤í”„ì—ì„œëŠ” 2ê°œ ë¶„ë¥˜ë§Œ ë‚˜ì˜¤ëŠ”ë°, ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ëŠ” 4ê°œê°€ ìˆìŒ\n",
        "- **í•´ê²°**: API ë³´ê°•ìœ¼ë¡œ '1941ë…„ ì‘í’ˆ', 'PD-old-50' ë“± ëˆ„ë½ëœ ë¶„ë¥˜ê¹Œì§€ ì™„ì „ ì¶”ì¶œ\n",
        "- **ê²°ê³¼**: ë¶„ë¥˜ì—ì„œ ì—°ë„ ì •ë³´(1941ë…„)ê¹Œì§€ ì •í™•í•˜ê²Œ ì¶”ì¶œ ì„±ê³µ!\n",
        "\n",
        "---\n",
        "**âœ… ë…¸íŠ¸ë¶ í…ŒìŠ¤íŠ¸ ì™„ë£Œ**: 2025-09-11 10:21:57\n",
        "- ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì„±ê³µ\n",
        "- API í•¨ìˆ˜ë“¤ ì •ìƒ ì‘ë™\n",
        "- ë¤í”„ íŒŒì¼ í™•ì¸ ì™„ë£Œ (147.8MB)\n",
        "- ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ ì¤€ë¹„ (11ê°œ ì½”ì–´)\n",
        "\n",
        "**ğŸš€ ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ!**\n",
        "\n",
        "---\n",
        "## ğŸ‰ **ìµœì¢… ë²„ì „ - Colab ì‹¤í–‰ ì¤€ë¹„ ì™„ë£Œ!**\n",
        "**ë²„ì „**: v1.0 Final | **ìƒì„±ì¼**: 2025-09-11 10:58:55\n",
        "\n",
        "### âœ… **ê²€ì¦ ì™„ë£Œ ì‚¬í•­:**\n",
        "- ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ í˜¸í™˜ì„± í™•ì¸\n",
        "- TypeError ë° ì •ë ¬ ì˜¤ë¥˜ ìˆ˜ì •\n",
        "- API í•¨ìˆ˜ ì •ìƒ ì‘ë™ í…ŒìŠ¤íŠ¸  \n",
        "- ë©€í‹°í”„ë¡œì„¸ì‹± í™˜ê²½ ìµœì í™”\n",
        "- ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ê¸°ëŠ¥ ê²€ì¦\n",
        "\n",
        "### ğŸš€ **ë°”ë¡œ ì‹¤í–‰ ê°€ëŠ¥:**\n",
        "1. Google Colabì— ì—…ë¡œë“œ\n",
        "2. ëŸ°íƒ€ì„ ì—°ê²° í›„ ìˆœì°¨ ì‹¤í–‰\n",
        "3. ì• êµ­ê°€ ë¶„ì„ë¶€í„° ì „ì²´ íŒŒì‹±ê¹Œì§€ ë‹¨ê³„ë³„ í•™ìŠµ\n",
        "\n",
        "**ğŸ’¡ ê¶Œì¥ ì‹¤í–‰ ìˆœì„œ**: ê¸°ë³¸ íŒŒì‹± â†’ API ë³´ê°• â†’ ëŒ€ëŸ‰ ì²˜ë¦¬ â†’ ì „ì²´ íŒŒì‹±\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## ğŸ”§ í™˜ê²½ ì„¤ì •\n",
        "\n",
        "ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ì„í¬íŠ¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_libraries",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9a1408-3dad-41c6-86eb-02d7b2ea6e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mwxml\n",
            "  Downloading mwxml-0.3.6-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from mwxml) (4.25.1)\n",
            "Collecting mwcli>=0.0.2 (from mwxml)\n",
            "  Downloading mwcli-0.0.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting mwtypes>=0.4.0 (from mwxml)\n",
            "  Downloading mwtypes-0.4.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting para>=0.0.1 (from mwxml)\n",
            "  Downloading para-0.0.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.27.1)\n",
            "Collecting docopt (from mwcli>=0.0.2->mwxml)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonable>=0.3.0 (from mwtypes>=0.4.0->mwxml)\n",
            "  Downloading jsonable-0.3.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.5.1->mwxml) (4.15.0)\n",
            "Downloading mwxml-0.3.6-py2.py3-none-any.whl (33 kB)\n",
            "Downloading mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading mwtypes-0.4.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading para-0.0.8-py3-none-any.whl (6.5 kB)\n",
            "Downloading jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=d05a2c530570b21cbdc935cfe7f27da781371df35b63c03b0b7c36486c0f9586\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built docopt\n",
            "Installing collected packages: para, jsonable, docopt, mwtypes, mwcli, mwxml\n",
            "Successfully installed docopt-0.6.2 jsonable-0.3.1 mwcli-0.0.3 mwtypes-0.4.0 mwxml-0.3.6 para-0.0.8\n",
            "âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\n",
            "ğŸ–¥ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´: 2ê°œ\n"
          ]
        }
      ],
      "source": [
        "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
        "!pip install mwxml requests tqdm pandas\n",
        "\n",
        "# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "import mwxml\n",
        "import bz2\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "print(\"âœ… ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "print(f\"ğŸ–¥ï¸  ì‚¬ìš© ê°€ëŠ¥í•œ CPU ì½”ì–´: {mp.cpu_count()}ê°œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## ğŸ“¥ ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
        "\n",
        "í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œ ë¤í”„ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "download_dump",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969ed330-feaa-4e37-e2d5-7b069464712c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¥ ìœ„í‚¤ë¬¸í—Œ ë¤í”„ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 150MB, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\n",
            "--2025-09-11 02:40:22--  https://dumps.wikimedia.org/kowikisource/20250901/kowikisource-20250901-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154956955 (148M) [application/octet-stream]\n",
            "Saving to: â€˜kowikisource-20250901-pages-articles.xml.bz2â€™\n",
            "\n",
            "kowikisource-202509 100%[===================>] 147.78M  4.23MB/s    in 35s     \n",
            "\n",
            "2025-09-11 02:40:58 (4.22 MB/s) - â€˜kowikisource-20250901-pages-articles.xml.bz2â€™ saved [154956955/154956955]\n",
            "\n",
            "âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\n",
            "ğŸ“ íŒŒì¼ í¬ê¸°: 147.8 MB\n"
          ]
        }
      ],
      "source": [
        "# ìœ„í‚¤ë¬¸í—Œ ë¤í”„ ë‹¤ìš´ë¡œë“œ\n",
        "dump_url = \"https://dumps.wikimedia.org/kowikisource/20250901/kowikisource-20250901-pages-articles.xml.bz2\"\n",
        "dump_file = \"kowikisource-20250901-pages-articles.xml.bz2\"\n",
        "\n",
        "if not os.path.exists(dump_file):\n",
        "    print(\"ğŸ“¥ ìœ„í‚¤ë¬¸í—Œ ë¤í”„ ë‹¤ìš´ë¡œë“œ ì¤‘... (ì•½ 150MB, ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)\")\n",
        "    !wget -O {dump_file} {dump_url}\n",
        "    print(\"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
        "else:\n",
        "    print(\"âœ… ë¤í”„ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤!\")\n",
        "\n",
        "# íŒŒì¼ í¬ê¸° í™•ì¸\n",
        "file_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "print(f\"ğŸ“ íŒŒì¼ í¬ê¸°: {file_size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_functions_section"
      },
      "source": [
        "## ğŸŒ API ë³´ê°• í•¨ìˆ˜ë“¤\n",
        "\n",
        "ìœ„í‚¤ë¬¸í—Œ APIì™€ ìœ„í‚¤ë°ì´í„° APIë¥¼ ì‚¬ìš©í•´ì„œ ëˆ„ë½ëœ ì •ë³´ë¥¼ ë³´ê°•í•˜ëŠ” í•¨ìˆ˜ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "api_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05be5958-e828-4959-97bd-8df443949517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API ë³´ê°• í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "def get_categories_from_api(page_title):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤ë¬¸í—Œ APIì—ì„œ í˜ì´ì§€ì˜ ëª¨ë“  ë¶„ë¥˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤\n",
        "\n",
        "    ì´ í•¨ìˆ˜ê°€ ì¤‘ìš”í•œ ì´ìœ :\n",
        "    - XML ë¤í”„ì—ëŠ” ê¸°ë³¸ ë¶„ë¥˜ë§Œ ìˆìŒ\n",
        "    - í…œí”Œë¦¿ì—ì„œ ìë™ ìƒì„±ë˜ëŠ” ë¶„ë¥˜ëŠ” APIë¡œë§Œ í™•ì¸ ê°€ëŠ¥\n",
        "    - ì˜ˆ: '1941ë…„ ì‘í’ˆ', 'PD-old-50' ë“±\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'categories',\n",
        "            'cllimit': 'max'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return []\n",
        "\n",
        "        categories = pages[page_id].get('categories', [])\n",
        "        category_names = []\n",
        "\n",
        "        for cat in categories:\n",
        "            cat_title = cat['title']\n",
        "            if cat_title.startswith('ë¶„ë¥˜:'):\n",
        "                category_names.append(cat_title[3:])  # 'ë¶„ë¥˜:' ì œê±°\n",
        "\n",
        "        return category_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"API ë¶„ë¥˜ ì¡°íšŒ ì˜¤ë¥˜ ({page_title}): {e}\")\n",
        "        return []\n",
        "\n",
        "def get_year_from_wikidata(page_title):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤ë°ì´í„°ì—ì„œ ì‘í’ˆì˜ ë°œí‘œ ì—°ë„ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤\n",
        "\n",
        "    ìœ„í‚¤ë°ì´í„° ì—°ë™ì˜ ì¥ì :\n",
        "    - êµ¬ì¡°í™”ëœ ì—°ë„ ì •ë³´ ì œê³µ\n",
        "    - ì—¬ëŸ¬ ì–¸ì–´íŒì—ì„œ ê³µìœ ë˜ëŠ” ì •í™•í•œ ë°ì´í„°\n",
        "    - ë¶„ë¥˜ ì •ë³´ì™€ êµì°¨ ê²€ì¦ ê°€ëŠ¥\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1ë‹¨ê³„: ìœ„í‚¤ë¬¸í—Œ í˜ì´ì§€ì—ì„œ ìœ„í‚¤ë°ì´í„° ID ê°€ì ¸ì˜¤ê¸°\n",
        "        wikisource_api = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'pageprops'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(wikisource_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return None\n",
        "\n",
        "        wikidata_id = pages[page_id].get('pageprops', {}).get('wikibase_item')\n",
        "\n",
        "        if not wikidata_id:\n",
        "            return None\n",
        "\n",
        "        # 2ë‹¨ê³„: ìœ„í‚¤ë°ì´í„°ì—ì„œ ë°œí‘œì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "        wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'format': 'json',\n",
        "            'ids': wikidata_id,\n",
        "            'props': 'claims'\n",
        "        }\n",
        "\n",
        "        response = requests.get(wikidata_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        entity = data.get('entities', {}).get(wikidata_id, {})\n",
        "        claims = entity.get('claims', {})\n",
        "\n",
        "        # ë°œí‘œ ê´€ë ¨ ì†ì„±ë“¤ í™•ì¸\n",
        "        date_properties = ['P577', 'P571', 'P585']  # ë°œí‘œì¼, ì‹œì‘ì¼, íŠ¹ì •ì‹œì \n",
        "\n",
        "        for prop in date_properties:\n",
        "            if prop in claims:\n",
        "                for claim in claims[prop]:\n",
        "                    try:\n",
        "                        time_value = claim['mainsnak']['datavalue']['value']['time']\n",
        "                        # +1941-00-00T00:00:00Z í˜•íƒœì—ì„œ ì—°ë„ ì¶”ì¶œ\n",
        "                        year = int(time_value[1:5])\n",
        "                        if 1800 <= year <= 2030:  # ìœ íš¨í•œ ì—°ë„ ë²”ìœ„\n",
        "                            return year\n",
        "                    except (KeyError, ValueError):\n",
        "                        continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ìœ„í‚¤ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜ ({page_title}): {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"âœ… API ë³´ê°• í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parsing_functions_section"
      },
      "source": [
        "## ğŸ” í…ìŠ¤íŠ¸ íŒŒì‹± í•¨ìˆ˜ë“¤\n",
        "\n",
        "ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ê³  ê¹”ë”í•œ ë³¸ë¬¸ì„ ë§Œë“œëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "parsing_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de72222a-fc65-40ad-9959-5407c46ddd61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… í…ìŠ¤íŠ¸ íŒŒì‹± í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "def extract_metadata(text):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
        "\n",
        "    ì¶”ì¶œí•˜ëŠ” ì •ë³´:\n",
        "    - ì €ì: [[ì €ì:ì´ë¦„]] íŒ¨í„´ì—ì„œ\n",
        "    - ë¶„ë¥˜: [[ë¶„ë¥˜:ì´ë¦„]] íŒ¨í„´ì—ì„œ\n",
        "    - ì‘ê³¡ê°€: 'ì‘ê³¡' í‚¤ì›Œë“œ ì£¼ë³€ì—ì„œ\n",
        "    - ë¼ì´ì„ ìŠ¤: {{PD-*}} í…œí”Œë¦¿ì—ì„œ\n",
        "    - ì–¸ì–´: 'í•œì', 'í•œê¸€' í‚¤ì›Œë“œì—ì„œ\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            'authors': [],\n",
        "            'categories': [],\n",
        "            'composer': None,\n",
        "            'translator': None,\n",
        "            'year': None,\n",
        "            'license': None,\n",
        "            'language': None\n",
        "        }\n",
        "\n",
        "    # ì €ì ì •ë³´ ì¶”ì¶œ\n",
        "    authors = []\n",
        "    author_patterns = [\n",
        "        r'\\[\\[ì €ì:([^|\\]]+)',  # [[ì €ì:ì´ë¦„]] ë˜ëŠ” [[ì €ì:ì´ë¦„|í‘œì‹œëª…]]\n",
        "        r'\\|\\s*author\\s*=\\s*\\[\\[ì €ì:([^|\\]]+)',  # author= ë§¤ê°œë³€ìˆ˜\n",
        "    ]\n",
        "\n",
        "    for pattern in author_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        authors.extend(matches)\n",
        "\n",
        "    # ë¶„ë¥˜ ì •ë³´ ì¶”ì¶œ\n",
        "    categories = re.findall(r'\\[\\[ë¶„ë¥˜:([^\\]]+)\\]\\]', text)\n",
        "\n",
        "    # ì‘ê³¡ê°€ ì •ë³´ ì¶”ì¶œ\n",
        "    composer = None\n",
        "    composer_patterns = [\n",
        "        r'([^.]+)\\s*ì‘ê³¡',\n",
        "        r'ì‘ê³¡ê°€?\\s*[:=]\\s*([^.\\n]+)'\n",
        "    ]\n",
        "    for pattern in composer_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            composer = matches[0].strip()\n",
        "            break\n",
        "\n",
        "    # ì—°ë„ ì •ë³´ ì¶”ì¶œ\n",
        "    year_matches = re.findall(r'(\\d{4})ë…„', text)\n",
        "    year = None\n",
        "    if year_matches:\n",
        "        year = max(set(year_matches), key=year_matches.count)\n",
        "\n",
        "    # ë¼ì´ì„ ìŠ¤ ì •ë³´ ì¶”ì¶œ\n",
        "    license_info = None\n",
        "    license_patterns = [\n",
        "        r'\\{\\{(PD-[^}]+)\\}\\}',\n",
        "        r'\\{\\{(CC-[^}]+)\\}\\}'\n",
        "    ]\n",
        "    for pattern in license_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            license_info = matches[0]\n",
        "            break\n",
        "\n",
        "    # ì–¸ì–´ ì •ë³´ ì¶”ì¶œ\n",
        "    language = None\n",
        "    if 'í•œì' in text and 'í•œê¸€' in text:\n",
        "        language = 'í•œì+í•œê¸€'\n",
        "    elif 'í•œì' in text:\n",
        "        language = 'í•œì'\n",
        "    elif 'í•œê¸€' in text:\n",
        "        language = 'í•œê¸€'\n",
        "\n",
        "    # ì¤‘ë³µ ì œê±°\n",
        "    authors = list(set([a.strip() for a in authors if a.strip()]))\n",
        "    categories = list(set([c.strip() for c in categories if c.strip()]))\n",
        "\n",
        "    return {\n",
        "        'authors': authors,\n",
        "        'categories': categories,\n",
        "        'composer': composer,\n",
        "        'translator': None,  # ë‚˜ì¤‘ì— í™•ì¥ ê°€ëŠ¥\n",
        "        'year': year,\n",
        "        'license': license_info,\n",
        "        'language': language\n",
        "    }\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë§ˆí¬ì—…ì„ ì œê±°í•˜ê³  ê¹”ë”í•œ ë³¸ë¬¸ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤\n",
        "\n",
        "    ì œê±°í•˜ëŠ” ê²ƒë“¤:\n",
        "    - í…œí”Œë¦¿: {{...}}\n",
        "    - HTML íƒœê·¸: <div>, <br> ë“±\n",
        "    - ìœ„í‚¤ ë§í¬: [[...]]\n",
        "    - ë§ˆí¬ì—…: ''', ''\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # í…œí”Œë¦¿ ì œê±°\n",
        "    cleaned = re.sub(r'\\{\\{[^{}]*\\}\\}', '', text)\n",
        "\n",
        "    # ë§í¬ì—ì„œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ\n",
        "    cleaned = re.sub(r'\\[\\[[^|\\]]*\\|([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "\n",
        "    # HTML íƒœê·¸ ì œê±°\n",
        "    cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
        "\n",
        "    # ìœ„í‚¤ ë§ˆí¬ì—… ì œê±°\n",
        "    cleaned = re.sub(r\"'''([^']+)'''\", r'\\1', cleaned)  # êµµì€ ê¸€ì”¨\n",
        "    cleaned = re.sub(r\"''([^']+)''\", r'\\1', cleaned)   # ê¸°ìš¸ì„\n",
        "\n",
        "    # ì„¹ì…˜ í—¤ë” ì œê±°\n",
        "    cleaned = re.sub(r'^=+\\s*([^=]+)\\s*=+$', r'\\1', cleaned, flags=re.MULTILINE)\n",
        "\n",
        "    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_urls(title, authors):\n",
        "    \"\"\"\n",
        "    í˜ì´ì§€ì™€ ì €ìì˜ URLì„ ìƒì„±í•©ë‹ˆë‹¤\n",
        "    \"\"\"\n",
        "    base_url = \"https://ko.wikisource.org/wiki/\"\n",
        "\n",
        "    # í˜ì´ì§€ URL\n",
        "    page_url = base_url + urllib.parse.quote(title.replace(' ', '_'))\n",
        "\n",
        "    # ì €ì URLë“¤\n",
        "    author_links = []\n",
        "    for author in authors:\n",
        "        author_url = base_url + urllib.parse.quote(f\"ì €ì:{author}\".replace(' ', '_'))\n",
        "        author_links.append({\n",
        "            'name': author,\n",
        "            'url': author_url\n",
        "        })\n",
        "\n",
        "    return page_url, author_links\n",
        "\n",
        "print(\"âœ… í…ìŠ¤íŠ¸ íŒŒì‹± í•¨ìˆ˜ë“¤ì´ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhancement_section"
      },
      "source": [
        "## ğŸš€ API ë³´ê°• í•¨ìˆ˜\n",
        "\n",
        "ë¤í”„ì—ì„œ íŒŒì‹±í•œ ë°ì´í„°ë¥¼ APIë¡œ ë³´ê°•í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "enhancement_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ade02bd-5f31-4ad9-a257-789510c89aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… API ë³´ê°• í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "def enhance_with_api(page_data):\n",
        "    \"\"\"\n",
        "    í˜ì´ì§€ ë°ì´í„°ë¥¼ API ì •ë³´ë¡œ ë³´ê°•í•©ë‹ˆë‹¤\n",
        "\n",
        "    ë³´ê°• ê³¼ì •:\n",
        "    1. APIì—ì„œ ì™„ì „í•œ ë¶„ë¥˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    2. ìœ„í‚¤ë°ì´í„°ì—ì„œ ì—°ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    3. ë¶„ë¥˜ì—ì„œ ì—°ë„ ì¶”ì¶œí•˜ê¸°\n",
        "    4. ìµœì¢… ì—°ë„ ê²°ì • (ë¶„ë¥˜ > ìœ„í‚¤ë°ì´í„° > ë¤í”„)\n",
        "    \"\"\"\n",
        "    title = page_data['title']\n",
        "\n",
        "    # APIì—ì„œ ì™„ì „í•œ ë¶„ë¥˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    api_categories = get_categories_from_api(title)\n",
        "\n",
        "    # ìœ„í‚¤ë°ì´í„°ì—ì„œ ì—°ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    wikidata_year = get_year_from_wikidata(title)\n",
        "\n",
        "    # ê¸°ì¡´ ë°ì´í„° ë³µì‚¬\n",
        "    enhanced = page_data.copy()\n",
        "\n",
        "    # ë¶„ë¥˜ ì •ë³´ ë³‘í•© (ì¤‘ë³µ ì œê±°)\n",
        "    all_categories = list(set(page_data.get('categories', []) + api_categories))\n",
        "    enhanced['categories'] = all_categories\n",
        "    enhanced['api_categories'] = api_categories\n",
        "\n",
        "    # ë¶„ë¥˜ì—ì„œ ì—°ë„ ì¶”ì¶œ\n",
        "    year_from_categories = None\n",
        "    year_categories = [cat for cat in all_categories if 'ë…„' in cat and 'ì‘í’ˆ' in cat]\n",
        "    if year_categories:\n",
        "        for cat in year_categories:\n",
        "            year_match = re.search(r'(\\d{4})ë…„', cat)\n",
        "            if year_match:\n",
        "                year_from_categories = int(year_match.group(1))\n",
        "                break\n",
        "\n",
        "    # ìµœì¢… ì—°ë„ ê²°ì • (ìš°ì„ ìˆœìœ„: ë¶„ë¥˜ > ìœ„í‚¤ë°ì´í„° > ë¤í”„)\n",
        "    enhanced['year'] = (\n",
        "        year_from_categories or\n",
        "        wikidata_year or\n",
        "        page_data.get('year')\n",
        "    )\n",
        "\n",
        "    # ë³´ê°• ì •ë³´ ì¶”ê°€\n",
        "    enhanced['year_from_categories'] = year_from_categories\n",
        "    enhanced['year_from_wikidata'] = wikidata_year\n",
        "\n",
        "    return enhanced\n",
        "\n",
        "print(\"âœ… API ë³´ê°• í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_parser_section"
      },
      "source": [
        "## âš¡ ë©”ì¸ íŒŒì„œ (ë©€í‹°í”„ë¡œì„¸ì‹±)\n",
        "\n",
        "ëª¨ë“  CPU ì½”ì–´ë¥¼ ì‚¬ìš©í•´ì„œ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ” ë©”ì¸ íŒŒì„œì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "main_parser",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f282dfb6-6ee7-4271-d9e6-b6faa196bbeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ë©”ì¸ íŒŒì„œê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "def process_pages_batch(pages_batch):\n",
        "    \"\"\"\n",
        "    í˜ì´ì§€ ë°°ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì›Œì»¤ í•¨ìˆ˜\n",
        "    (ë©€í‹°í”„ë¡œì„¸ì‹±ì—ì„œ ê° ì½”ì–´ê°€ ì‹¤í–‰)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for page_data in pages_batch:\n",
        "        try:\n",
        "            page_id, title, namespace, redirect, revisions = page_data\n",
        "\n",
        "            if not revisions:\n",
        "                continue\n",
        "\n",
        "            # ìµœì‹  ë¦¬ë¹„ì „ ì‚¬ìš©\n",
        "            revision = revisions[0]\n",
        "            revision_id, timestamp, username, comment, text, size = revision\n",
        "\n",
        "            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
        "            metadata = extract_metadata(text)\n",
        "\n",
        "            # ë³¸ë¬¸ ì •ë¦¬\n",
        "            clean_content = clean_text(text)\n",
        "\n",
        "            # URL ìƒì„±\n",
        "            page_url, author_links = generate_urls(title, metadata['authors'])\n",
        "\n",
        "            # í˜ì´ì§€ ë°ì´í„° êµ¬ì„±\n",
        "            page_result = {\n",
        "                'page_id': page_id,\n",
        "                'title': title,\n",
        "                'url': page_url,\n",
        "                'namespace': namespace,\n",
        "                'redirect': redirect,\n",
        "\n",
        "                # í•„ìˆ˜ ë©”íƒ€ë°ì´í„°\n",
        "                'authors': metadata['authors'],\n",
        "                'author_links': author_links,\n",
        "                'categories': metadata['categories'],\n",
        "                'content': clean_content,\n",
        "                'raw_content': text,\n",
        "\n",
        "                # ì¶”ê°€ ë©”íƒ€ë°ì´í„°\n",
        "                'composer': metadata['composer'],\n",
        "                'translator': metadata['translator'],\n",
        "                'year': metadata['year'],\n",
        "                'license': metadata['license'],\n",
        "                'language': metadata['language'],\n",
        "\n",
        "                # ë¦¬ë¹„ì „ ì •ë³´\n",
        "                'revision_id': revision_id,\n",
        "                'last_modified': str(timestamp) if timestamp else None,\n",
        "                'last_contributor': username,\n",
        "                'size': len(text) if text else 0,\n",
        "                'content_size': len(clean_content)\n",
        "            }\n",
        "\n",
        "            results.append(page_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def parse_wikisource(dump_file, limit=None, enable_api=False, batch_size=50):\n",
        "    \"\"\"\n",
        "    ìœ„í‚¤ë¬¸í—Œ ë¤í”„ë¥¼ íŒŒì‹±í•©ë‹ˆë‹¤\n",
        "\n",
        "    Args:\n",
        "        dump_file: ë¤í”„ íŒŒì¼ ê²½ë¡œ\n",
        "        limit: ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì œí•œ (Noneì´ë©´ ì „ì²´)\n",
        "        enable_api: API ë³´ê°• ì‚¬ìš© ì—¬ë¶€\n",
        "        batch_size: ë°°ì¹˜ í¬ê¸°\n",
        "\n",
        "    Returns:\n",
        "        list: íŒŒì‹±ëœ í˜ì´ì§€ ë°ì´í„°\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # CPU ì½”ì–´ ìˆ˜ (Colabì—ì„œ ìµœëŒ€ í™œìš©)\n",
        "    max_workers = mp.cpu_count()\n",
        "\n",
        "    print(f\"ğŸš€ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\")\n",
        "    print(f\"  ğŸ“Š CPU ì½”ì–´: {max_workers}ê°œ (ìµœëŒ€ í™œìš©)\")\n",
        "    print(f\"  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}\")\n",
        "    print(f\"  ğŸŒ API ë³´ê°•: {'ì‚¬ìš©' if enable_api else 'ì‚¬ìš© ì•ˆí•¨'}\")\n",
        "\n",
        "    # 1ë‹¨ê³„: ë¤í”„ì—ì„œ ë°ì´í„° ìˆ˜ì§‘\n",
        "    print(\"\\nğŸ“¥ 1ë‹¨ê³„: ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\")\n",
        "    pages_batches = []\n",
        "    current_batch = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"í˜ì´ì§€ ìˆ˜ì§‘\"):\n",
        "            if limit and total_pages >= limit:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # ë¦¬ë¹„ì „ ë°ì´í„° ìˆ˜ì§‘\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break  # ìµœì‹  ë¦¬ë¹„ì „ë§Œ\n",
        "\n",
        "                page_data = (\n",
        "                    page.id,\n",
        "                    page.title,\n",
        "                    page.namespace,\n",
        "                    str(page.redirect.title) if page.redirect else None,\n",
        "                    revisions\n",
        "                )\n",
        "                current_batch.append(page_data)\n",
        "\n",
        "                if len(current_batch) >= batch_size:\n",
        "                    pages_batches.append(current_batch)\n",
        "                    current_batch = []\n",
        "\n",
        "                total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if current_batch:\n",
        "            pages_batches.append(current_batch)\n",
        "\n",
        "    print(f\"âœ… {total_pages}ê°œ í˜ì´ì§€ë¥¼ {len(pages_batches)}ê°œ ë°°ì¹˜ë¡œ ìˆ˜ì§‘\")\n",
        "\n",
        "    # 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬\n",
        "    print(\"\\nâš¡ 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬\")\n",
        "    results = []\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in pages_batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"ë°°ì¹˜ ì²˜ë¦¬\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                results.extend(batch_results)\n",
        "            except Exception as e:\n",
        "                print(f\"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "\n",
        "    # 3ë‹¨ê³„: API ë³´ê°• (ì„ íƒì )\n",
        "    if enable_api and results:\n",
        "        print(\"\\nğŸŒ 3ë‹¨ê³„: API ë³´ê°• ì²˜ë¦¬\")\n",
        "        enhanced_results = []\n",
        "\n",
        "        for page_data in tqdm(results, desc=\"API ë³´ê°•\"):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "                time.sleep(0.1)  # API ì œí•œ ê³ ë ¤\n",
        "            except Exception as e:\n",
        "                print(f\"API ë³´ê°• ì˜¤ë¥˜ ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        results = enhanced_results\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nğŸ‰ íŒŒì‹± ì™„ë£Œ!\")\n",
        "    print(f\"  â±ï¸  ì´ ì‹œê°„: {total_time:.1f}ì´ˆ\")\n",
        "    print(f\"  ğŸ“Š ì²˜ë¦¬ ì†ë„: {len(results)/total_time:.1f} í˜ì´ì§€/ì´ˆ\")\n",
        "    print(f\"  ğŸ“„ ì´ í˜ì´ì§€: {len(results)}ê°œ\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… ë©”ì¸ íŒŒì„œê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_section"
      },
      "source": [
        "## ğŸ¯ ì‹¤ìŠµ 1: ì• êµ­ê°€ ë¶„ì„ (ê¸°ë³¸ íŒŒì‹±)\n",
        "\n",
        "ë¨¼ì € XML ë¤í”„ë§Œìœ¼ë¡œ ì• êµ­ê°€ë¥¼ íŒŒì‹±í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "demo_basic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7879b545-aa8b-40f9-ca24-f4fe8b616ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“š ì‹¤ìŠµ 1: ê¸°ë³¸ XML ë¤í”„ íŒŒì‹±\n",
            "==================================================\n",
            "ğŸš€ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\n",
            "  ğŸ“Š CPU ì½”ì–´: 2ê°œ (ìµœëŒ€ í™œìš©)\n",
            "  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: 1\n",
            "  ğŸŒ API ë³´ê°•: ì‚¬ìš© ì•ˆí•¨\n",
            "\n",
            "ğŸ“¥ 1ë‹¨ê³„: ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "í˜ì´ì§€ ìˆ˜ì§‘: 1it [00:00, 304.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 1ê°œ í˜ì´ì§€ë¥¼ 1ê°œ ë°°ì¹˜ë¡œ ìˆ˜ì§‘\n",
            "\n",
            "âš¡ 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ë°°ì¹˜ ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 60.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‰ íŒŒì‹± ì™„ë£Œ!\n",
            "  â±ï¸  ì´ ì‹œê°„: 0.1ì´ˆ\n",
            "  ğŸ“Š ì²˜ë¦¬ ì†ë„: 11.0 í˜ì´ì§€/ì´ˆ\n",
            "  ğŸ“„ ì´ í˜ì´ì§€: 1ê°œ\n",
            "\n",
            "ğŸ“„ ì œëª©: ì• êµ­ê°€ (ëŒ€í•œë¯¼êµ­)\n",
            "ğŸ‘¤ ì €ì: ['ìœ¤ì¹˜í˜¸']\n",
            "ğŸ“‚ ë¶„ë¥˜: ['ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë˜', 'êµ­ê°€']\n",
            "ğŸµ ì‘ê³¡ê°€: ì•ˆìµíƒœ(å®‰ç›Šæ³°)\n",
            "ğŸ“… ì—°ë„: None\n",
            "ğŸ“œ ë¼ì´ì„ ìŠ¤: PD-old-50\n",
            "ğŸ“ ë³¸ë¬¸ ê¸¸ì´: 426 ë¬¸ì\n",
            "\n",
            "ğŸ“– ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:\n",
            "í•œì í˜¼ìš© ;1 :æ±æµ·ë¬¼ê³¼ ç™½é ­å±±ì´ ë§ˆë¥´ê³  ë‹³ë„ë¡í•˜ëŠë‹˜ì´ ä¿ä½‘í•˜ì‚¬ ìš°ë¦¬ ë‚˜ë¼ è¬æ­² :ç„¡çª®èŠ± ä¸‰åƒé‡Œ è¯éº—æ±Ÿå±±å¤§éŸ“ì‚¬ëŒ å¤§éŸ“ìœ¼ë¡œ ê¸¸ì´ ä¿å…¨í•˜ì„¸ ;2 :å—å±± ìœ„ì— ì € ì†Œë‚˜ë¬´ éµç”²ì„ ë‘ë¥¸ ë“¯ë°”ëŒ ì„œë¦¬ ä¸è®Ší•¨ì€ ìš°ë¦¬ æ°£åƒì¼ì„¸ ;3 :ê°€ì„ í•˜ëŠ˜ ç©ºè±í•œë° ë†’ê³  êµ¬ë¦„ ì—†ì´ë°ì€ ë‹¬ì€ ìš°ë¦¬ ê°€ìŠ´ ä¸€ç‰‡ä¸¹å¿ƒì¼ì„¸ ;4 :ì´ æ°£åƒê³¼ ì´ ë§˜ìœ¼ë¡œ å¿ èª ì„ ë‹¤í•˜ì—¬ê´´ë¡œìš°ë‚˜ ì¦ê±°ìš°ë‚˜ ë‚˜ë¼ ì‚¬ë‘...\n",
            "\n",
            "ğŸ” ë¬¸ì œì  ë°œê²¬:\n",
            "  âŒ ë¶„ë¥˜ê°€ 2ê°œë§Œ ë‚˜ì˜´ (ì‹¤ì œë¡œëŠ” 4ê°œ)\n",
            "  âŒ '1941ë…„ ì‘í’ˆ' ë¶„ë¥˜ê°€ ëˆ„ë½ë¨\n",
            "  âŒ 'PD-old-50' ë¶„ë¥˜ê°€ ëˆ„ë½ë¨\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ì• êµ­ê°€ë§Œ íŒŒì‹± (API ë³´ê°• ì—†ìŒ)\n",
        "print(\"ğŸ“š ì‹¤ìŠµ 1: ê¸°ë³¸ XML ë¤í”„ íŒŒì‹±\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "basic_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=1,  # ì²« ë²ˆì§¸ í˜ì´ì§€ (ì• êµ­ê°€)ë§Œ\n",
        "    enable_api=False,  # API ë³´ê°• ì•ˆí•¨\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "if basic_results:\n",
        "    aegukga_basic = basic_results[0]\n",
        "\n",
        "    print(f\"\\nğŸ“„ ì œëª©: {aegukga_basic['title']}\")\n",
        "    print(f\"ğŸ‘¤ ì €ì: {aegukga_basic['authors']}\")\n",
        "    print(f\"ğŸ“‚ ë¶„ë¥˜: {aegukga_basic['categories']}\")\n",
        "    print(f\"ğŸµ ì‘ê³¡ê°€: {aegukga_basic.get('composer', 'N/A')}\")\n",
        "    print(f\"ğŸ“… ì—°ë„: {aegukga_basic.get('year', 'N/A')}\")\n",
        "    print(f\"ğŸ“œ ë¼ì´ì„ ìŠ¤: {aegukga_basic.get('license', 'N/A')}\")\n",
        "    print(f\"ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {aegukga_basic['content_size']} ë¬¸ì\")\n",
        "\n",
        "    print(f\"\\nğŸ“– ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "    print(f\"{aegukga_basic['content'][:200]}...\")\n",
        "\n",
        "    print(f\"\\nğŸ” ë¬¸ì œì  ë°œê²¬:\")\n",
        "    print(f\"  âŒ ë¶„ë¥˜ê°€ {len(aegukga_basic['categories'])}ê°œë§Œ ë‚˜ì˜´ (ì‹¤ì œë¡œëŠ” 4ê°œ)\")\n",
        "    print(f\"  âŒ '1941ë…„ ì‘í’ˆ' ë¶„ë¥˜ê°€ ëˆ„ë½ë¨\")\n",
        "    print(f\"  âŒ 'PD-old-50' ë¶„ë¥˜ê°€ ëˆ„ë½ë¨\")\n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_api_section"
      },
      "source": [
        "## ğŸŒŸ ì‹¤ìŠµ 2: ì• êµ­ê°€ ë¶„ì„ (API ë³´ê°•)\n",
        "\n",
        "ì´ì œ API ë³´ê°•ì„ ì‚¬ìš©í•´ì„œ ì™„ì „í•œ ì •ë³´ë¥¼ ì¶”ì¶œí•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "demo_api",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5520ae02-12b8-450e-dc9b-a60a9f18a58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒŸ ì‹¤ìŠµ 2: API ë³´ê°• íŒŒì‹±\n",
            "==================================================\n",
            "ğŸš€ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\n",
            "  ğŸ“Š CPU ì½”ì–´: 2ê°œ (ìµœëŒ€ í™œìš©)\n",
            "  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: 1\n",
            "  ğŸŒ API ë³´ê°•: ì‚¬ìš©\n",
            "\n",
            "ğŸ“¥ 1ë‹¨ê³„: ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "í˜ì´ì§€ ìˆ˜ì§‘: 1it [00:00, 712.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 1ê°œ í˜ì´ì§€ë¥¼ 1ê°œ ë°°ì¹˜ë¡œ ìˆ˜ì§‘\n",
            "\n",
            "âš¡ 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ë°°ì¹˜ ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸŒ 3ë‹¨ê³„: API ë³´ê°• ì²˜ë¦¬\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "API ë³´ê°•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‰ íŒŒì‹± ì™„ë£Œ!\n",
            "  â±ï¸  ì´ ì‹œê°„: 0.9ì´ˆ\n",
            "  ğŸ“Š ì²˜ë¦¬ ì†ë„: 1.2 í˜ì´ì§€/ì´ˆ\n",
            "  ğŸ“„ ì´ í˜ì´ì§€: 1ê°œ\n",
            "\n",
            "ğŸ“„ ì œëª©: ì• êµ­ê°€ (ëŒ€í•œë¯¼êµ­)\n",
            "ğŸ‘¤ ì €ì: ['ìœ¤ì¹˜í˜¸']\n",
            "\n",
            "ğŸ“‚ ë¶„ë¥˜ ì •ë³´:\n",
            "  ë¤í”„ì—ì„œ: []\n",
            "  APIì—ì„œ: ['1941ë…„ ì‘í’ˆ', 'PD-old-50', 'êµ­ê°€', 'ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë˜']\n",
            "  ì „ì²´: ['PD-old-50', '1941ë…„ ì‘í’ˆ', 'ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë˜', 'êµ­ê°€']\n",
            "\n",
            "ğŸµ ì‘ê³¡ê°€: ì•ˆìµíƒœ(å®‰ç›Šæ³°)\n",
            "\n",
            "ğŸ“… ì—°ë„ ì •ë³´:\n",
            "  ìµœì¢…: 1941\n",
            "  ë¶„ë¥˜ì—ì„œ: 1941\n",
            "  ìœ„í‚¤ë°ì´í„°ì—ì„œ: 1941\n",
            "\n",
            "ğŸ“œ ë¼ì´ì„ ìŠ¤: PD-old-50\n",
            "ğŸŒ ì–¸ì–´: í•œì+í•œê¸€\n",
            "ğŸ“ ë³¸ë¬¸ ê¸¸ì´: 426 ë¬¸ì\n",
            "\n",
            "ğŸ¯ ëª©í‘œ ë‹¬ì„±:\n",
            "  âœ… 1941ë…„ ì‘í’ˆ ë¶„ë¥˜\n",
            "  âœ… PD-old-50 ë¶„ë¥˜\n",
            "  âœ… 1941ë…„ ì—°ë„ ì¶”ì¶œ\n",
            "  âœ… 4ê°œ ë¶„ë¥˜ ì™„ì „ ì¶”ì¶œ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ì• êµ­ê°€ API ë³´ê°• íŒŒì‹±\n",
        "print(\"ğŸŒŸ ì‹¤ìŠµ 2: API ë³´ê°• íŒŒì‹±\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "api_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=1,  # ì²« ë²ˆì§¸ í˜ì´ì§€ (ì• êµ­ê°€)ë§Œ\n",
        "    enable_api=True,  # API ë³´ê°• ì‚¬ìš©!\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "if api_results:\n",
        "    aegukga_api = api_results[0]\n",
        "\n",
        "    print(f\"\\nğŸ“„ ì œëª©: {aegukga_api['title']}\")\n",
        "    print(f\"ğŸ‘¤ ì €ì: {aegukga_api['authors']}\")\n",
        "\n",
        "    # ë¶„ë¥˜ ë¹„êµ\n",
        "    dump_cats = [cat for cat in aegukga_api['categories'] if cat not in aegukga_api.get('api_categories', [])]\n",
        "    api_cats = aegukga_api.get('api_categories', [])\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ë¶„ë¥˜ ì •ë³´:\")\n",
        "    print(f\"  ë¤í”„ì—ì„œ: {dump_cats}\")\n",
        "    print(f\"  APIì—ì„œ: {api_cats}\")\n",
        "    print(f\"  ì „ì²´: {aegukga_api['categories']}\")\n",
        "\n",
        "    print(f\"\\nğŸµ ì‘ê³¡ê°€: {aegukga_api.get('composer', 'N/A')}\")\n",
        "\n",
        "    # ì—°ë„ ì •ë³´ ìƒì„¸\n",
        "    print(f\"\\nğŸ“… ì—°ë„ ì •ë³´:\")\n",
        "    print(f\"  ìµœì¢…: {aegukga_api.get('year', 'N/A')}\")\n",
        "    print(f\"  ë¶„ë¥˜ì—ì„œ: {aegukga_api.get('year_from_categories', 'N/A')}\")\n",
        "    print(f\"  ìœ„í‚¤ë°ì´í„°ì—ì„œ: {aegukga_api.get('year_from_wikidata', 'N/A')}\")\n",
        "\n",
        "    print(f\"\\nğŸ“œ ë¼ì´ì„ ìŠ¤: {aegukga_api.get('license', 'N/A')}\")\n",
        "    print(f\"ğŸŒ ì–¸ì–´: {aegukga_api.get('language', 'N/A')}\")\n",
        "    print(f\"ğŸ“ ë³¸ë¬¸ ê¸¸ì´: {aegukga_api['content_size']} ë¬¸ì\")\n",
        "\n",
        "    # ì„±ê³µ í™•ì¸\n",
        "    success_checks = [\n",
        "        ('âœ…' if '1941ë…„ ì‘í’ˆ' in api_cats else 'âŒ', '1941ë…„ ì‘í’ˆ ë¶„ë¥˜'),\n",
        "        ('âœ…' if 'PD-old-50' in api_cats else 'âŒ', 'PD-old-50 ë¶„ë¥˜'),\n",
        "        ('âœ…' if aegukga_api.get('year') == 1941 else 'âŒ', '1941ë…„ ì—°ë„ ì¶”ì¶œ'),\n",
        "        ('âœ…' if len(api_cats) >= 4 else 'âŒ', '4ê°œ ë¶„ë¥˜ ì™„ì „ ì¶”ì¶œ')\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ¯ ëª©í‘œ ë‹¬ì„±:\")\n",
        "    for status, description in success_checks:\n",
        "        print(f\"  {status} {description}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison_section"
      },
      "source": [
        "## ğŸ“Š ë¹„êµ ë¶„ì„\n",
        "\n",
        "ê¸°ë³¸ íŒŒì‹±ê³¼ API ë³´ê°• íŒŒì‹±ì˜ ì°¨ì´ì ì„ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "comparison",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ccc41f-3d13-4112-b22b-16da9264556d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š ê¸°ë³¸ íŒŒì‹± vs API ë³´ê°• íŒŒì‹± ë¹„êµ\n",
            "============================================================\n",
            "\n",
            "ğŸ“‚ ë¶„ë¥˜ ì •ë³´:\n",
            "  ê¸°ë³¸ íŒŒì‹±: 2ê°œ â†’ ['ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë˜', 'êµ­ê°€']\n",
            "  API ë³´ê°•: 4ê°œ â†’ ['PD-old-50', '1941ë…„ ì‘í’ˆ', 'ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë˜', 'êµ­ê°€']\n",
            "  ê°œì„ : +2ê°œ ë¶„ë¥˜ ì¶”ê°€\n",
            "\n",
            "ğŸ“… ì—°ë„ ì •ë³´:\n",
            "  ê¸°ë³¸ íŒŒì‹±: None\n",
            "  API ë³´ê°•: 1941 (ë¶„ë¥˜: 1941, ìœ„í‚¤ë°ì´í„°: 1941)\n",
            "\n",
            "ğŸ¯ í•µì‹¬ ì„±ê³¼:\n",
            "  âœ… ì¶”ê°€ëœ ë¶„ë¥˜: ['PD-old-50', '1941ë…„ ì‘í’ˆ']\n",
            "  âœ… ì—°ë„ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: 1941ë…„\n",
            "  âœ… ë¶„ë¥˜ì™€ ìœ„í‚¤ë°ì´í„° ì—°ë„ ì¼ì¹˜: 1941ë…„\n",
            "\n",
            "ğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:\n",
            "  1. XML ë¤í”„ë§Œìœ¼ë¡œëŠ” í…œí”Œë¦¿ ê¸°ë°˜ ë¶„ë¥˜ë¥¼ ë†“ì¹  ìˆ˜ ìˆìŒ\n",
            "  2. API ë³´ê°•ìœ¼ë¡œ ì›¹ì‚¬ì´íŠ¸ì™€ ë™ì¼í•œ ì™„ì „í•œ ì •ë³´ ì¶”ì¶œ ê°€ëŠ¥\n",
            "  3. ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì—°ë„ ì •ë³´ë¥¼ êµì°¨ ê²€ì¦í•˜ì—¬ ì •í™•ì„± í–¥ìƒ\n",
            "  4. êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„°ë¡œ í›„ì† ë¶„ì„ ì‘ì—… ìš©ì´\n"
          ]
        }
      ],
      "source": [
        "print(\"ğŸ“Š ê¸°ë³¸ íŒŒì‹± vs API ë³´ê°• íŒŒì‹± ë¹„êµ\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if basic_results and api_results:\n",
        "    basic = basic_results[0]\n",
        "    enhanced = api_results[0]\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ë¶„ë¥˜ ì •ë³´:\")\n",
        "    print(f\"  ê¸°ë³¸ íŒŒì‹±: {len(basic['categories'])}ê°œ â†’ {basic['categories']}\")\n",
        "    print(f\"  API ë³´ê°•: {len(enhanced['categories'])}ê°œ â†’ {enhanced['categories']}\")\n",
        "    print(f\"  ê°œì„ : +{len(enhanced['categories']) - len(basic['categories'])}ê°œ ë¶„ë¥˜ ì¶”ê°€\")\n",
        "\n",
        "    print(f\"\\nğŸ“… ì—°ë„ ì •ë³´:\")\n",
        "    print(f\"  ê¸°ë³¸ íŒŒì‹±: {basic.get('year', 'None')}\")\n",
        "    print(f\"  API ë³´ê°•: {enhanced.get('year', 'None')} (ë¶„ë¥˜: {enhanced.get('year_from_categories')}, ìœ„í‚¤ë°ì´í„°: {enhanced.get('year_from_wikidata')})\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ í•µì‹¬ ì„±ê³¼:\")\n",
        "    missing_categories = set(enhanced['categories']) - set(basic['categories'])\n",
        "    if missing_categories:\n",
        "        print(f\"  âœ… ì¶”ê°€ëœ ë¶„ë¥˜: {list(missing_categories)}\")\n",
        "\n",
        "    if enhanced.get('year') and not basic.get('year'):\n",
        "        print(f\"  âœ… ì—°ë„ ì •ë³´ ì¶”ì¶œ ì„±ê³µ: {enhanced.get('year')}ë…„\")\n",
        "\n",
        "    if enhanced.get('year_from_categories') == enhanced.get('year_from_wikidata'):\n",
        "        print(f\"  âœ… ë¶„ë¥˜ì™€ ìœ„í‚¤ë°ì´í„° ì—°ë„ ì¼ì¹˜: {enhanced.get('year')}ë…„\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ë¹„êµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "print(f\"\\nğŸ’¡ í•™ìŠµ í¬ì¸íŠ¸:\")\n",
        "print(f\"  1. XML ë¤í”„ë§Œìœ¼ë¡œëŠ” í…œí”Œë¦¿ ê¸°ë°˜ ë¶„ë¥˜ë¥¼ ë†“ì¹  ìˆ˜ ìˆìŒ\")\n",
        "print(f\"  2. API ë³´ê°•ìœ¼ë¡œ ì›¹ì‚¬ì´íŠ¸ì™€ ë™ì¼í•œ ì™„ì „í•œ ì •ë³´ ì¶”ì¶œ ê°€ëŠ¥\")\n",
        "print(f\"  3. ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì—°ë„ ì •ë³´ë¥¼ êµì°¨ ê²€ì¦í•˜ì—¬ ì •í™•ì„± í–¥ìƒ\")\n",
        "print(f\"  4. êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„°ë¡œ í›„ì† ë¶„ì„ ì‘ì—… ìš©ì´\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bulk_processing_section"
      },
      "source": [
        "## ğŸ”„ ì‹¤ìŠµ 3: ëŒ€ëŸ‰ ì²˜ë¦¬ (ì„ íƒì )\n",
        "\n",
        "ë” ë§ì€ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•´ë³´ê³  ì‹¶ë‹¤ë©´ ì•„ë˜ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bulk_processing",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df7c595-f9ac-4a9b-ff8a-4249b7e27b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ ì‹¤ìŠµ 3: ëŒ€ëŸ‰ ì²˜ë¦¬\n",
            "==================================================\n",
            "ğŸ“Š ì„¤ì •:\n",
            "  ì²˜ë¦¬ í˜ì´ì§€: 50ê°œ\n",
            "  API ë³´ê°•: ì‚¬ìš©\n",
            "  CPU í™œìš©: 2ê°œ ì½”ì–´ ìµœëŒ€ í™œìš©\n",
            "ğŸš€ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\n",
            "  ğŸ“Š CPU ì½”ì–´: 2ê°œ (ìµœëŒ€ í™œìš©)\n",
            "  ğŸ“¦ ë°°ì¹˜ í¬ê¸°: 10\n",
            "  ğŸŒ API ë³´ê°•: ì‚¬ìš©\n",
            "\n",
            "ğŸ“¥ 1ë‹¨ê³„: ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "í˜ì´ì§€ ìˆ˜ì§‘: 50it [00:00, 119.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… 50ê°œ í˜ì´ì§€ë¥¼ 5ê°œ ë°°ì¹˜ë¡œ ìˆ˜ì§‘\n",
            "\n",
            "âš¡ 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "ë°°ì¹˜ ì²˜ë¦¬: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:11<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸŒ 3ë‹¨ê³„: API ë³´ê°• ì²˜ë¦¬\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "API ë³´ê°•: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [00:25<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‰ íŒŒì‹± ì™„ë£Œ!\n",
            "  â±ï¸  ì´ ì‹œê°„: 38.0ì´ˆ\n",
            "  ğŸ“Š ì²˜ë¦¬ ì†ë„: 1.3 í˜ì´ì§€/ì´ˆ\n",
            "  ğŸ“„ ì´ í˜ì´ì§€: 50ê°œ\n",
            "\n",
            "âœ… ëŒ€ëŸ‰ ì²˜ë¦¬ ì™„ë£Œ!\n",
            "ì²˜ë¦¬ëœ í˜ì´ì§€: 50ê°œ\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ğŸš€ ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰\n",
        "print(\"ğŸš€ ì‹¤ìŠµ 3: ëŒ€ëŸ‰ ì²˜ë¦¬\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜ ì„¤ì • (í•„ìš”ì— ë”°ë¼ ì¡°ì •)\n",
        "BULK_LIMIT = 50  # 50ê°œ í˜ì´ì§€ ì²˜ë¦¬\n",
        "API_ENHANCEMENT = True  # API ë³´ê°• ì‚¬ìš© ì—¬ë¶€\n",
        "\n",
        "print(f\"ğŸ“Š ì„¤ì •:\")\n",
        "print(f\"  ì²˜ë¦¬ í˜ì´ì§€: {BULK_LIMIT}ê°œ\")\n",
        "print(f\"  API ë³´ê°•: {'ì‚¬ìš©' if API_ENHANCEMENT else 'ì‚¬ìš© ì•ˆí•¨'}\")\n",
        "print(f\"  CPU í™œìš©: {mp.cpu_count()}ê°œ ì½”ì–´ ìµœëŒ€ í™œìš©\")\n",
        "\n",
        "# ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰\n",
        "bulk_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=BULK_LIMIT,\n",
        "    enable_api=API_ENHANCEMENT,\n",
        "    batch_size=10  # ë°°ì¹˜ í¬ê¸°\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… ëŒ€ëŸ‰ ì²˜ë¦¬ ì™„ë£Œ!\")\n",
        "print(f\"ì²˜ë¦¬ëœ í˜ì´ì§€: {len(bulk_results)}ê°œ\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "## ğŸ“Š ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\n",
        "\n",
        "ì²˜ë¦¬ëœ ë°ì´í„°ì˜ í†µê³„ì™€ ìƒ˜í”Œì„ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "save_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477e7d12-b826-4207-b222-31eb05f2e33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\n",
            "==================================================\n",
            "ğŸ“ˆ ì²˜ë¦¬ í†µê³„:\n",
            "  ì´ í˜ì´ì§€: 50ê°œ\n",
            "  API ë³´ê°•ëœ í˜ì´ì§€: 28ê°œ (56.0%)\n",
            "  ì—°ë„ ì •ë³´ ìˆëŠ” í˜ì´ì§€: 26ê°œ (52.0%)\n",
            "  ì €ì ì •ë³´ ìˆëŠ” í˜ì´ì§€: 8ê°œ (16.0%)\n",
            "  ì‹¤ì§ˆì  ë³¸ë¬¸ ìˆëŠ” í˜ì´ì§€: 42ê°œ (84.0%)\n",
            "\n",
            "ğŸ“‚ ìƒìœ„ 10ê°œ ë¶„ë¥˜:\n",
            "  PD-South Korea-exempt: 11ê°œ\n",
            "  ì—°ë„ ë¯¸ì…ë ¥ ì‘í’ˆ: 6ê°œ\n",
            "  ë„ì›€ë§: 4ê°œ\n",
            "  ë™ìŒì´ì˜ì–´ ë¬¸ì„œ: 4ê°œ\n",
            "  PD-old-100: 3ê°œ\n",
            "  ëŒ€í•œë¯¼êµ­ì˜ ì¼ë¶€ê°œì •ëœ ë²•ë¥ : 3ê°œ\n",
            "  ëŒ€í•œë¯¼êµ­ì˜ ê°œì •ëœ ë²•ë¥ : 3ê°œ\n",
            "  PD-old-70: 3ê°œ\n",
            "  í•œê¸€: 2ê°œ\n",
            "  ëŒ€í•œë¯¼êµ­ì˜ íƒ€ë²•ê°œì •ëœ ë²•ë¥ : 2ê°œ\n",
            "\n",
            "ğŸ“… ì—°ë„ë³„ ë¶„í¬:\n",
            "  1429ë…„: 1ê°œ\n",
            "  1446ë…„: 1ê°œ\n",
            "  1446ë…„: 1ê°œ\n",
            "  1459ë…„: 1ê°œ\n",
            "  1541ë…„: 1ê°œ\n",
            "  1926ë…„: 1ê°œ\n",
            "  1933ë…„: 1ê°œ\n",
            "  1934ë…„: 1ê°œ\n",
            "  1936ë…„: 1ê°œ\n",
            "  1936ë…„: 1ê°œ\n",
            "  1941ë…„: 1ê°œ\n",
            "  1943ë…„: 1ê°œ\n",
            "  1948ë…„: 1ê°œ\n",
            "  1955ë…„: 1ê°œ\n",
            "  1956ë…„: 1ê°œ\n",
            "  1959ë…„: 1ê°œ\n",
            "  1962ë…„: 1ê°œ\n",
            "  1965ë…„: 2ê°œ\n",
            "  1987ë…„: 1ê°œ\n",
            "  1998ë…„: 1ê°œ\n",
            "  2003ë…„: 1ê°œ\n",
            "  2004ë…„: 1ê°œ\n",
            "  2010ë…„: 1ê°œ\n",
            "  2014ë…„: 1ê°œ\n",
            "  2019ë…„: 1ê°œ\n",
            "\n",
            "ğŸ“ ìƒ˜í”Œ í˜ì´ì§€ (ìƒìœ„ 10ê°œ):\n",
            "   1. ê¸°ë…êµ ê°•ìš”\n",
            "      ë¶„ë¥˜: 1ê°œ, ì—°ë„: 1541, ë³¸ë¬¸: 2782ì\n",
            "   2. í›ˆë¯¼ì •ìŒ\n",
            "      ë¶„ë¥˜: 3ê°œ, ì—°ë„: 1446, ë³¸ë¬¸: 5551ì\n",
            "   3. ëŒ€ë¬¸\n",
            "      ë¶„ë¥˜: 0ê°œ, ì—°ë„: 1955, ë³¸ë¬¸: 909ì\n",
            "   4. ìë§¤í”„ë¡œì íŠ¸\n",
            "      ë¶„ë¥˜: 0ê°œ, ì—°ë„: None, ë³¸ë¬¸: 1511ì\n",
            "   5. ìœ„í‚¤ë¯¸ë””ì–´ ì¬ë‹¨\n",
            "      ë¶„ë¥˜: 1ê°œ, ì—°ë„: 2003, ë³¸ë¬¸: 1175ì\n",
            "   6. ë„ì›€ë§\n",
            "      ë¶„ë¥˜: 1ê°œ, ì—°ë„: None, ë³¸ë¬¸: 1220ì\n",
            "   7. FAQ\n",
            "      ë¶„ë¥˜: 1ê°œ, ì—°ë„: None, ë³¸ë¬¸: 4023ì\n",
            "   8. ì €ì‘ê¶Œë²• (ëŒ€í•œë¯¼êµ­, ì œ7233í˜¸)\n",
            "      ë¶„ë¥˜: 4ê°œ, ì—°ë„: 2004, ë³¸ë¬¸: 37104ì\n",
            "   9. í™˜ê²½ë³´í˜¸ë²•\n",
            "      ë¶„ë¥˜: 1ê°œ, ì—°ë„: None, ë³¸ë¬¸: 115ì\n",
            "  10. í›ˆë¯¼ì •ìŒì–¸í•´\n",
            "      ë¶„ë¥˜: 4ê°œ, ì—°ë„: 1459, ë³¸ë¬¸: 257ì\n"
          ]
        }
      ],
      "source": [
        "if 'bulk_results' in locals() and bulk_results:\n",
        "    print(\"ğŸ“Š ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ë¶„ì„\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # ê¸°ë³¸ í†µê³„\n",
        "    total_pages = len(bulk_results)\n",
        "    api_enhanced = sum(1 for page in bulk_results if page.get('api_categories'))\n",
        "    year_found = sum(1 for page in bulk_results if page.get('year'))\n",
        "    authors_found = sum(1 for page in bulk_results if page.get('authors'))\n",
        "    content_pages = sum(1 for page in bulk_results if page.get('content_size', 0) > 100)\n",
        "\n",
        "    print(f\"ğŸ“ˆ ì²˜ë¦¬ í†µê³„:\")\n",
        "    print(f\"  ì´ í˜ì´ì§€: {total_pages}ê°œ\")\n",
        "    print(f\"  API ë³´ê°•ëœ í˜ì´ì§€: {api_enhanced}ê°œ ({api_enhanced/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì—°ë„ ì •ë³´ ìˆëŠ” í˜ì´ì§€: {year_found}ê°œ ({year_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì €ì ì •ë³´ ìˆëŠ” í˜ì´ì§€: {authors_found}ê°œ ({authors_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì‹¤ì§ˆì  ë³¸ë¬¸ ìˆëŠ” í˜ì´ì§€: {content_pages}ê°œ ({content_pages/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë¶„ë¥˜ í†µê³„\n",
        "    all_categories = []\n",
        "    for page in bulk_results:\n",
        "        all_categories.extend(page.get('categories', []))\n",
        "\n",
        "    category_counts = {}\n",
        "    for cat in all_categories:\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "    top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ìƒìœ„ 10ê°œ ë¶„ë¥˜:\")\n",
        "    for cat, count in top_categories:\n",
        "        print(f\"  {cat}: {count}ê°œ\")\n",
        "\n",
        "    # ì—°ë„ë³„ í†µê³„\n",
        "    years = [page.get('year') for page in bulk_results if page.get('year')]\n",
        "    if years:\n",
        "        year_counts = {}\n",
        "        for year in years:\n",
        "            year_counts[year] = year_counts.get(year, 0) + 1\n",
        "\n",
        "        print(f\"\\nğŸ“… ì—°ë„ë³„ ë¶„í¬:\")\n",
        "        sorted_years = sorted(year_counts.items(), key=lambda x: int(x[0]) if str(x[0]).isdigit() else 0)\n",
        "        for year, count in sorted_years:\n",
        "            print(f\"  {year}ë…„: {count}ê°œ\")\n",
        "\n",
        "    # ìƒ˜í”Œ í˜ì´ì§€ í‘œì‹œ\n",
        "    print(f\"\\nğŸ“ ìƒ˜í”Œ í˜ì´ì§€ (ìƒìœ„ 10ê°œ):\")\n",
        "    for i, page in enumerate(bulk_results[:10], 1):\n",
        "        title = page['title']\n",
        "        categories_count = len(page.get('categories', []))\n",
        "        year = page.get('year', 'N/A')\n",
        "        content_size = page.get('content_size', 0)\n",
        "\n",
        "        print(f\"  {i:2d}. {title}\")\n",
        "        print(f\"      ë¶„ë¥˜: {categories_count}ê°œ, ì—°ë„: {year}, ë³¸ë¬¸: {content_size}ì\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ìœ„ì˜ ëŒ€ëŸ‰ ì²˜ë¦¬ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ (CSV + JSON)\n",
        "\n",
        "ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ì—¬ í›„ì† ë¶„ì„ì— í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r8nQ09hEkhzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484e0240-6d03-435b-aa5d-08ad3cc2e642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… ëŒ€ëŸ‰ ì €ì¥ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\n"
          ]
        }
      ],
      "source": [
        "def save_bulk_results(data, base_filename=\"kowikisource_bulk\"):\n",
        "    \"\"\"\n",
        "    ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥\n",
        "\n",
        "    ì €ì¥ í˜•ì‹:\n",
        "    1. ì™„ì „í•œ JSON (ëª¨ë“  ë©”íƒ€ë°ì´í„° í¬í•¨)\n",
        "    2. ìš”ì•½ CSV (í•µì‹¬ ì •ë³´ë§Œ)\n",
        "    3. ìƒì„¸ CSV (API ë³´ê°• ì •ë³´ í¬í•¨)\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        print(\"âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "        return\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    print(f\"ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì¤‘...\")\n",
        "    print(f\"ğŸ“Š ë°ì´í„°: {len(data)}ê°œ í˜ì´ì§€\")\n",
        "\n",
        "    # 1. ì™„ì „í•œ JSON ì €ì¥ (ëª¨ë“  ë°ì´í„°)\n",
        "    json_filename = f\"{base_filename}_{timestamp}_complete.json\"\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    file_size_mb = os.path.getsize(json_filename) / (1024 * 1024)\n",
        "    print(f\"  âœ… ì™„ì „í•œ JSON: {json_filename} ({file_size_mb:.1f}MB)\")\n",
        "\n",
        "    # 2. ìš”ì•½ CSV ì €ì¥ (í•µì‹¬ ì •ë³´ë§Œ)\n",
        "    summary_csv_data = []\n",
        "    for page in data:\n",
        "        summary_csv_data.append({\n",
        "            'page_id': page['page_id'],\n",
        "            'title': page['title'],\n",
        "            'url': page['url'],\n",
        "            'namespace': page['namespace'],\n",
        "            'authors': ', '.join(page['authors']) if page['authors'] else '',\n",
        "            'categories': ', '.join(page['categories']) if page['categories'] else '',\n",
        "            'composer': page.get('composer', ''),\n",
        "            'translator': page.get('translator', ''),\n",
        "            'year': page.get('year', ''),\n",
        "            'license': page.get('license', ''),\n",
        "            'language': page.get('language', ''),\n",
        "            'content_length': page.get('content_size', 0),\n",
        "            'last_modified': page.get('last_modified', ''),\n",
        "            'last_contributor': page.get('last_contributor', '')\n",
        "        })\n",
        "\n",
        "    summary_csv_filename = f\"{base_filename}_{timestamp}_summary.csv\"\n",
        "    df_summary = pd.DataFrame(summary_csv_data)\n",
        "    df_summary.to_csv(summary_csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"  âœ… ìš”ì•½ CSV: {summary_csv_filename}\")\n",
        "\n",
        "    # 3. ìƒì„¸ CSV ì €ì¥ (API ë³´ê°• ì •ë³´ í¬í•¨)\n",
        "    detailed_csv_data = []\n",
        "    for page in data:\n",
        "        # API ë³´ê°• ì •ë³´ ë¶„ë¦¬\n",
        "        dump_categories = [cat for cat in page.get('categories', []) if cat not in page.get('api_categories', [])]\n",
        "        api_categories = page.get('api_categories', [])\n",
        "\n",
        "        detailed_csv_data.append({\n",
        "            'page_id': page['page_id'],\n",
        "            'title': page['title'],\n",
        "            'url': page['url'],\n",
        "            'namespace': page['namespace'],\n",
        "            'authors': ', '.join(page['authors']) if page['authors'] else '',\n",
        "            'dump_categories': ', '.join(dump_categories),\n",
        "            'api_categories': ', '.join(api_categories),\n",
        "            'all_categories': ', '.join(page['categories']) if page['categories'] else '',\n",
        "            'composer': page.get('composer', ''),\n",
        "            'year_final': page.get('year', ''),\n",
        "            'year_from_categories': page.get('year_from_categories', ''),\n",
        "            'year_from_wikidata': page.get('year_from_wikidata', ''),\n",
        "            'license': page.get('license', ''),\n",
        "            'language': page.get('language', ''),\n",
        "            'content_length': page.get('content_size', 0),\n",
        "            'raw_content_length': page.get('size', 0),\n",
        "            'last_modified': page.get('last_modified', ''),\n",
        "            'last_contributor': page.get('last_contributor', '')\n",
        "        })\n",
        "\n",
        "    detailed_csv_filename = f\"{base_filename}_{timestamp}_detailed.csv\"\n",
        "    df_detailed = pd.DataFrame(detailed_csv_data)\n",
        "    df_detailed.to_csv(detailed_csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"  âœ… ìƒì„¸ CSV: {detailed_csv_filename}\")\n",
        "\n",
        "    # 4. ë©”íƒ€ë°ì´í„° ìš”ì•½ ì €ì¥\n",
        "    metadata = {\n",
        "        'generation_info': {\n",
        "            'timestamp': timestamp,\n",
        "            'total_pages': len(data),\n",
        "            'api_enhanced_pages': sum(1 for page in data if page.get('api_categories')),\n",
        "            'pages_with_year': sum(1 for page in data if page.get('year')),\n",
        "            'pages_with_authors': sum(1 for page in data if page.get('authors')),\n",
        "            'pages_with_content': sum(1 for page in data if page.get('content_size', 0) > 100)\n",
        "        },\n",
        "        'files_generated': {\n",
        "            'complete_json': json_filename,\n",
        "            'summary_csv': summary_csv_filename,\n",
        "            'detailed_csv': detailed_csv_filename\n",
        "        }\n",
        "    }\n",
        "\n",
        "    metadata_filename = f\"{base_filename}_{timestamp}_metadata.json\"\n",
        "    with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"  âœ… ë©”íƒ€ë°ì´í„°: {metadata_filename}\")\n",
        "\n",
        "    print(f\"\\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:\")\n",
        "    print(f\"  ğŸ“„ {json_filename} - ì™„ì „í•œ ë°ì´í„° (JSON)\")\n",
        "    print(f\"  ğŸ“Š {summary_csv_filename} - í•µì‹¬ ì •ë³´ (CSV)\")\n",
        "    print(f\"  ğŸ“ˆ {detailed_csv_filename} - API ë³´ê°• ì •ë³´ í¬í•¨ (CSV)\")\n",
        "    print(f\"  â„¹ï¸  {metadata_filename} - ìƒì„± ì •ë³´ (JSON)\")\n",
        "\n",
        "    return {\n",
        "        'json_file': json_filename,\n",
        "        'summary_csv': summary_csv_filename,\n",
        "        'detailed_csv': detailed_csv_filename,\n",
        "        'metadata_file': metadata_filename\n",
        "    }\n",
        "\n",
        "print(\"âœ… ëŒ€ëŸ‰ ì €ì¥ í•¨ìˆ˜ê°€ ì •ì˜ë˜ì—ˆìŠµë‹ˆë‹¤!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ETxyif4WkhzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f29ab9-ab52-40a2-e565-3b77190b3b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...\n",
            "ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì¤‘...\n",
            "ğŸ“Š ë°ì´í„°: 50ê°œ í˜ì´ì§€\n",
            "  âœ… ì™„ì „í•œ JSON: kowikisource_bulk_20250911_024137_complete.json (3.3MB)\n",
            "  âœ… ìš”ì•½ CSV: kowikisource_bulk_20250911_024137_summary.csv\n",
            "  âœ… ìƒì„¸ CSV: kowikisource_bulk_20250911_024137_detailed.csv\n",
            "  âœ… ë©”íƒ€ë°ì´í„°: kowikisource_bulk_20250911_024137_metadata.json\n",
            "\n",
            "ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:\n",
            "  ğŸ“„ kowikisource_bulk_20250911_024137_complete.json - ì™„ì „í•œ ë°ì´í„° (JSON)\n",
            "  ğŸ“Š kowikisource_bulk_20250911_024137_summary.csv - í•µì‹¬ ì •ë³´ (CSV)\n",
            "  ğŸ“ˆ kowikisource_bulk_20250911_024137_detailed.csv - API ë³´ê°• ì •ë³´ í¬í•¨ (CSV)\n",
            "  â„¹ï¸  kowikisource_bulk_20250911_024137_metadata.json - ìƒì„± ì •ë³´ (JSON)\n",
            "\n",
            "ğŸ¯ íŒŒì¼ í™œìš© ê°€ì´ë“œ:\n",
            "  ğŸ“Š pandas ë¶„ì„:\n",
            "     df = pd.read_csv('kowikisource_bulk_20250911_024137_summary.csv')\n",
            "     df.head()\n",
            "\n",
            "  ğŸ” ìƒì„¸ ë¶„ì„:\n",
            "     detailed_df = pd.read_csv('kowikisource_bulk_20250911_024137_detailed.csv')\n",
            "     # API ë³´ê°• íš¨ê³¼ í™•ì¸\n",
            "     detailed_df[['dump_categories', 'api_categories']].head()\n",
            "\n",
            "  ğŸ“„ ì™„ì „í•œ ë°ì´í„°:\n",
            "     with open('kowikisource_bulk_20250911_024137_complete.json', 'r') as f:\n",
            "         data = json.load(f)\n",
            "\n",
            "âœ… ì €ì¥ ì™„ë£Œ í™•ì¸:\n",
            "  ğŸ“Š CSV íŒŒì¼: 50í–‰ Ã— 14ì—´\n",
            "     ì£¼ìš” ì»¬ëŸ¼: page_id, title, url, namespace, authors...\n",
            "  ğŸ“„ JSON íŒŒì¼: 50ê°œ í•­ëª©\n",
            "     í‚¤ ê°œìˆ˜: 23ê°œ\n"
          ]
        }
      ],
      "source": [
        "# ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ ì €ì¥ ì‹¤í–‰\n",
        "if 'bulk_results' in locals() and bulk_results:\n",
        "    print(\"ğŸ’¾ ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì—¬ëŸ¬ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤...\")\n",
        "    saved_files = save_bulk_results(bulk_results)\n",
        "\n",
        "    print(f\"\\nğŸ¯ íŒŒì¼ í™œìš© ê°€ì´ë“œ:\")\n",
        "    print(f\"  ğŸ“Š pandas ë¶„ì„:\")\n",
        "    print(f\"     df = pd.read_csv('{saved_files['summary_csv']}')\")\n",
        "    print(f\"     df.head()\")\n",
        "\n",
        "    print(f\"\\n  ğŸ” ìƒì„¸ ë¶„ì„:\")\n",
        "    print(f\"     detailed_df = pd.read_csv('{saved_files['detailed_csv']}')\")\n",
        "    print(f\"     # API ë³´ê°• íš¨ê³¼ í™•ì¸\")\n",
        "    print(f\"     detailed_df[['dump_categories', 'api_categories']].head()\")\n",
        "\n",
        "    print(f\"\\n  ğŸ“„ ì™„ì „í•œ ë°ì´í„°:\")\n",
        "    print(f\"     with open('{saved_files['json_file']}', 'r') as f:\")\n",
        "    print(f\"         data = json.load(f)\")\n",
        "\n",
        "    # ê°„ë‹¨í•œ ë°ì´í„° ê²€ì¦\n",
        "    print(f\"\\nâœ… ì €ì¥ ì™„ë£Œ í™•ì¸:\")\n",
        "\n",
        "    # CSV íŒŒì¼ ê²€ì¦\n",
        "    try:\n",
        "        test_df = pd.read_csv(saved_files['summary_csv'])\n",
        "        print(f\"  ğŸ“Š CSV íŒŒì¼: {len(test_df)}í–‰ Ã— {len(test_df.columns)}ì—´\")\n",
        "        print(f\"     ì£¼ìš” ì»¬ëŸ¼: {', '.join(test_df.columns[:5])}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ CSV ê²€ì¦ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "    # JSON íŒŒì¼ ê²€ì¦\n",
        "    try:\n",
        "        with open(saved_files['json_file'], 'r', encoding='utf-8') as f:\n",
        "            test_json = json.load(f)\n",
        "        print(f\"  ğŸ“„ JSON íŒŒì¼: {len(test_json)}ê°œ í•­ëª©\")\n",
        "        if test_json:\n",
        "            print(f\"     í‚¤ ê°œìˆ˜: {len(test_json[0].keys())}ê°œ\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ JSON ê²€ì¦ ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ì €ì¥í•  ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ğŸ’¡ ìœ„ì˜ 'ëŒ€ëŸ‰ ì²˜ë¦¬ ì‹¤í–‰' ì…€ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2eTJaEMkhzW"
      },
      "source": [
        "## ğŸ” ì• êµ­ê°€ ê°€ì‚¬ ê²€ì¦\n",
        "\n",
        "ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ì• êµ­ê°€ ë³¸ë¬¸(ê°€ì‚¬)ì´ ì œëŒ€ë¡œ íŒŒì‹±ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xoWQ6sq5khzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835e5e89-c229-4984-e1d4-104d6f2ac6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” ì• êµ­ê°€ ê°€ì‚¬ ê²€ì¦\n",
            "==================================================\n",
            "ğŸ“„ í˜ì´ì§€: ì• êµ­ê°€ (ëŒ€í•œë¯¼êµ­)\n",
            "ğŸŒ URL: https://ko.wikisource.org/wiki/%EC%95%A0%EA%B5%AD%EA%B0%80_%28%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%29\n",
            "ğŸ“ ì „ì²´ ë³¸ë¬¸ ê¸¸ì´: 426 ë¬¸ì\n",
            "ğŸ“ ì›ë³¸ ê¸¸ì´: 866 ë¬¸ì\n",
            "\n",
            "ğŸ“– ì •ì œëœ ë³¸ë¬¸:\n",
            "í•œì í˜¼ìš© ;1 :æ±æµ·ë¬¼ê³¼ ç™½é ­å±±ì´ ë§ˆë¥´ê³  ë‹³ë„ë¡í•˜ëŠë‹˜ì´ ä¿ä½‘í•˜ì‚¬ ìš°ë¦¬ ë‚˜ë¼ è¬æ­² :ç„¡çª®èŠ± ä¸‰åƒé‡Œ è¯éº—æ±Ÿå±±å¤§éŸ“ì‚¬ëŒ å¤§éŸ“ìœ¼ë¡œ ê¸¸ì´ ä¿å…¨í•˜ì„¸ ;2 :å—å±± ìœ„ì— ì € ì†Œë‚˜ë¬´ éµç”²ì„ ë‘ë¥¸ ë“¯ë°”ëŒ ì„œë¦¬ ä¸è®Ší•¨ì€ ìš°ë¦¬ æ°£åƒì¼ì„¸ ;3 :ê°€ì„ í•˜ëŠ˜ ç©ºè±í•œë° ë†’ê³  êµ¬ë¦„ ì—†ì´ë°ì€ ë‹¬ì€ ìš°ë¦¬ ê°€ìŠ´ ä¸€ç‰‡ä¸¹å¿ƒì¼ì„¸ ;4 :ì´ æ°£åƒê³¼ ì´ ë§˜ìœ¼ë¡œ å¿ èª ì„ ë‹¤í•˜ì—¬ê´´ë¡œìš°ë‚˜ ì¦ê±°ìš°ë‚˜ ë‚˜ë¼ ì‚¬ë‘í•˜ì„¸ í•œê¸€ ;1 :ë™í•´ ë¬¼ê³¼ ë°±ë‘ì‚°ì´ ë§ˆë¥´ê³  ë‹³ë„ë¡í•˜ëŠë‹˜ì´ ë³´ìš°í•˜ì‚¬ ìš°ë¦¬ë‚˜ë¼ ë§Œì„¸ :ë¬´ê¶í™” ì‚¼ì²œë¦¬ í™”ë ¤ê°•ì‚°ëŒ€í•œ ì‚¬ëŒ ëŒ€í•œìœ¼ë¡œ ê¸¸ì´ ë³´ì „í•˜ì„¸ ;2 :ë‚¨ì‚° ìœ„ì— ì € ì†Œë‚˜ë¬´ ì² ê°‘ì„ ë‘ë¥¸ ë“¯ë°”ëŒ ì„œë¦¬ ë¶ˆë³€í•¨ì€ ìš°ë¦¬ ê¸°ìƒì¼ì„¸ ;3 :ê°€ì„ í•˜ëŠ˜ ê³µí™œí•œë° ë†’ê³  êµ¬ë¦„ ì—†ì´ë°ì€ ë‹¬ì€ ìš°ë¦¬ ê°€ìŠ´ ì¼í¸ë‹¨ì‹¬ì¼ì„¸ ;4 :ì´ ê¸°ìƒê³¼ ì´ ë§˜ìœ¼ë¡œ ì¶©ì„±ì„ ë‹¤í•˜ì—¬ê´´ë¡œìš°ë‚˜ ì¦ê±°ìš°ë‚˜ ë‚˜ë¼ ì‚¬ë‘í•˜ì„¸ ë¼ì´ì„ ìŠ¤ ë¶„ë¥˜:êµ­ê°€ ë¶„ë¥˜:ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë˜\n",
            "\n",
            "ğŸµ ê°€ì‚¬ í‚¤ì›Œë“œ í™•ì¸:\n",
            "  âŒ 'ë™í•´ë¬¼ê³¼' ì—†ìŒ\n",
            "  âœ… 'ë°±ë‘ì‚°ì´' ë°œê²¬ë¨\n",
            "  âœ… 'í•˜ëŠë‹˜ì´' ë°œê²¬ë¨\n",
            "  âœ… 'ë³´ìš°í•˜ì‚¬' ë°œê²¬ë¨\n",
            "  âœ… 'ë¬´ê¶í™”' ë°œê²¬ë¨\n",
            "\n",
            "ğŸ”¤ ì–¸ì–´ ë¶„ì„:\n",
            "  âœ… í•œì êµ¬ë¬¸ ë°œê²¬: 17ê°œ\n",
            "     ì˜ˆì‹œ: æ±æµ·, ç™½é ­å±±, ä¿ä½‘...\n",
            "  âœ… í•œê¸€ êµ¬ë¬¸ ë°œê²¬: 94ê°œ\n",
            "     ì˜ˆì‹œ: í•œì, í˜¼ìš©, ë¬¼ê³¼...\n",
            "\n",
            "ğŸ“ ê°€ì‚¬ êµ¬ì¡°:\n",
            "  ì „ì²´ ì¤„ ìˆ˜: 1ì¤„\n",
            "  ì˜ë¯¸ìˆëŠ” ì¤„: 1ì¤„\n",
            "\n",
            "ğŸ¼ ê°€ì‚¬ ë‚´ìš© (ì²˜ìŒ 10ì¤„):\n",
            "   1. í•œì í˜¼ìš© ;1 :æ±æµ·ë¬¼ê³¼ ç™½é ­å±±ì´ ë§ˆë¥´ê³  ë‹³ë„ë¡í•˜ëŠë‹˜ì´ ä¿ä½‘í•˜ì‚¬ ìš°ë¦¬ ë‚˜ë¼ è¬æ­² :ç„¡çª®èŠ± ä¸‰åƒé‡Œ è¯éº—æ±Ÿå±±å¤§éŸ“ì‚¬ëŒ å¤§éŸ“ìœ¼ë¡œ ê¸¸ì´ ä¿å…¨í•˜ì„¸ ;2 :å—å±± ìœ„ì— ì € ì†Œë‚˜ë¬´ éµç”²ì„ ë‘ë¥¸ ë“¯ë°”ëŒ ì„œë¦¬ ä¸è®Ší•¨ì€ ìš°ë¦¬ æ°£åƒì¼ì„¸ ;3 :ê°€ì„ í•˜ëŠ˜ ç©ºè±í•œë° ë†’ê³  êµ¬ë¦„ ì—†ì´ë°ì€ ë‹¬ì€ ìš°ë¦¬ ê°€ìŠ´ ä¸€ç‰‡ä¸¹å¿ƒì¼ì„¸ ;4 :ì´ æ°£åƒê³¼ ì´ ë§˜ìœ¼ë¡œ å¿ èª ì„ ë‹¤í•˜ì—¬ê´´ë¡œìš°ë‚˜ ì¦ê±°ìš°ë‚˜ ë‚˜ë¼ ì‚¬ë‘í•˜ì„¸ í•œê¸€ ;1 :ë™í•´ ë¬¼ê³¼ ë°±ë‘ì‚°ì´ ë§ˆë¥´ê³  ë‹³ë„ë¡í•˜ëŠë‹˜ì´ ë³´ìš°í•˜ì‚¬ ìš°ë¦¬ë‚˜ë¼ ë§Œì„¸ :ë¬´ê¶í™” ì‚¼ì²œë¦¬ í™”ë ¤ê°•ì‚°ëŒ€í•œ ì‚¬ëŒ ëŒ€í•œìœ¼ë¡œ ê¸¸ì´ ë³´ì „í•˜ì„¸ ;2 :ë‚¨ì‚° ìœ„ì— ì € ì†Œë‚˜ë¬´ ì² ê°‘ì„ ë‘ë¥¸ ë“¯ë°”ëŒ ì„œë¦¬ ë¶ˆë³€í•¨ì€ ìš°ë¦¬ ê¸°ìƒì¼ì„¸ ;3 :ê°€ì„ í•˜ëŠ˜ ê³µí™œí•œë° ë†’ê³  êµ¬ë¦„ ì—†ì´ë°ì€ ë‹¬ì€ ìš°ë¦¬ ê°€ìŠ´ ì¼í¸ë‹¨ì‹¬ì¼ì„¸ ;4 :ì´ ê¸°ìƒê³¼ ì´ ë§˜ìœ¼ë¡œ ì¶©ì„±ì„ ë‹¤í•˜ì—¬ê´´ë¡œìš°ë‚˜ ì¦ê±°ìš°ë‚˜ ë‚˜ë¼ ì‚¬ë‘í•˜ì„¸ ë¼ì´ì„ ìŠ¤ ë¶„ë¥˜:êµ­ê°€ ë¶„ë¥˜:ëŒ€í•œë¯¼êµ­ì˜ ë…¸ë˜\n",
            "\n",
            "ğŸ¯ íŒŒì‹± ì„±ê³µ ì—¬ë¶€:\n",
            "  âœ… ì£¼ìš” ê°€ì‚¬ í‚¤ì›Œë“œ 4/5ê°œ ë°œê²¬\n",
            "  âœ… í•œì ê°€ì‚¬ ë³´ì¡´ë¨\n",
            "  âœ… í•œê¸€ ê°€ì‚¬ ë³´ì¡´ë¨\n",
            "  âœ… ì¶©ë¶„í•œ ë³¸ë¬¸ ê¸¸ì´ (426ì)\n",
            "  âŒ ê°€ì‚¬ êµ¬ì¡° ë³´ì¡´ (1ì¤„)\n",
            "\n",
            "âš ï¸  ì¼ë¶€ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ],
      "source": [
        "# ì• êµ­ê°€ ë³¸ë¬¸ ê²€ì¦\n",
        "print(\"ğŸ” ì• êµ­ê°€ ê°€ì‚¬ ê²€ì¦\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# API ë³´ê°• ê²°ê³¼ì—ì„œ ì• êµ­ê°€ í™•ì¸\n",
        "if 'api_results' in locals() and api_results:\n",
        "    aegukga = api_results[0]\n",
        "\n",
        "    print(f\"ğŸ“„ í˜ì´ì§€: {aegukga['title']}\")\n",
        "    print(f\"ğŸŒ URL: {aegukga['url']}\")\n",
        "    print(f\"ğŸ“ ì „ì²´ ë³¸ë¬¸ ê¸¸ì´: {aegukga['content_size']} ë¬¸ì\")\n",
        "    print(f\"ğŸ“ ì›ë³¸ ê¸¸ì´: {aegukga.get('size', 0)} ë¬¸ì\")\n",
        "\n",
        "    # ë³¸ë¬¸ ë‚´ìš© í™•ì¸\n",
        "    content = aegukga['content']\n",
        "    raw_content = aegukga.get('raw_content', '')\n",
        "\n",
        "    print(f\"\\nğŸ“– ì •ì œëœ ë³¸ë¬¸:\")\n",
        "    print(f\"{content}\")\n",
        "\n",
        "    # ê°€ì‚¬ í‚¤ì›Œë“œ í™•ì¸\n",
        "    lyrics_keywords = ['ë™í•´ë¬¼ê³¼', 'ë°±ë‘ì‚°ì´', 'í•˜ëŠë‹˜ì´', 'ë³´ìš°í•˜ì‚¬', 'ë¬´ê¶í™”']\n",
        "    found_keywords = []\n",
        "\n",
        "    for keyword in lyrics_keywords:\n",
        "        if keyword in content:\n",
        "            found_keywords.append(keyword)\n",
        "\n",
        "    print(f\"\\nğŸµ ê°€ì‚¬ í‚¤ì›Œë“œ í™•ì¸:\")\n",
        "    for keyword in lyrics_keywords:\n",
        "        status = 'âœ…' if keyword in content else 'âŒ'\n",
        "        print(f\"  {status} '{keyword}' {'ë°œê²¬ë¨' if keyword in content else 'ì—†ìŒ'}\")\n",
        "\n",
        "    # í•œì/í•œê¸€ ë¶„ì„\n",
        "    import re\n",
        "\n",
        "    # í•œì íŒ¨í„´ í™•ì¸\n",
        "    hanja_pattern = r'[\\u4e00-\\u9fff]+'\n",
        "    hangul_pattern = r'[\\uac00-\\ud7af]+'\n",
        "\n",
        "    hanja_matches = re.findall(hanja_pattern, content)\n",
        "    hangul_matches = re.findall(hangul_pattern, content)\n",
        "\n",
        "    print(f\"\\nğŸ”¤ ì–¸ì–´ ë¶„ì„:\")\n",
        "    print(f\"  âœ… í•œì êµ¬ë¬¸ ë°œê²¬: {len(hanja_matches)}ê°œ\")\n",
        "    if hanja_matches:\n",
        "        print(f\"     ì˜ˆì‹œ: {', '.join(hanja_matches[:3])}...\")\n",
        "\n",
        "    print(f\"  âœ… í•œê¸€ êµ¬ë¬¸ ë°œê²¬: {len(hangul_matches)}ê°œ\")\n",
        "    if hangul_matches:\n",
        "        print(f\"     ì˜ˆì‹œ: {', '.join(hangul_matches[:3])}...\")\n",
        "\n",
        "    # ê°€ì‚¬ êµ¬ì¡° í™•ì¸\n",
        "    lines = [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "    meaningful_lines = [line for line in lines if len(line) > 3]\n",
        "\n",
        "    print(f\"\\nğŸ“ ê°€ì‚¬ êµ¬ì¡°:\")\n",
        "    print(f\"  ì „ì²´ ì¤„ ìˆ˜: {len(lines)}ì¤„\")\n",
        "    print(f\"  ì˜ë¯¸ìˆëŠ” ì¤„: {len(meaningful_lines)}ì¤„\")\n",
        "\n",
        "    print(f\"\\nğŸ¼ ê°€ì‚¬ ë‚´ìš© (ì²˜ìŒ 10ì¤„):\")\n",
        "    for i, line in enumerate(meaningful_lines[:10], 1):\n",
        "        print(f\"  {i:2d}. {line}\")\n",
        "\n",
        "    # ì„±ê³µ ì—¬ë¶€ íŒë‹¨\n",
        "    success_criteria = [\n",
        "        (len(found_keywords) >= 3, f\"ì£¼ìš” ê°€ì‚¬ í‚¤ì›Œë“œ {len(found_keywords)}/5ê°œ ë°œê²¬\"),\n",
        "        (len(hanja_matches) > 0, \"í•œì ê°€ì‚¬ ë³´ì¡´ë¨\"),\n",
        "        (len(hangul_matches) > 0, \"í•œê¸€ ê°€ì‚¬ ë³´ì¡´ë¨\"),\n",
        "        (aegukga['content_size'] > 300, f\"ì¶©ë¶„í•œ ë³¸ë¬¸ ê¸¸ì´ ({aegukga['content_size']}ì)\"),\n",
        "        (len(meaningful_lines) >= 5, f\"ê°€ì‚¬ êµ¬ì¡° ë³´ì¡´ ({len(meaningful_lines)}ì¤„)\")\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nğŸ¯ íŒŒì‹± ì„±ê³µ ì—¬ë¶€:\")\n",
        "    all_success = True\n",
        "    for success, description in success_criteria:\n",
        "        status = 'âœ…' if success else 'âŒ'\n",
        "        print(f\"  {status} {description}\")\n",
        "        if not success:\n",
        "            all_success = False\n",
        "\n",
        "    if all_success:\n",
        "        print(f\"\\nğŸ‰ ì• êµ­ê°€ ê°€ì‚¬ íŒŒì‹± ì™„ë²½ ì„±ê³µ!\")\n",
        "        print(f\"âœ… í•œì ê°€ì‚¬ ë³´ì¡´ë¨\")\n",
        "        print(f\"âœ… í•œê¸€ ê°€ì‚¬ ë³´ì¡´ë¨\")\n",
        "        print(f\"âœ… ê°€ì‚¬ êµ¬ì¡° ì™„ì „ ë³´ì¡´ë¨\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸  ì¼ë¶€ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ì• êµ­ê°€ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ğŸ’¡ ë¨¼ì € 'API ë³´ê°• íŒŒì‹±' ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i6Dn8yukhzX"
      },
      "source": [
        "## ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± (ê³ ê¸‰)\n",
        "\n",
        "**ì£¼ì˜**: ì´ ì„¹ì…˜ì€ ì „ì²´ í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œì„ íŒŒì‹±í•©ë‹ˆë‹¤. ìˆ˜ì²œ ê°œì˜ í˜ì´ì§€ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "### âš ï¸ ì‹¤í–‰ ì „ í™•ì¸ì‚¬í•­:\n",
        "- Colab Pro ì‚¬ìš© ê¶Œì¥ (ë” ë§ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„ ì œí•œ)\n",
        "- ì•ˆì •ì ì¸ ì¸í„°ë„· ì—°ê²° í•„ìš” (API í˜¸ì¶œ)\n",
        "- ì™„ë£Œê¹Œì§€ 30ë¶„-2ì‹œê°„ ì†Œìš” ì˜ˆìƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qqLzRPD_khzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3029417e-dc0e-4252-8432-0d1ebac2fa9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì¤€ë¹„\n",
            "==================================================\n",
            "ğŸ“Š ì„¤ì •:\n",
            "  API ë³´ê°•: âœ…\n",
            "  ë°°ì¹˜ í¬ê¸°: 20\n",
            "  CPU í™œìš©: 2ê°œ ì½”ì–´\n",
            "  ì¤‘ê°„ ì €ì¥: 1000ê°œë§ˆë‹¤\n",
            "\n",
            "ğŸ“ˆ ì˜ˆìƒ ì •ë³´:\n",
            "  ë¤í”„ í¬ê¸°: 147.8MB\n",
            "  ì˜ˆìƒ í˜ì´ì§€: ~1,477ê°œ\n",
            "  ì˜ˆìƒ ì‹œê°„: 15-30ë¶„ (API í¬í•¨)\n",
            "\n",
            "âš ï¸  ì „ì²´ íŒŒì‹±ì„ ì‹¤í–‰í•˜ë ¤ë©´:\n",
            "  1. ì•„ë˜ EXECUTE_FULL_PARSINGì„ Trueë¡œ ë³€ê²½\n",
            "  2. ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰\n",
            "  3. ì§„í–‰ìƒí™©ì„ ëª¨ë‹ˆí„°ë§\n",
            "\n",
            "ğŸ”¥ ì „ì²´ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n"
          ]
        }
      ],
      "source": [
        "# ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì„¤ì •\n",
        "print(\"ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì¤€ë¹„\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# íŒŒì‹± ì„¤ì •\n",
        "FULL_PARSING_CONFIG = {\n",
        "    'enable_api': True,           # API ë³´ê°• ì‚¬ìš© (ê¶Œì¥)\n",
        "    'batch_size': 20,            # ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ê³ ë ¤)\n",
        "    'api_delay': 0.05,           # API í˜¸ì¶œ ê°„ê²© (ì´ˆ)\n",
        "    'save_interval': 1000,       # ì¤‘ê°„ ì €ì¥ ê°„ê²©\n",
        "    'max_retries': 3,            # API ì¬ì‹œë„ íšŸìˆ˜\n",
        "}\n",
        "\n",
        "print(f\"ğŸ“Š ì„¤ì •:\")\n",
        "print(f\"  API ë³´ê°•: {'âœ…' if FULL_PARSING_CONFIG['enable_api'] else 'âŒ'}\")\n",
        "print(f\"  ë°°ì¹˜ í¬ê¸°: {FULL_PARSING_CONFIG['batch_size']}\")\n",
        "print(f\"  CPU í™œìš©: {mp.cpu_count()}ê°œ ì½”ì–´\")\n",
        "print(f\"  ì¤‘ê°„ ì €ì¥: {FULL_PARSING_CONFIG['save_interval']}ê°œë§ˆë‹¤\")\n",
        "\n",
        "# ì˜ˆìƒ ì‹œê°„ ê³„ì‚°\n",
        "import os\n",
        "dump_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "estimated_pages = int(dump_size * 10)  # ëŒ€ëµì  ì¶”ì •\n",
        "\n",
        "print(f\"\\nğŸ“ˆ ì˜ˆìƒ ì •ë³´:\")\n",
        "print(f\"  ë¤í”„ í¬ê¸°: {dump_size:.1f}MB\")\n",
        "print(f\"  ì˜ˆìƒ í˜ì´ì§€: ~{estimated_pages:,}ê°œ\")\n",
        "print(f\"  ì˜ˆìƒ ì‹œê°„: {estimated_pages/100:.0f}-{estimated_pages/50:.0f}ë¶„ (API í¬í•¨)\")\n",
        "\n",
        "# ì‚¬ìš©ì í™•ì¸\n",
        "print(f\"\\nâš ï¸  ì „ì²´ íŒŒì‹±ì„ ì‹¤í–‰í•˜ë ¤ë©´:\")\n",
        "print(f\"  1. ì•„ë˜ EXECUTE_FULL_PARSINGì„ Trueë¡œ ë³€ê²½\")\n",
        "print(f\"  2. ì…€ì„ ë‹¤ì‹œ ì‹¤í–‰\")\n",
        "print(f\"  3. ì§„í–‰ìƒí™©ì„ ëª¨ë‹ˆí„°ë§\")\n",
        "\n",
        "# ì•ˆì „ì¥ì¹˜\n",
        "EXECUTE_FULL_PARSING = True  # ì „ì²´ íŒŒì‹± ì‹¤í–‰í•˜ë ¤ë©´ Trueë¡œ ë³€ê²½\n",
        "\n",
        "if EXECUTE_FULL_PARSING:\n",
        "    print(f\"\\nğŸ”¥ ì „ì²´ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
        "else:\n",
        "    print(f\"\\nğŸ’¡ ì•„ì§ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. EXECUTE_FULL_PARSING = Trueë¡œ ì„¤ì •í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVGMd4vEkhzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0964b74-6744-40f4-e53b-da375304a503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”¥ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤!\n",
            "ì´ ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§„í–‰ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.\n",
            "ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\n",
            "  ğŸ“Š CPU ì½”ì–´: 2ê°œ í™œìš©\n",
            "  ğŸŒ API ë³´ê°•: ì‚¬ìš©\n",
            "  ğŸ’¾ ì¤‘ê°„ ì €ì¥: 1000ê°œë§ˆë‹¤\n",
            "\n",
            "ğŸ“¥ 1ë‹¨ê³„: ì „ì²´ ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ì „ì²´ í˜ì´ì§€ ìˆ˜ì§‘: 19825it [01:14, 503.15it/s]"
          ]
        }
      ],
      "source": [
        "# í–¥ìƒëœ ì „ì²´ íŒŒì‹± í•¨ìˆ˜ (ì¤‘ê°„ ì €ì¥ í¬í•¨)\n",
        "def parse_full_wikisource(dump_file, config):\n",
        "    \"\"\"\n",
        "    ì „ì²´ ìœ„í‚¤ë¬¸í—Œì„ íŒŒì‹±í•˜ê³  ì¤‘ê°„ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"ğŸš€ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ì‹œì‘!\")\n",
        "    print(f\"  ğŸ“Š CPU ì½”ì–´: {mp.cpu_count()}ê°œ í™œìš©\")\n",
        "    print(f\"  ğŸŒ API ë³´ê°•: {'ì‚¬ìš©' if config['enable_api'] else 'ì‚¬ìš© ì•ˆí•¨'}\")\n",
        "    print(f\"  ğŸ’¾ ì¤‘ê°„ ì €ì¥: {config['save_interval']}ê°œë§ˆë‹¤\")\n",
        "\n",
        "    # 1ë‹¨ê³„: ëª¨ë“  í˜ì´ì§€ ìˆ˜ì§‘\n",
        "    print(f\"\\nğŸ“¥ 1ë‹¨ê³„: ì „ì²´ ë¤í”„ ë°ì´í„° ìˆ˜ì§‘\")\n",
        "    all_pages = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"ì „ì²´ í˜ì´ì§€ ìˆ˜ì§‘\"):\n",
        "            try:\n",
        "                # ë¦¬ë¹„ì „ ë°ì´í„° ìˆ˜ì§‘\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break\n",
        "\n",
        "                if revisions:  # ë¦¬ë¹„ì „ì´ ìˆëŠ” í˜ì´ì§€ë§Œ\n",
        "                    page_data = (\n",
        "                        page.id,\n",
        "                        page.title,\n",
        "                        page.namespace,\n",
        "                        str(page.redirect.title) if page.redirect else None,\n",
        "                        revisions\n",
        "                    )\n",
        "                    all_pages.append(page_data)\n",
        "                    total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    print(f\"âœ… ì´ {total_pages:,}ê°œ í˜ì´ì§€ ìˆ˜ì§‘ ì™„ë£Œ\")\n",
        "\n",
        "    # 2ë‹¨ê³„: ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°\n",
        "    batch_size = config['batch_size']\n",
        "    batches = [all_pages[i:i+batch_size] for i in range(0, len(all_pages), batch_size)]\n",
        "    print(f\"ğŸ“¦ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  (ë°°ì¹˜ë‹¹ {batch_size}ê°œ)\")\n",
        "\n",
        "    # 3ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ì²˜ë¦¬\n",
        "    print(f\"\\nâš¡ 2ë‹¨ê³„: ë©€í‹°í”„ë¡œì„¸ì‹± ë°°ì¹˜ ì²˜ë¦¬\")\n",
        "    all_results = []\n",
        "    processed_count = 0\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:\n",
        "        # ëª¨ë“  ë°°ì¹˜ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"ë°°ì¹˜ ì²˜ë¦¬\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                all_results.extend(batch_results)\n",
        "                processed_count += len(batch_results)\n",
        "\n",
        "                # ì¤‘ê°„ ì €ì¥\n",
        "                if processed_count % config['save_interval'] == 0:\n",
        "                    temp_filename = f\"temp_wikisource_{processed_count}.json\"\n",
        "                    with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
        "                    print(f\"\\nğŸ’¾ ì¤‘ê°„ ì €ì¥: {temp_filename} ({processed_count:,}ê°œ í˜ì´ì§€)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"âœ… ê¸°ë³¸ íŒŒì‹± ì™„ë£Œ: {len(all_results):,}ê°œ í˜ì´ì§€\")\n",
        "\n",
        "    # 4ë‹¨ê³„: API ë³´ê°• (ì„ íƒì )\n",
        "    if config['enable_api'] and all_results:\n",
        "        print(f\"\\nğŸŒ 3ë‹¨ê³„: API ë³´ê°• ì²˜ë¦¬\")\n",
        "        enhanced_results = []\n",
        "        api_success = 0\n",
        "\n",
        "        for i, page_data in enumerate(tqdm(all_results, desc=\"API ë³´ê°•\")):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "\n",
        "                # API ì„±ê³µ ì¹´ìš´íŠ¸\n",
        "                if enhanced.get('api_categories'):\n",
        "                    api_success += 1\n",
        "\n",
        "                # API í˜¸ì¶œ ì œí•œ\n",
        "                time.sleep(config['api_delay'])\n",
        "\n",
        "                # ì¤‘ê°„ ì €ì¥ (API ë³´ê°• ë²„ì „)\n",
        "                if (i + 1) % config['save_interval'] == 0:\n",
        "                    temp_filename = f\"temp_wikisource_api_{i+1}.json\"\n",
        "                    with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(enhanced_results, f, ensure_ascii=False, indent=2)\n",
        "                    print(f\"\\nğŸ’¾ API ë³´ê°• ì¤‘ê°„ ì €ì¥: {temp_filename} ({i+1:,}ê°œ í˜ì´ì§€)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"API ë³´ê°• ì˜¤ë¥˜ ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        all_results = enhanced_results\n",
        "        print(f\"âœ… API ë³´ê°• ì™„ë£Œ: {api_success:,}/{len(all_results):,}ê°œ ì„±ê³µ ({api_success/len(all_results)*100:.1f}%)\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nğŸ‰ ì „ì²´ íŒŒì‹± ì™„ë£Œ!\")\n",
        "    print(f\"  â±ï¸  ì´ ì‹œê°„: {total_time/60:.1f}ë¶„\")\n",
        "    print(f\"  ğŸ“Š ì²˜ë¦¬ ì†ë„: {len(all_results)/(total_time/60):.1f} í˜ì´ì§€/ë¶„\")\n",
        "    print(f\"  ğŸ“„ ì´ í˜ì´ì§€: {len(all_results):,}ê°œ\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# ì „ì²´ íŒŒì‹± ì‹¤í–‰\n",
        "if EXECUTE_FULL_PARSING:\n",
        "    print(\"ğŸ”¥ ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹±ì„ ì‹œì‘í•©ë‹ˆë‹¤!\")\n",
        "    print(\"ì´ ì‘ì—…ì€ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì§„í–‰ìƒí™©ì„ í™•ì¸í•˜ì„¸ìš”.\")\n",
        "\n",
        "    full_results = parse_full_wikisource(dump_file, FULL_PARSING_CONFIG)\n",
        "\n",
        "    print(f\"\\nâœ… ì „ì²´ íŒŒì‹±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
        "    print(f\"ë³€ìˆ˜ 'full_results'ì— {len(full_results):,}ê°œ í˜ì´ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
        "\n",
        "else:\n",
        "    print(\"â¸ï¸  ì „ì²´ íŒŒì‹±ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\")\n",
        "    print(\"EXECUTE_FULL_PARSING = Trueë¡œ ì„¤ì •í•˜ê³  ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCGuUXhjkhzY"
      },
      "source": [
        "## ğŸ“Š ì „ì²´ íŒŒì‹± ê²°ê³¼ ë¶„ì„\n",
        "\n",
        "ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹±ì´ ì™„ë£Œëœ í›„ ìƒì„¸í•œ í†µê³„ì™€ ë¶„ì„ì„ í™•ì¸í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "n24XDQHNkhzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa772ee-42ab-4b7a-d7ed-6eb14cc210d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âŒ ì „ì²´ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
            "ğŸ’¡ ë¨¼ì € ì „ì²´ íŒŒì‹±ì„ ì‹¤í–‰í•˜ì„¸ìš”.\n"
          ]
        }
      ],
      "source": [
        "# ì „ì²´ íŒŒì‹± ê²°ê³¼ ë¶„ì„\n",
        "if 'full_results' in locals() and full_results:\n",
        "    print(\"ğŸ“Š ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ê²°ê³¼ ìƒì„¸ ë¶„ì„\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # ê¸°ë³¸ í†µê³„\n",
        "    total_pages = len(full_results)\n",
        "    api_enhanced = sum(1 for page in full_results if page.get('api_categories'))\n",
        "    year_found = sum(1 for page in full_results if page.get('year'))\n",
        "    authors_found = sum(1 for page in full_results if page.get('authors'))\n",
        "    content_pages = sum(1 for page in full_results if page.get('content_size', 0) > 100)\n",
        "    substantial_content = sum(1 for page in full_results if page.get('content_size', 0) > 1000)\n",
        "\n",
        "    print(f\"ğŸ“ˆ ê¸°ë³¸ í†µê³„:\")\n",
        "    print(f\"  ì´ í˜ì´ì§€: {total_pages:,}ê°œ\")\n",
        "    print(f\"  API ë³´ê°•ëœ í˜ì´ì§€: {api_enhanced:,}ê°œ ({api_enhanced/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì—°ë„ ì •ë³´ ìˆëŠ” í˜ì´ì§€: {year_found:,}ê°œ ({year_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì €ì ì •ë³´ ìˆëŠ” í˜ì´ì§€: {authors_found:,}ê°œ ({authors_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ë³¸ë¬¸ ìˆëŠ” í˜ì´ì§€: {content_pages:,}ê°œ ({content_pages/total_pages*100:.1f}%)\")\n",
        "    print(f\"  ì‹¤ì§ˆì  ë³¸ë¬¸(1000ì+): {substantial_content:,}ê°œ ({substantial_content/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶„ì„\n",
        "    namespace_counts = {}\n",
        "    for page in full_results:\n",
        "        ns = page.get('namespace', 0)\n",
        "        namespace_counts[ns] = namespace_counts.get(ns, 0) + 1\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë¶„í¬:\")\n",
        "    for ns, count in sorted(namespace_counts.items()):\n",
        "        ns_name = {0: 'ë³¸ë¬¸', 100: 'ì €ì', 102: 'ìƒ‰ì¸', 104: 'ë²ˆì—­'}.get(ns, f'ê¸°íƒ€({ns})')\n",
        "        print(f\"  {ns_name}: {count:,}ê°œ ({count/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë¶„ë¥˜ ë¶„ì„\n",
        "    all_categories = []\n",
        "    for page in full_results:\n",
        "        all_categories.extend(page.get('categories', []))\n",
        "\n",
        "    category_counts = {}\n",
        "    for cat in all_categories:\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "    top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "    print(f\"\\nğŸ“‚ ìƒìœ„ 20ê°œ ë¶„ë¥˜:\")\n",
        "    for i, (cat, count) in enumerate(top_categories, 1):\n",
        "        print(f\"  {i:2d}. {cat}: {count:,}ê°œ\")\n",
        "\n",
        "    # ì—°ë„ë³„ ë¶„í¬\n",
        "    years = [page.get('year') for page in full_results if page.get('year')]\n",
        "    if years:\n",
        "        year_counts = {}\n",
        "        for year in years:\n",
        "            try:\n",
        "                year_int = int(year)\n",
        "                decade = (year_int // 10) * 10\n",
        "                year_counts[decade] = year_counts.get(decade, 0) + 1\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "\n",
        "        print(f\"\\nğŸ“… ì—°ëŒ€ë³„ ì‘í’ˆ ë¶„í¬:\")\n",
        "        for decade in sorted(year_counts.keys()):\n",
        "            count = year_counts[decade]\n",
        "            print(f\"  {decade}ë…„ëŒ€: {count:,}ê°œ\")\n",
        "\n",
        "    # ì–¸ì–´ ë¶„ì„\n",
        "    language_counts = {}\n",
        "    for page in full_results:\n",
        "        lang = page.get('language', 'ë¯¸ë¶„ë¥˜')\n",
        "        language_counts[lang] = language_counts.get(lang, 0) + 1\n",
        "\n",
        "    print(f\"\\nğŸ”¤ ì–¸ì–´ ë¶„í¬:\")\n",
        "    for lang, count in sorted(language_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        if lang and count > 0:\n",
        "            print(f\"  {lang}: {count:,}ê°œ ({count/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # ë³¸ë¬¸ ê¸¸ì´ ë¶„ì„\n",
        "    content_lengths = [page.get('content_size', 0) for page in full_results]\n",
        "    content_lengths = [x for x in content_lengths if x > 0]\n",
        "\n",
        "    if content_lengths:\n",
        "        import statistics\n",
        "        avg_length = statistics.mean(content_lengths)\n",
        "        median_length = statistics.median(content_lengths)\n",
        "        max_length = max(content_lengths)\n",
        "\n",
        "        print(f\"\\nğŸ“ ë³¸ë¬¸ ê¸¸ì´ ë¶„ì„:\")\n",
        "        print(f\"  í‰ê·  ê¸¸ì´: {avg_length:.0f}ì\")\n",
        "        print(f\"  ì¤‘ê°„ê°’: {median_length:.0f}ì\")\n",
        "        print(f\"  ìµœëŒ€ ê¸¸ì´: {max_length:,}ì\")\n",
        "\n",
        "        # ê¸¸ì´ë³„ ë¶„í¬\n",
        "        length_ranges = [\n",
        "            (0, 100, \"ë§¤ìš° ì§§ìŒ\"),\n",
        "            (100, 1000, \"ì§§ìŒ\"),\n",
        "            (1000, 5000, \"ì¤‘ê°„\"),\n",
        "            (5000, 20000, \"ê¸´ í¸\"),\n",
        "            (20000, float('inf'), \"ë§¤ìš° ê¸´\")\n",
        "        ]\n",
        "\n",
        "        print(f\"\\nğŸ“Š ê¸¸ì´ë³„ ë¶„í¬:\")\n",
        "        for min_len, max_len, desc in length_ranges:\n",
        "            count = sum(1 for x in content_lengths if min_len <= x < max_len)\n",
        "            if count > 0:\n",
        "                print(f\"  {desc} ({min_len:,}-{max_len:,}ì): {count:,}ê°œ ({count/len(content_lengths)*100:.1f}%)\")\n",
        "\n",
        "    # ê°€ì¥ ê¸´ ë¬¸ì„œë“¤\n",
        "    longest_pages = sorted(full_results, key=lambda x: x.get('content_size', 0), reverse=True)[:10]\n",
        "\n",
        "    print(f\"\\nğŸ“– ê°€ì¥ ê¸´ ë¬¸ì„œ Top 10:\")\n",
        "    for i, page in enumerate(longest_pages, 1):\n",
        "        title = page['title']\n",
        "        length = page.get('content_size', 0)\n",
        "        print(f\"  {i:2d}. {title}: {length:,}ì\")\n",
        "\n",
        "    print(f\"\\nğŸ¯ í’ˆì§ˆ ì§€í‘œ:\")\n",
        "    print(f\"  ì™„ì „í•œ ë©”íƒ€ë°ì´í„° í˜ì´ì§€: {sum(1 for p in full_results if p.get('authors') and p.get('categories') and p.get('year')):,}ê°œ\")\n",
        "    print(f\"  API ë³´ê°• ì„±ê³µë¥ : {api_enhanced/total_pages*100:.1f}%\")\n",
        "    print(f\"  ìœ ì˜ë¯¸í•œ ë³¸ë¬¸ ë¹„ìœ¨: {content_pages/total_pages*100:.1f}%\")\n",
        "    print(f\"  êµ¬ì¡°í™”ëœ ì •ë³´ ë¹„ìœ¨: {year_found/total_pages*100:.1f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"âŒ ì „ì²´ íŒŒì‹± ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print(\"ğŸ’¡ ë¨¼ì € ì „ì²´ íŒŒì‹±ì„ ì‹¤í–‰í•˜ì„¸ìš”.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9HvEvvIkhzZ"
      },
      "source": [
        "## ğŸ’¾ ì „ì²´ ë°ì´í„°ì…‹ ìµœì¢… ì €ì¥\n",
        "\n",
        "ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± ê²°ê³¼ë¥¼ ì™„ì „í•œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVYiBjynkhzZ"
      },
      "source": [
        "## ğŸ“ íŠœí† ë¦¬ì–¼ ì™„ë£Œ!\n",
        "\n",
        "### ğŸ‰ ì¶•í•˜í•©ë‹ˆë‹¤! ë‹¤ìŒì„ ë°°ì› ìŠµë‹ˆë‹¤:\n",
        "\n",
        "1. **ìœ„í‚¤ë¬¸í—Œ XML ë¤í”„ íŒŒì‹±**\n",
        "   - ìœ„í‚¤í…ìŠ¤íŠ¸ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
        "   - ë§ˆí¬ì—… ì œê±°í•˜ì—¬ ê¹”ë”í•œ ë³¸ë¬¸ ìƒì„±\n",
        "   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë¹ ë¥¸ ì²˜ë¦¬\n",
        "\n",
        "2. **API ë³´ê°• ê¸°ë²•**\n",
        "   - ìœ„í‚¤ë¬¸í—Œ APIë¡œ ì™„ì „í•œ ë¶„ë¥˜ ì •ë³´ ì¶”ì¶œ\n",
        "   - ìœ„í‚¤ë°ì´í„° APIë¡œ êµ¬ì¡°í™”ëœ ì—°ë„ ì •ë³´ íšë“\n",
        "   - ì—¬ëŸ¬ ì†ŒìŠ¤ ì •ë³´ í†µí•© ë° ê²€ì¦\n",
        "\n",
        "3. **ì‹¤ì œ ë¬¸ì œ í•´ê²°**\n",
        "   - ì• êµ­ê°€ ì‚¬ë¡€: 2ê°œ â†’ 4ê°œ ë¶„ë¥˜ ì™„ì „ ì¶”ì¶œ\n",
        "   - \"1941ë…„ ì‘í’ˆ\" ë¶„ë¥˜ì—ì„œ ì—°ë„ ì •ë³´ ì¶”ì¶œ\n",
        "   - í…œí”Œë¦¿ ê¸°ë°˜ ë¶„ë¥˜(PD-old-50) ì¶”ì¶œ\n",
        "   - ì• êµ­ê°€ ê°€ì‚¬(í•œì+í•œê¸€) ì™„ì „ ë³´ì¡´ í™•ì¸\n",
        "\n",
        "4. **ëŒ€ëŸ‰ ì²˜ë¦¬ ë° ë°ì´í„° ì €ì¥**\n",
        "   - ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ëŒ€ëŸ‰ í˜ì´ì§€ ì²˜ë¦¬\n",
        "   - JSON, CSV ë‹¤ì¤‘ í˜•ì‹ ì €ì¥\n",
        "   - API ë³´ê°• íš¨ê³¼ ì¶”ì  ë° ë¶„ì„\n",
        "   - ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
        "\n",
        "5. **ì „ì²´ ìœ„í‚¤ë¬¸í—Œ íŒŒì‹± (ê³ ê¸‰)**\n",
        "   - ìˆ˜ì²œ ê°œ í˜ì´ì§€ì˜ ì™„ì „í•œ ë°ì´í„°ì…‹ ìƒì„±\n",
        "   - ì¤‘ê°„ ì €ì¥ìœ¼ë¡œ ì•ˆì •ì„± ë³´ì¥\n",
        "   - ì—°êµ¬ìš©, ë¶„ì„ìš©, í…ìŠ¤íŠ¸ìš© ë‹¤ì¤‘ í˜•ì‹\n",
        "   - ì™„ì „í•œ ë©”íƒ€ë°ì´í„°ì™€ í†µê³„ ì •ë³´\n",
        "\n",
        "### ğŸš€ ë‹¤ìŒ ë‹¨ê³„:\n",
        "- **ì†Œê·œëª¨ ì—°êµ¬**: ëŒ€ëŸ‰ ì²˜ë¦¬ ê²°ê³¼ë¡œ íŠ¹ì • ì£¼ì œ ë¶„ì„\n",
        "- **ëŒ€ê·œëª¨ ì—°êµ¬**: ì „ì²´ ìœ„í‚¤ë¬¸í—Œ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¢…í•© ë¶„ì„\n",
        "- **ì‘ìš© ê°œë°œ**: ì¶”ì¶œëœ ë°ì´í„°ë¡œ ê²€ìƒ‰ ì—”ì§„, ì¶”ì²œ ì‹œìŠ¤í…œ êµ¬ì¶•\n",
        "- **ë‹¤ë¥¸ í”„ë¡œì íŠ¸**: ìœ„í‚¤ë°±ê³¼, ìœ„í‚¤ë‚±ë§ì‚¬ì „ ë“±ìœ¼ë¡œ í™•ì¥\n",
        "\n",
        "### ğŸ’¡ í•µì‹¬ êµí›ˆ:\n",
        "- **ë‹¨ì¼ ì†ŒìŠ¤ì˜ í•œê³„**: XML ë¤í”„ë§Œìœ¼ë¡œëŠ” ì™„ì „í•œ ì •ë³´ ì¶”ì¶œ ì–´ë ¤ì›€\n",
        "- **API ë³´ê°•ì˜ ì¤‘ìš”ì„±**: ì›¹ì‚¬ì´íŠ¸ì™€ ë™ì¼í•œ ì™„ì „í•œ ë°ì´í„° íšë“\n",
        "- **ê²€ì¦ì˜ í•„ìš”ì„±**: ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ì •ë³´ë¥¼ êµì°¨ í™•ì¸\n",
        "- **ì„±ëŠ¥ ìµœì í™”**: ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ëŒ€ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬\n",
        "- **ë°ì´í„° í’ˆì§ˆ**: ë³¸ë¬¸ ë‚´ìš©ê¹Œì§€ ì™„ì „íˆ ë³´ì¡´í•˜ëŠ” íŒŒì‹±\n",
        "- **í™•ì¥ì„± ê³ ë ¤**: ì¤‘ê°„ ì €ì¥ê³¼ ë°°ì¹˜ ì²˜ë¦¬ë¡œ ëŒ€ê·œëª¨ ë°ì´í„° ì•ˆì •ì  ì²˜ë¦¬\n",
        "\n",
        "### ğŸ“Š ìƒì„± ê°€ëŠ¥í•œ ë°ì´í„°:\n",
        "- **ì™„ì „í•œ JSON**: ëª¨ë“  ë©”íƒ€ë°ì´í„°ì™€ ë³¸ë¬¸ í¬í•¨\n",
        "- **ì—°êµ¬ìš© CSV**: í•µì‹¬ ì •ë³´ë§Œ ì •ë¦¬ëœ ë¶„ì„ìš© ë°ì´í„°\n",
        "- **ìƒì„¸ CSV**: API ë³´ê°• ê³¼ì •ê¹Œì§€ ì¶”ì  ê°€ëŠ¥í•œ ìƒì„¸ ë°ì´í„°\n",
        "- **í…ìŠ¤íŠ¸ CSV**: ë³¸ë¬¸ ë¶„ì„ ì „ìš© ë°ì´í„°\n",
        "- **ë©”íƒ€ë°ì´í„°**: ë°ì´í„°ì…‹ í†µê³„ì™€ í’ˆì§ˆ ì •ë³´\n",
        "\n",
        "**ì´ì œ ì—¬ëŸ¬ë¶„ë„ ìœ„í‚¤ë¬¸í—Œ ë°ì´í„° ê³¼í•™ìì…ë‹ˆë‹¤! ğŸŒŸ**\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ”„ ì‹¤í–‰ ê°€ì´ë“œ:\n",
        "\n",
        "**ğŸ“ ê¸°ë³¸ í•™ìŠµ (ê¶Œì¥):**\n",
        "1. \"ê¸°ë³¸ íŒŒì‹±\" â†’ \"API ë³´ê°• íŒŒì‹±\" â†’ \"ë¹„êµ ë¶„ì„\" ìˆœì„œë¡œ ì‹¤í–‰\n",
        "2. ì• êµ­ê°€ ì‚¬ë¡€ë¡œ í•µì‹¬ ê°œë… ì´í•´\n",
        "3. ì†Œê·œëª¨ ëŒ€ëŸ‰ ì²˜ë¦¬(10-50ê°œ)ë¡œ ê²½í—˜ ìŒ“ê¸°\n",
        "\n",
        "**ğŸš€ ëŒ€ëŸ‰ ì²˜ë¦¬:**\n",
        "1. `BULK_LIMIT` ê°’ì„ 100-1000ìœ¼ë¡œ ì„¤ì •\n",
        "2. ê²°ê³¼ ë¶„ì„ ë° ì €ì¥ ì‹¤í–‰\n",
        "3. ìƒì„±ëœ CSV/JSONìœ¼ë¡œ ë¶„ì„ ì‹¤ìŠµ\n",
        "\n",
        "**ğŸ”¥ ì „ì²´ íŒŒì‹± (ê³ ê¸‰):**\n",
        "1. `EXECUTE_FULL_PARSING = True` ì„¤ì •\n",
        "2. ìˆ˜ì‹­ ë¶„-ëª‡ ì‹œê°„ ëŒ€ê¸° (ì§„í–‰ìƒí™© ëª¨ë‹ˆí„°ë§)\n",
        "3. ì™„ì „í•œ í•œêµ­ì–´ ìœ„í‚¤ë¬¸í—Œ ë°ì´í„°ì…‹ íšë“\n",
        "4. ì—°êµ¬ í”„ë¡œì íŠ¸ë‚˜ ë…¼ë¬¸ì— í™œìš©\n",
        "\n",
        "**âš ï¸ ì£¼ì˜ì‚¬í•­:**\n",
        "- ì „ì²´ íŒŒì‹±ì€ ë§ì€ ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ í•„ìš”\n",
        "- Colab Pro ê¶Œì¥ (ë” ë§ì€ ë©”ëª¨ë¦¬ì™€ ì‹œê°„)\n",
        "- API í˜¸ì¶œ ì œí•œìœ¼ë¡œ ì¸í•œ ì§€ì—° ê°€ëŠ¥"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "final_version": {
      "created": "2025-09-11 10:58:55",
      "fixes_applied": [
        "TypeError in year sorting resolved",
        "Safe type conversion for year data",
        "Improved exception handling",
        "Colab optimization"
      ],
      "status": "Production Ready",
      "tested": true
    },
    "fixed_issues": [
      "TypeError in year sorting fixed",
      "Added type checking for year data",
      "Improved exception handling for year parsing",
      "Safe integer conversion for year values"
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12+"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}