{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ByungjunKim/WikisourceParsing/blob/main/wikisource_tutorial_colab_FINAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial_header"
      },
      "source": [
        "# 📚 위키문헌 파서 튜토리얼\n",
        "\n",
        "**한국어 위키문헌에서 완전한 메타데이터를 추출하는 방법을 배워보세요!**\n",
        "\n",
        "## 🎯 학습 목표\n",
        "- 위키문헌 XML 덤프에서 텍스트와 메타데이터 추출하기\n",
        "- API를 활용해 누락된 분류 정보 보강하기\n",
        "- 위키데이터와 연동해 연도 정보 추출하기\n",
        "- 실제 사용 가능한 깔끔한 텍스트 데이터 생성하기\n",
        "\n",
        "## 📖 예제: 애국가 완전 분석\n",
        "- **문제**: XML 덤프에서는 2개 분류만 나오는데, 실제 웹사이트에는 4개가 있음\n",
        "- **해결**: API 보강으로 '1941년 작품', 'PD-old-50' 등 누락된 분류까지 완전 추출\n",
        "- **결과**: 분류에서 연도 정보(1941년)까지 정확하게 추출 성공!\n",
        "\n",
        "---\n",
        "**✅ 노트북 테스트 완료**: 2025-09-11 10:21:57\n",
        "- 모든 라이브러리 로딩 성공\n",
        "- API 함수들 정상 작동\n",
        "- 덤프 파일 확인 완료 (147.8MB)\n",
        "- 멀티프로세싱 환경 준비 (11개 코어)\n",
        "\n",
        "**🚀 실행 준비 완료!**\n",
        "\n",
        "---\n",
        "## 🎉 **최종 버전 - Colab 실행 준비 완료!**\n",
        "**버전**: v1.0 Final | **생성일**: 2025-09-11 10:58:55\n",
        "\n",
        "### ✅ **검증 완료 사항:**\n",
        "- 모든 라이브러리 호환성 확인\n",
        "- TypeError 및 정렬 오류 수정\n",
        "- API 함수 정상 작동 테스트  \n",
        "- 멀티프로세싱 환경 최적화\n",
        "- 전체 위키문헌 파싱 기능 검증\n",
        "\n",
        "### 🚀 **바로 실행 가능:**\n",
        "1. Google Colab에 업로드\n",
        "2. 런타임 연결 후 순차 실행\n",
        "3. 애국가 분석부터 전체 파싱까지 단계별 학습\n",
        "\n",
        "**💡 권장 실행 순서**: 기본 파싱 → API 보강 → 대량 처리 → 전체 파싱\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## 🔧 환경 설정\n",
        "\n",
        "먼저 필요한 라이브러리를 설치하고 임포트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_libraries",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9a1408-3dad-41c6-86eb-02d7b2ea6e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mwxml\n",
            "  Downloading mwxml-0.3.6-py2.py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: jsonschema>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from mwxml) (4.25.1)\n",
            "Collecting mwcli>=0.0.2 (from mwxml)\n",
            "  Downloading mwcli-0.0.3-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting mwtypes>=0.4.0 (from mwxml)\n",
            "  Downloading mwtypes-0.4.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting para>=0.0.1 (from mwxml)\n",
            "  Downloading para-0.0.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=2.5.1->mwxml) (0.27.1)\n",
            "Collecting docopt (from mwcli>=0.0.2->mwxml)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jsonable>=0.3.0 (from mwtypes>=0.4.0->mwxml)\n",
            "  Downloading jsonable-0.3.1-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema>=2.5.1->mwxml) (4.15.0)\n",
            "Downloading mwxml-0.3.6-py2.py3-none-any.whl (33 kB)\n",
            "Downloading mwcli-0.0.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Downloading mwtypes-0.4.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading para-0.0.8-py3-none-any.whl (6.5 kB)\n",
            "Downloading jsonable-0.3.1-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=d05a2c530570b21cbdc935cfe7f27da781371df35b63c03b0b7c36486c0f9586\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built docopt\n",
            "Installing collected packages: para, jsonable, docopt, mwtypes, mwcli, mwxml\n",
            "Successfully installed docopt-0.6.2 jsonable-0.3.1 mwcli-0.0.3 mwtypes-0.4.0 mwxml-0.3.6 para-0.0.8\n",
            "✅ 모든 라이브러리가 성공적으로 로드되었습니다!\n",
            "🖥️  사용 가능한 CPU 코어: 2개\n"
          ]
        }
      ],
      "source": [
        "# 필요한 라이브러리 설치\n",
        "!pip install mwxml requests tqdm pandas\n",
        "\n",
        "# 기본 라이브러리 임포트\n",
        "import mwxml\n",
        "import bz2\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import urllib.parse\n",
        "import multiprocessing as mp\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "print(\"✅ 모든 라이브러리가 성공적으로 로드되었습니다!\")\n",
        "print(f\"🖥️  사용 가능한 CPU 코어: {mp.cpu_count()}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "download_section"
      },
      "source": [
        "## 📥 데이터 다운로드\n",
        "\n",
        "한국어 위키문헌 덤프 파일을 다운로드합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "download_dump",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "969ed330-feaa-4e37-e2d5-7b069464712c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 위키문헌 덤프 다운로드 중... (약 150MB, 시간이 걸릴 수 있습니다)\n",
            "--2025-09-11 02:40:22--  https://dumps.wikimedia.org/kowikisource/20250901/kowikisource-20250901-pages-articles.xml.bz2\n",
            "Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.71, 2620:0:861:3:208:80:154:71\n",
            "Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.71|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154956955 (148M) [application/octet-stream]\n",
            "Saving to: ‘kowikisource-20250901-pages-articles.xml.bz2’\n",
            "\n",
            "kowikisource-202509 100%[===================>] 147.78M  4.23MB/s    in 35s     \n",
            "\n",
            "2025-09-11 02:40:58 (4.22 MB/s) - ‘kowikisource-20250901-pages-articles.xml.bz2’ saved [154956955/154956955]\n",
            "\n",
            "✅ 다운로드 완료!\n",
            "📁 파일 크기: 147.8 MB\n"
          ]
        }
      ],
      "source": [
        "# 위키문헌 덤프 다운로드\n",
        "dump_url = \"https://dumps.wikimedia.org/kowikisource/20250901/kowikisource-20250901-pages-articles.xml.bz2\"\n",
        "dump_file = \"kowikisource-20250901-pages-articles.xml.bz2\"\n",
        "\n",
        "if not os.path.exists(dump_file):\n",
        "    print(\"📥 위키문헌 덤프 다운로드 중... (약 150MB, 시간이 걸릴 수 있습니다)\")\n",
        "    !wget -O {dump_file} {dump_url}\n",
        "    print(\"✅ 다운로드 완료!\")\n",
        "else:\n",
        "    print(\"✅ 덤프 파일이 이미 존재합니다!\")\n",
        "\n",
        "# 파일 크기 확인\n",
        "file_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "print(f\"📁 파일 크기: {file_size:.1f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_functions_section"
      },
      "source": [
        "## 🌐 API 보강 함수들\n",
        "\n",
        "위키문헌 API와 위키데이터 API를 사용해서 누락된 정보를 보강하는 함수들을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "api_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05be5958-e828-4959-97bd-8df443949517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API 보강 함수들이 정의되었습니다!\n"
          ]
        }
      ],
      "source": [
        "def get_categories_from_api(page_title):\n",
        "    \"\"\"\n",
        "    위키문헌 API에서 페이지의 모든 분류를 가져옵니다\n",
        "\n",
        "    이 함수가 중요한 이유:\n",
        "    - XML 덤프에는 기본 분류만 있음\n",
        "    - 템플릿에서 자동 생성되는 분류는 API로만 확인 가능\n",
        "    - 예: '1941년 작품', 'PD-old-50' 등\n",
        "    \"\"\"\n",
        "    try:\n",
        "        api_url = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'categories',\n",
        "            'cllimit': 'max'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(api_url, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return []\n",
        "\n",
        "        categories = pages[page_id].get('categories', [])\n",
        "        category_names = []\n",
        "\n",
        "        for cat in categories:\n",
        "            cat_title = cat['title']\n",
        "            if cat_title.startswith('분류:'):\n",
        "                category_names.append(cat_title[3:])  # '분류:' 제거\n",
        "\n",
        "        return category_names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"API 분류 조회 오류 ({page_title}): {e}\")\n",
        "        return []\n",
        "\n",
        "def get_year_from_wikidata(page_title):\n",
        "    \"\"\"\n",
        "    위키데이터에서 작품의 발표 연도를 가져옵니다\n",
        "\n",
        "    위키데이터 연동의 장점:\n",
        "    - 구조화된 연도 정보 제공\n",
        "    - 여러 언어판에서 공유되는 정확한 데이터\n",
        "    - 분류 정보와 교차 검증 가능\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1단계: 위키문헌 페이지에서 위키데이터 ID 가져오기\n",
        "        wikisource_api = \"https://ko.wikisource.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'query',\n",
        "            'format': 'json',\n",
        "            'titles': page_title,\n",
        "            'prop': 'pageprops'\n",
        "        }\n",
        "\n",
        "        headers = {'User-Agent': 'WikisourceParser/1.0 (Educational Tutorial)'}\n",
        "        response = requests.get(wikisource_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        pages = data.get('query', {}).get('pages', {})\n",
        "        page_id = list(pages.keys())[0]\n",
        "\n",
        "        if page_id == '-1':\n",
        "            return None\n",
        "\n",
        "        wikidata_id = pages[page_id].get('pageprops', {}).get('wikibase_item')\n",
        "\n",
        "        if not wikidata_id:\n",
        "            return None\n",
        "\n",
        "        # 2단계: 위키데이터에서 발표일 정보 가져오기\n",
        "        wikidata_api = \"https://www.wikidata.org/w/api.php\"\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'format': 'json',\n",
        "            'ids': wikidata_id,\n",
        "            'props': 'claims'\n",
        "        }\n",
        "\n",
        "        response = requests.get(wikidata_api, params=params, headers=headers, timeout=10)\n",
        "        data = response.json()\n",
        "\n",
        "        entity = data.get('entities', {}).get(wikidata_id, {})\n",
        "        claims = entity.get('claims', {})\n",
        "\n",
        "        # 발표 관련 속성들 확인\n",
        "        date_properties = ['P577', 'P571', 'P585']  # 발표일, 시작일, 특정시점\n",
        "\n",
        "        for prop in date_properties:\n",
        "            if prop in claims:\n",
        "                for claim in claims[prop]:\n",
        "                    try:\n",
        "                        time_value = claim['mainsnak']['datavalue']['value']['time']\n",
        "                        # +1941-00-00T00:00:00Z 형태에서 연도 추출\n",
        "                        year = int(time_value[1:5])\n",
        "                        if 1800 <= year <= 2030:  # 유효한 연도 범위\n",
        "                            return year\n",
        "                    except (KeyError, ValueError):\n",
        "                        continue\n",
        "\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"위키데이터 조회 오류 ({page_title}): {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"✅ API 보강 함수들이 정의되었습니다!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "parsing_functions_section"
      },
      "source": [
        "## 🔍 텍스트 파싱 함수들\n",
        "\n",
        "위키텍스트에서 메타데이터를 추출하고 깔끔한 본문을 만드는 함수들입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "parsing_functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de72222a-fc65-40ad-9959-5407c46ddd61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 텍스트 파싱 함수들이 정의되었습니다!\n"
          ]
        }
      ],
      "source": [
        "def extract_metadata(text):\n",
        "    \"\"\"\n",
        "    위키텍스트에서 메타데이터를 추출합니다\n",
        "\n",
        "    추출하는 정보:\n",
        "    - 저자: [[저자:이름]] 패턴에서\n",
        "    - 분류: [[분류:이름]] 패턴에서\n",
        "    - 작곡가: '작곡' 키워드 주변에서\n",
        "    - 라이선스: {{PD-*}} 템플릿에서\n",
        "    - 언어: '한자', '한글' 키워드에서\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return {\n",
        "            'authors': [],\n",
        "            'categories': [],\n",
        "            'composer': None,\n",
        "            'translator': None,\n",
        "            'year': None,\n",
        "            'license': None,\n",
        "            'language': None\n",
        "        }\n",
        "\n",
        "    # 저자 정보 추출\n",
        "    authors = []\n",
        "    author_patterns = [\n",
        "        r'\\[\\[저자:([^|\\]]+)',  # [[저자:이름]] 또는 [[저자:이름|표시명]]\n",
        "        r'\\|\\s*author\\s*=\\s*\\[\\[저자:([^|\\]]+)',  # author= 매개변수\n",
        "    ]\n",
        "\n",
        "    for pattern in author_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        authors.extend(matches)\n",
        "\n",
        "    # 분류 정보 추출\n",
        "    categories = re.findall(r'\\[\\[분류:([^\\]]+)\\]\\]', text)\n",
        "\n",
        "    # 작곡가 정보 추출\n",
        "    composer = None\n",
        "    composer_patterns = [\n",
        "        r'([^.]+)\\s*작곡',\n",
        "        r'작곡가?\\s*[:=]\\s*([^.\\n]+)'\n",
        "    ]\n",
        "    for pattern in composer_patterns:\n",
        "        matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "        if matches:\n",
        "            composer = matches[0].strip()\n",
        "            break\n",
        "\n",
        "    # 연도 정보 추출\n",
        "    year_matches = re.findall(r'(\\d{4})년', text)\n",
        "    year = None\n",
        "    if year_matches:\n",
        "        year = max(set(year_matches), key=year_matches.count)\n",
        "\n",
        "    # 라이선스 정보 추출\n",
        "    license_info = None\n",
        "    license_patterns = [\n",
        "        r'\\{\\{(PD-[^}]+)\\}\\}',\n",
        "        r'\\{\\{(CC-[^}]+)\\}\\}'\n",
        "    ]\n",
        "    for pattern in license_patterns:\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            license_info = matches[0]\n",
        "            break\n",
        "\n",
        "    # 언어 정보 추출\n",
        "    language = None\n",
        "    if '한자' in text and '한글' in text:\n",
        "        language = '한자+한글'\n",
        "    elif '한자' in text:\n",
        "        language = '한자'\n",
        "    elif '한글' in text:\n",
        "        language = '한글'\n",
        "\n",
        "    # 중복 제거\n",
        "    authors = list(set([a.strip() for a in authors if a.strip()]))\n",
        "    categories = list(set([c.strip() for c in categories if c.strip()]))\n",
        "\n",
        "    return {\n",
        "        'authors': authors,\n",
        "        'categories': categories,\n",
        "        'composer': composer,\n",
        "        'translator': None,  # 나중에 확장 가능\n",
        "        'year': year,\n",
        "        'license': license_info,\n",
        "        'language': language\n",
        "    }\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    위키텍스트에서 마크업을 제거하고 깔끔한 본문만 추출합니다\n",
        "\n",
        "    제거하는 것들:\n",
        "    - 템플릿: {{...}}\n",
        "    - HTML 태그: <div>, <br> 등\n",
        "    - 위키 링크: [[...]]\n",
        "    - 마크업: ''', ''\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # 템플릿 제거\n",
        "    cleaned = re.sub(r'\\{\\{[^{}]*\\}\\}', '', text)\n",
        "\n",
        "    # 링크에서 텍스트만 추출\n",
        "    cleaned = re.sub(r'\\[\\[[^|\\]]*\\|([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "    cleaned = re.sub(r'\\[\\[([^\\]]+)\\]\\]', r'\\1', cleaned)\n",
        "\n",
        "    # HTML 태그 제거\n",
        "    cleaned = re.sub(r'<[^>]+>', '', cleaned)\n",
        "\n",
        "    # 위키 마크업 제거\n",
        "    cleaned = re.sub(r\"'''([^']+)'''\", r'\\1', cleaned)  # 굵은 글씨\n",
        "    cleaned = re.sub(r\"''([^']+)''\", r'\\1', cleaned)   # 기울임\n",
        "\n",
        "    # 섹션 헤더 제거\n",
        "    cleaned = re.sub(r'^=+\\s*([^=]+)\\s*=+$', r'\\1', cleaned, flags=re.MULTILINE)\n",
        "\n",
        "    # 여러 공백을 하나로\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned)\n",
        "\n",
        "    return cleaned.strip()\n",
        "\n",
        "def generate_urls(title, authors):\n",
        "    \"\"\"\n",
        "    페이지와 저자의 URL을 생성합니다\n",
        "    \"\"\"\n",
        "    base_url = \"https://ko.wikisource.org/wiki/\"\n",
        "\n",
        "    # 페이지 URL\n",
        "    page_url = base_url + urllib.parse.quote(title.replace(' ', '_'))\n",
        "\n",
        "    # 저자 URL들\n",
        "    author_links = []\n",
        "    for author in authors:\n",
        "        author_url = base_url + urllib.parse.quote(f\"저자:{author}\".replace(' ', '_'))\n",
        "        author_links.append({\n",
        "            'name': author,\n",
        "            'url': author_url\n",
        "        })\n",
        "\n",
        "    return page_url, author_links\n",
        "\n",
        "print(\"✅ 텍스트 파싱 함수들이 정의되었습니다!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enhancement_section"
      },
      "source": [
        "## 🚀 API 보강 함수\n",
        "\n",
        "덤프에서 파싱한 데이터를 API로 보강하는 핵심 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "enhancement_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ade02bd-5f31-4ad9-a257-789510c89aa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API 보강 함수가 정의되었습니다!\n"
          ]
        }
      ],
      "source": [
        "def enhance_with_api(page_data):\n",
        "    \"\"\"\n",
        "    페이지 데이터를 API 정보로 보강합니다\n",
        "\n",
        "    보강 과정:\n",
        "    1. API에서 완전한 분류 정보 가져오기\n",
        "    2. 위키데이터에서 연도 정보 가져오기\n",
        "    3. 분류에서 연도 추출하기\n",
        "    4. 최종 연도 결정 (분류 > 위키데이터 > 덤프)\n",
        "    \"\"\"\n",
        "    title = page_data['title']\n",
        "\n",
        "    # API에서 완전한 분류 정보 가져오기\n",
        "    api_categories = get_categories_from_api(title)\n",
        "\n",
        "    # 위키데이터에서 연도 정보 가져오기\n",
        "    wikidata_year = get_year_from_wikidata(title)\n",
        "\n",
        "    # 기존 데이터 복사\n",
        "    enhanced = page_data.copy()\n",
        "\n",
        "    # 분류 정보 병합 (중복 제거)\n",
        "    all_categories = list(set(page_data.get('categories', []) + api_categories))\n",
        "    enhanced['categories'] = all_categories\n",
        "    enhanced['api_categories'] = api_categories\n",
        "\n",
        "    # 분류에서 연도 추출\n",
        "    year_from_categories = None\n",
        "    year_categories = [cat for cat in all_categories if '년' in cat and '작품' in cat]\n",
        "    if year_categories:\n",
        "        for cat in year_categories:\n",
        "            year_match = re.search(r'(\\d{4})년', cat)\n",
        "            if year_match:\n",
        "                year_from_categories = int(year_match.group(1))\n",
        "                break\n",
        "\n",
        "    # 최종 연도 결정 (우선순위: 분류 > 위키데이터 > 덤프)\n",
        "    enhanced['year'] = (\n",
        "        year_from_categories or\n",
        "        wikidata_year or\n",
        "        page_data.get('year')\n",
        "    )\n",
        "\n",
        "    # 보강 정보 추가\n",
        "    enhanced['year_from_categories'] = year_from_categories\n",
        "    enhanced['year_from_wikidata'] = wikidata_year\n",
        "\n",
        "    return enhanced\n",
        "\n",
        "print(\"✅ API 보강 함수가 정의되었습니다!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main_parser_section"
      },
      "source": [
        "## ⚡ 메인 파서 (멀티프로세싱)\n",
        "\n",
        "모든 CPU 코어를 사용해서 빠르게 처리하는 메인 파서입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "main_parser",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f282dfb6-6ee7-4271-d9e6-b6faa196bbeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 메인 파서가 정의되었습니다!\n"
          ]
        }
      ],
      "source": [
        "def process_pages_batch(pages_batch):\n",
        "    \"\"\"\n",
        "    페이지 배치를 처리하는 워커 함수\n",
        "    (멀티프로세싱에서 각 코어가 실행)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for page_data in pages_batch:\n",
        "        try:\n",
        "            page_id, title, namespace, redirect, revisions = page_data\n",
        "\n",
        "            if not revisions:\n",
        "                continue\n",
        "\n",
        "            # 최신 리비전 사용\n",
        "            revision = revisions[0]\n",
        "            revision_id, timestamp, username, comment, text, size = revision\n",
        "\n",
        "            # 메타데이터 추출\n",
        "            metadata = extract_metadata(text)\n",
        "\n",
        "            # 본문 정리\n",
        "            clean_content = clean_text(text)\n",
        "\n",
        "            # URL 생성\n",
        "            page_url, author_links = generate_urls(title, metadata['authors'])\n",
        "\n",
        "            # 페이지 데이터 구성\n",
        "            page_result = {\n",
        "                'page_id': page_id,\n",
        "                'title': title,\n",
        "                'url': page_url,\n",
        "                'namespace': namespace,\n",
        "                'redirect': redirect,\n",
        "\n",
        "                # 필수 메타데이터\n",
        "                'authors': metadata['authors'],\n",
        "                'author_links': author_links,\n",
        "                'categories': metadata['categories'],\n",
        "                'content': clean_content,\n",
        "                'raw_content': text,\n",
        "\n",
        "                # 추가 메타데이터\n",
        "                'composer': metadata['composer'],\n",
        "                'translator': metadata['translator'],\n",
        "                'year': metadata['year'],\n",
        "                'license': metadata['license'],\n",
        "                'language': metadata['language'],\n",
        "\n",
        "                # 리비전 정보\n",
        "                'revision_id': revision_id,\n",
        "                'last_modified': str(timestamp) if timestamp else None,\n",
        "                'last_contributor': username,\n",
        "                'size': len(text) if text else 0,\n",
        "                'content_size': len(clean_content)\n",
        "            }\n",
        "\n",
        "            results.append(page_result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"페이지 처리 오류: {e}\")\n",
        "            continue\n",
        "\n",
        "    return results\n",
        "\n",
        "def parse_wikisource(dump_file, limit=None, enable_api=False, batch_size=50):\n",
        "    \"\"\"\n",
        "    위키문헌 덤프를 파싱합니다\n",
        "\n",
        "    Args:\n",
        "        dump_file: 덤프 파일 경로\n",
        "        limit: 처리할 페이지 수 제한 (None이면 전체)\n",
        "        enable_api: API 보강 사용 여부\n",
        "        batch_size: 배치 크기\n",
        "\n",
        "    Returns:\n",
        "        list: 파싱된 페이지 데이터\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # CPU 코어 수 (Colab에서 최대 활용)\n",
        "    max_workers = mp.cpu_count()\n",
        "\n",
        "    print(f\"🚀 위키문헌 파싱 시작!\")\n",
        "    print(f\"  📊 CPU 코어: {max_workers}개 (최대 활용)\")\n",
        "    print(f\"  📦 배치 크기: {batch_size}\")\n",
        "    print(f\"  🌐 API 보강: {'사용' if enable_api else '사용 안함'}\")\n",
        "\n",
        "    # 1단계: 덤프에서 데이터 수집\n",
        "    print(\"\\n📥 1단계: 덤프 데이터 수집\")\n",
        "    pages_batches = []\n",
        "    current_batch = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"페이지 수집\"):\n",
        "            if limit and total_pages >= limit:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                # 리비전 데이터 수집\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break  # 최신 리비전만\n",
        "\n",
        "                page_data = (\n",
        "                    page.id,\n",
        "                    page.title,\n",
        "                    page.namespace,\n",
        "                    str(page.redirect.title) if page.redirect else None,\n",
        "                    revisions\n",
        "                )\n",
        "                current_batch.append(page_data)\n",
        "\n",
        "                if len(current_batch) >= batch_size:\n",
        "                    pages_batches.append(current_batch)\n",
        "                    current_batch = []\n",
        "\n",
        "                total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        if current_batch:\n",
        "            pages_batches.append(current_batch)\n",
        "\n",
        "    print(f\"✅ {total_pages}개 페이지를 {len(pages_batches)}개 배치로 수집\")\n",
        "\n",
        "    # 2단계: 멀티프로세싱으로 병렬 처리\n",
        "    print(\"\\n⚡ 2단계: 멀티프로세싱 처리\")\n",
        "    results = []\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
        "        # 모든 배치를 병렬로 처리\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in pages_batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"배치 처리\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                results.extend(batch_results)\n",
        "            except Exception as e:\n",
        "                print(f\"배치 처리 오류: {e}\")\n",
        "\n",
        "    # 3단계: API 보강 (선택적)\n",
        "    if enable_api and results:\n",
        "        print(\"\\n🌐 3단계: API 보강 처리\")\n",
        "        enhanced_results = []\n",
        "\n",
        "        for page_data in tqdm(results, desc=\"API 보강\"):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "                time.sleep(0.1)  # API 제한 고려\n",
        "            except Exception as e:\n",
        "                print(f\"API 보강 오류 ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        results = enhanced_results\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n🎉 파싱 완료!\")\n",
        "    print(f\"  ⏱️  총 시간: {total_time:.1f}초\")\n",
        "    print(f\"  📊 처리 속도: {len(results)/total_time:.1f} 페이지/초\")\n",
        "    print(f\"  📄 총 페이지: {len(results)}개\")\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"✅ 메인 파서가 정의되었습니다!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_section"
      },
      "source": [
        "## 🎯 실습 1: 애국가 분석 (기본 파싱)\n",
        "\n",
        "먼저 XML 덤프만으로 애국가를 파싱해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "demo_basic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7879b545-aa8b-40f9-ca24-f4fe8b616ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 실습 1: 기본 XML 덤프 파싱\n",
            "==================================================\n",
            "🚀 위키문헌 파싱 시작!\n",
            "  📊 CPU 코어: 2개 (최대 활용)\n",
            "  📦 배치 크기: 1\n",
            "  🌐 API 보강: 사용 안함\n",
            "\n",
            "📥 1단계: 덤프 데이터 수집\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "페이지 수집: 1it [00:00, 304.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 1개 페이지를 1개 배치로 수집\n",
            "\n",
            "⚡ 2단계: 멀티프로세싱 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "배치 처리: 100%|██████████| 1/1 [00:00<00:00, 60.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎉 파싱 완료!\n",
            "  ⏱️  총 시간: 0.1초\n",
            "  📊 처리 속도: 11.0 페이지/초\n",
            "  📄 총 페이지: 1개\n",
            "\n",
            "📄 제목: 애국가 (대한민국)\n",
            "👤 저자: ['윤치호']\n",
            "📂 분류: ['대한민국의 노래', '국가']\n",
            "🎵 작곡가: 안익태(安益泰)\n",
            "📅 연도: None\n",
            "📜 라이선스: PD-old-50\n",
            "📏 본문 길이: 426 문자\n",
            "\n",
            "📖 본문 미리보기:\n",
            "한자 혼용 ;1 :東海물과 白頭山이 마르고 닳도록하느님이 保佑하사 우리 나라 萬歲 :無窮花 三千里 華麗江山大韓사람 大韓으로 길이 保全하세 ;2 :南山 위에 저 소나무 鐵甲을 두른 듯바람 서리 不變함은 우리 氣像일세 ;3 :가을 하늘 空豁한데 높고 구름 없이밝은 달은 우리 가슴 一片丹心일세 ;4 :이 氣像과 이 맘으로 忠誠을 다하여괴로우나 즐거우나 나라 사랑...\n",
            "\n",
            "🔍 문제점 발견:\n",
            "  ❌ 분류가 2개만 나옴 (실제로는 4개)\n",
            "  ❌ '1941년 작품' 분류가 누락됨\n",
            "  ❌ 'PD-old-50' 분류가 누락됨\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 애국가만 파싱 (API 보강 없음)\n",
        "print(\"📚 실습 1: 기본 XML 덤프 파싱\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "basic_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=1,  # 첫 번째 페이지 (애국가)만\n",
        "    enable_api=False,  # API 보강 안함\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "if basic_results:\n",
        "    aegukga_basic = basic_results[0]\n",
        "\n",
        "    print(f\"\\n📄 제목: {aegukga_basic['title']}\")\n",
        "    print(f\"👤 저자: {aegukga_basic['authors']}\")\n",
        "    print(f\"📂 분류: {aegukga_basic['categories']}\")\n",
        "    print(f\"🎵 작곡가: {aegukga_basic.get('composer', 'N/A')}\")\n",
        "    print(f\"📅 연도: {aegukga_basic.get('year', 'N/A')}\")\n",
        "    print(f\"📜 라이선스: {aegukga_basic.get('license', 'N/A')}\")\n",
        "    print(f\"📏 본문 길이: {aegukga_basic['content_size']} 문자\")\n",
        "\n",
        "    print(f\"\\n📖 본문 미리보기:\")\n",
        "    print(f\"{aegukga_basic['content'][:200]}...\")\n",
        "\n",
        "    print(f\"\\n🔍 문제점 발견:\")\n",
        "    print(f\"  ❌ 분류가 {len(aegukga_basic['categories'])}개만 나옴 (실제로는 4개)\")\n",
        "    print(f\"  ❌ '1941년 작품' 분류가 누락됨\")\n",
        "    print(f\"  ❌ 'PD-old-50' 분류가 누락됨\")\n",
        "else:\n",
        "    print(\"❌ 데이터를 찾을 수 없습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "demo_api_section"
      },
      "source": [
        "## 🌟 실습 2: 애국가 분석 (API 보강)\n",
        "\n",
        "이제 API 보강을 사용해서 완전한 정보를 추출해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "demo_api",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5520ae02-12b8-450e-dc9b-a60a9f18a58f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🌟 실습 2: API 보강 파싱\n",
            "==================================================\n",
            "🚀 위키문헌 파싱 시작!\n",
            "  📊 CPU 코어: 2개 (최대 활용)\n",
            "  📦 배치 크기: 1\n",
            "  🌐 API 보강: 사용\n",
            "\n",
            "📥 1단계: 덤프 데이터 수집\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "페이지 수집: 1it [00:00, 712.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 1개 페이지를 1개 배치로 수집\n",
            "\n",
            "⚡ 2단계: 멀티프로세싱 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "배치 처리: 100%|██████████| 1/1 [00:00<00:00, 37.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🌐 3단계: API 보강 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "API 보강: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎉 파싱 완료!\n",
            "  ⏱️  총 시간: 0.9초\n",
            "  📊 처리 속도: 1.2 페이지/초\n",
            "  📄 총 페이지: 1개\n",
            "\n",
            "📄 제목: 애국가 (대한민국)\n",
            "👤 저자: ['윤치호']\n",
            "\n",
            "📂 분류 정보:\n",
            "  덤프에서: []\n",
            "  API에서: ['1941년 작품', 'PD-old-50', '국가', '대한민국의 노래']\n",
            "  전체: ['PD-old-50', '1941년 작품', '대한민국의 노래', '국가']\n",
            "\n",
            "🎵 작곡가: 안익태(安益泰)\n",
            "\n",
            "📅 연도 정보:\n",
            "  최종: 1941\n",
            "  분류에서: 1941\n",
            "  위키데이터에서: 1941\n",
            "\n",
            "📜 라이선스: PD-old-50\n",
            "🌐 언어: 한자+한글\n",
            "📏 본문 길이: 426 문자\n",
            "\n",
            "🎯 목표 달성:\n",
            "  ✅ 1941년 작품 분류\n",
            "  ✅ PD-old-50 분류\n",
            "  ✅ 1941년 연도 추출\n",
            "  ✅ 4개 분류 완전 추출\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 애국가 API 보강 파싱\n",
        "print(\"🌟 실습 2: API 보강 파싱\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "api_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=1,  # 첫 번째 페이지 (애국가)만\n",
        "    enable_api=True,  # API 보강 사용!\n",
        "    batch_size=1\n",
        ")\n",
        "\n",
        "if api_results:\n",
        "    aegukga_api = api_results[0]\n",
        "\n",
        "    print(f\"\\n📄 제목: {aegukga_api['title']}\")\n",
        "    print(f\"👤 저자: {aegukga_api['authors']}\")\n",
        "\n",
        "    # 분류 비교\n",
        "    dump_cats = [cat for cat in aegukga_api['categories'] if cat not in aegukga_api.get('api_categories', [])]\n",
        "    api_cats = aegukga_api.get('api_categories', [])\n",
        "\n",
        "    print(f\"\\n📂 분류 정보:\")\n",
        "    print(f\"  덤프에서: {dump_cats}\")\n",
        "    print(f\"  API에서: {api_cats}\")\n",
        "    print(f\"  전체: {aegukga_api['categories']}\")\n",
        "\n",
        "    print(f\"\\n🎵 작곡가: {aegukga_api.get('composer', 'N/A')}\")\n",
        "\n",
        "    # 연도 정보 상세\n",
        "    print(f\"\\n📅 연도 정보:\")\n",
        "    print(f\"  최종: {aegukga_api.get('year', 'N/A')}\")\n",
        "    print(f\"  분류에서: {aegukga_api.get('year_from_categories', 'N/A')}\")\n",
        "    print(f\"  위키데이터에서: {aegukga_api.get('year_from_wikidata', 'N/A')}\")\n",
        "\n",
        "    print(f\"\\n📜 라이선스: {aegukga_api.get('license', 'N/A')}\")\n",
        "    print(f\"🌐 언어: {aegukga_api.get('language', 'N/A')}\")\n",
        "    print(f\"📏 본문 길이: {aegukga_api['content_size']} 문자\")\n",
        "\n",
        "    # 성공 확인\n",
        "    success_checks = [\n",
        "        ('✅' if '1941년 작품' in api_cats else '❌', '1941년 작품 분류'),\n",
        "        ('✅' if 'PD-old-50' in api_cats else '❌', 'PD-old-50 분류'),\n",
        "        ('✅' if aegukga_api.get('year') == 1941 else '❌', '1941년 연도 추출'),\n",
        "        ('✅' if len(api_cats) >= 4 else '❌', '4개 분류 완전 추출')\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n🎯 목표 달성:\")\n",
        "    for status, description in success_checks:\n",
        "        print(f\"  {status} {description}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 데이터를 찾을 수 없습니다.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comparison_section"
      },
      "source": [
        "## 📊 비교 분석\n",
        "\n",
        "기본 파싱과 API 보강 파싱의 차이점을 비교해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "comparison",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ccc41f-3d13-4112-b22b-16da9264556d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 기본 파싱 vs API 보강 파싱 비교\n",
            "============================================================\n",
            "\n",
            "📂 분류 정보:\n",
            "  기본 파싱: 2개 → ['대한민국의 노래', '국가']\n",
            "  API 보강: 4개 → ['PD-old-50', '1941년 작품', '대한민국의 노래', '국가']\n",
            "  개선: +2개 분류 추가\n",
            "\n",
            "📅 연도 정보:\n",
            "  기본 파싱: None\n",
            "  API 보강: 1941 (분류: 1941, 위키데이터: 1941)\n",
            "\n",
            "🎯 핵심 성과:\n",
            "  ✅ 추가된 분류: ['PD-old-50', '1941년 작품']\n",
            "  ✅ 연도 정보 추출 성공: 1941년\n",
            "  ✅ 분류와 위키데이터 연도 일치: 1941년\n",
            "\n",
            "💡 학습 포인트:\n",
            "  1. XML 덤프만으로는 템플릿 기반 분류를 놓칠 수 있음\n",
            "  2. API 보강으로 웹사이트와 동일한 완전한 정보 추출 가능\n",
            "  3. 여러 소스에서 연도 정보를 교차 검증하여 정확성 향상\n",
            "  4. 구조화된 메타데이터로 후속 분석 작업 용이\n"
          ]
        }
      ],
      "source": [
        "print(\"📊 기본 파싱 vs API 보강 파싱 비교\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if basic_results and api_results:\n",
        "    basic = basic_results[0]\n",
        "    enhanced = api_results[0]\n",
        "\n",
        "    print(f\"\\n📂 분류 정보:\")\n",
        "    print(f\"  기본 파싱: {len(basic['categories'])}개 → {basic['categories']}\")\n",
        "    print(f\"  API 보강: {len(enhanced['categories'])}개 → {enhanced['categories']}\")\n",
        "    print(f\"  개선: +{len(enhanced['categories']) - len(basic['categories'])}개 분류 추가\")\n",
        "\n",
        "    print(f\"\\n📅 연도 정보:\")\n",
        "    print(f\"  기본 파싱: {basic.get('year', 'None')}\")\n",
        "    print(f\"  API 보강: {enhanced.get('year', 'None')} (분류: {enhanced.get('year_from_categories')}, 위키데이터: {enhanced.get('year_from_wikidata')})\")\n",
        "\n",
        "    print(f\"\\n🎯 핵심 성과:\")\n",
        "    missing_categories = set(enhanced['categories']) - set(basic['categories'])\n",
        "    if missing_categories:\n",
        "        print(f\"  ✅ 추가된 분류: {list(missing_categories)}\")\n",
        "\n",
        "    if enhanced.get('year') and not basic.get('year'):\n",
        "        print(f\"  ✅ 연도 정보 추출 성공: {enhanced.get('year')}년\")\n",
        "\n",
        "    if enhanced.get('year_from_categories') == enhanced.get('year_from_wikidata'):\n",
        "        print(f\"  ✅ 분류와 위키데이터 연도 일치: {enhanced.get('year')}년\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 비교할 데이터가 없습니다.\")\n",
        "\n",
        "print(f\"\\n💡 학습 포인트:\")\n",
        "print(f\"  1. XML 덤프만으로는 템플릿 기반 분류를 놓칠 수 있음\")\n",
        "print(f\"  2. API 보강으로 웹사이트와 동일한 완전한 정보 추출 가능\")\n",
        "print(f\"  3. 여러 소스에서 연도 정보를 교차 검증하여 정확성 향상\")\n",
        "print(f\"  4. 구조화된 메타데이터로 후속 분석 작업 용이\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bulk_processing_section"
      },
      "source": [
        "## 🔄 실습 3: 대량 처리 (선택적)\n",
        "\n",
        "더 많은 페이지를 처리해보고 싶다면 아래 셀을 실행하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "bulk_processing",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7df7c595-f9ac-4a9b-ff8a-4249b7e27b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 실습 3: 대량 처리\n",
            "==================================================\n",
            "📊 설정:\n",
            "  처리 페이지: 50개\n",
            "  API 보강: 사용\n",
            "  CPU 활용: 2개 코어 최대 활용\n",
            "🚀 위키문헌 파싱 시작!\n",
            "  📊 CPU 코어: 2개 (최대 활용)\n",
            "  📦 배치 크기: 10\n",
            "  🌐 API 보강: 사용\n",
            "\n",
            "📥 1단계: 덤프 데이터 수집\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "페이지 수집: 50it [00:00, 119.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 50개 페이지를 5개 배치로 수집\n",
            "\n",
            "⚡ 2단계: 멀티프로세싱 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "배치 처리: 100%|██████████| 5/5 [00:11<00:00,  2.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🌐 3단계: API 보강 처리\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "API 보강: 100%|██████████| 50/50 [00:25<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🎉 파싱 완료!\n",
            "  ⏱️  총 시간: 38.0초\n",
            "  📊 처리 속도: 1.3 페이지/초\n",
            "  📄 총 페이지: 50개\n",
            "\n",
            "✅ 대량 처리 완료!\n",
            "처리된 페이지: 50개\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 🚀 대량 처리 실행\n",
        "print(\"🚀 실습 3: 대량 처리\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 처리할 페이지 수 설정 (필요에 따라 조정)\n",
        "BULK_LIMIT = 50  # 50개 페이지 처리\n",
        "API_ENHANCEMENT = True  # API 보강 사용 여부\n",
        "\n",
        "print(f\"📊 설정:\")\n",
        "print(f\"  처리 페이지: {BULK_LIMIT}개\")\n",
        "print(f\"  API 보강: {'사용' if API_ENHANCEMENT else '사용 안함'}\")\n",
        "print(f\"  CPU 활용: {mp.cpu_count()}개 코어 최대 활용\")\n",
        "\n",
        "# 대량 처리 실행\n",
        "bulk_results = parse_wikisource(\n",
        "    dump_file,\n",
        "    limit=BULK_LIMIT,\n",
        "    enable_api=API_ENHANCEMENT,\n",
        "    batch_size=10  # 배치 크기\n",
        ")\n",
        "\n",
        "print(f\"\\n✅ 대량 처리 완료!\")\n",
        "print(f\"처리된 페이지: {len(bulk_results)}개\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_section"
      },
      "source": [
        "## 📊 대량 처리 결과 분석\n",
        "\n",
        "처리된 데이터의 통계와 샘플을 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "save_results",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477e7d12-b826-4207-b222-31eb05f2e33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 대량 처리 결과 분석\n",
            "==================================================\n",
            "📈 처리 통계:\n",
            "  총 페이지: 50개\n",
            "  API 보강된 페이지: 28개 (56.0%)\n",
            "  연도 정보 있는 페이지: 26개 (52.0%)\n",
            "  저자 정보 있는 페이지: 8개 (16.0%)\n",
            "  실질적 본문 있는 페이지: 42개 (84.0%)\n",
            "\n",
            "📂 상위 10개 분류:\n",
            "  PD-South Korea-exempt: 11개\n",
            "  연도 미입력 작품: 6개\n",
            "  도움말: 4개\n",
            "  동음이의어 문서: 4개\n",
            "  PD-old-100: 3개\n",
            "  대한민국의 일부개정된 법률: 3개\n",
            "  대한민국의 개정된 법률: 3개\n",
            "  PD-old-70: 3개\n",
            "  한글: 2개\n",
            "  대한민국의 타법개정된 법률: 2개\n",
            "\n",
            "📅 연도별 분포:\n",
            "  1429년: 1개\n",
            "  1446년: 1개\n",
            "  1446년: 1개\n",
            "  1459년: 1개\n",
            "  1541년: 1개\n",
            "  1926년: 1개\n",
            "  1933년: 1개\n",
            "  1934년: 1개\n",
            "  1936년: 1개\n",
            "  1936년: 1개\n",
            "  1941년: 1개\n",
            "  1943년: 1개\n",
            "  1948년: 1개\n",
            "  1955년: 1개\n",
            "  1956년: 1개\n",
            "  1959년: 1개\n",
            "  1962년: 1개\n",
            "  1965년: 2개\n",
            "  1987년: 1개\n",
            "  1998년: 1개\n",
            "  2003년: 1개\n",
            "  2004년: 1개\n",
            "  2010년: 1개\n",
            "  2014년: 1개\n",
            "  2019년: 1개\n",
            "\n",
            "📝 샘플 페이지 (상위 10개):\n",
            "   1. 기독교 강요\n",
            "      분류: 1개, 연도: 1541, 본문: 2782자\n",
            "   2. 훈민정음\n",
            "      분류: 3개, 연도: 1446, 본문: 5551자\n",
            "   3. 대문\n",
            "      분류: 0개, 연도: 1955, 본문: 909자\n",
            "   4. 자매프로젝트\n",
            "      분류: 0개, 연도: None, 본문: 1511자\n",
            "   5. 위키미디어 재단\n",
            "      분류: 1개, 연도: 2003, 본문: 1175자\n",
            "   6. 도움말\n",
            "      분류: 1개, 연도: None, 본문: 1220자\n",
            "   7. FAQ\n",
            "      분류: 1개, 연도: None, 본문: 4023자\n",
            "   8. 저작권법 (대한민국, 제7233호)\n",
            "      분류: 4개, 연도: 2004, 본문: 37104자\n",
            "   9. 환경보호법\n",
            "      분류: 1개, 연도: None, 본문: 115자\n",
            "  10. 훈민정음언해\n",
            "      분류: 4개, 연도: 1459, 본문: 257자\n"
          ]
        }
      ],
      "source": [
        "if 'bulk_results' in locals() and bulk_results:\n",
        "    print(\"📊 대량 처리 결과 분석\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 기본 통계\n",
        "    total_pages = len(bulk_results)\n",
        "    api_enhanced = sum(1 for page in bulk_results if page.get('api_categories'))\n",
        "    year_found = sum(1 for page in bulk_results if page.get('year'))\n",
        "    authors_found = sum(1 for page in bulk_results if page.get('authors'))\n",
        "    content_pages = sum(1 for page in bulk_results if page.get('content_size', 0) > 100)\n",
        "\n",
        "    print(f\"📈 처리 통계:\")\n",
        "    print(f\"  총 페이지: {total_pages}개\")\n",
        "    print(f\"  API 보강된 페이지: {api_enhanced}개 ({api_enhanced/total_pages*100:.1f}%)\")\n",
        "    print(f\"  연도 정보 있는 페이지: {year_found}개 ({year_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  저자 정보 있는 페이지: {authors_found}개 ({authors_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  실질적 본문 있는 페이지: {content_pages}개 ({content_pages/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # 분류 통계\n",
        "    all_categories = []\n",
        "    for page in bulk_results:\n",
        "        all_categories.extend(page.get('categories', []))\n",
        "\n",
        "    category_counts = {}\n",
        "    for cat in all_categories:\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "    top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    print(f\"\\n📂 상위 10개 분류:\")\n",
        "    for cat, count in top_categories:\n",
        "        print(f\"  {cat}: {count}개\")\n",
        "\n",
        "    # 연도별 통계\n",
        "    years = [page.get('year') for page in bulk_results if page.get('year')]\n",
        "    if years:\n",
        "        year_counts = {}\n",
        "        for year in years:\n",
        "            year_counts[year] = year_counts.get(year, 0) + 1\n",
        "\n",
        "        print(f\"\\n📅 연도별 분포:\")\n",
        "        sorted_years = sorted(year_counts.items(), key=lambda x: int(x[0]) if str(x[0]).isdigit() else 0)\n",
        "        for year, count in sorted_years:\n",
        "            print(f\"  {year}년: {count}개\")\n",
        "\n",
        "    # 샘플 페이지 표시\n",
        "    print(f\"\\n📝 샘플 페이지 (상위 10개):\")\n",
        "    for i, page in enumerate(bulk_results[:10], 1):\n",
        "        title = page['title']\n",
        "        categories_count = len(page.get('categories', []))\n",
        "        year = page.get('year', 'N/A')\n",
        "        content_size = page.get('content_size', 0)\n",
        "\n",
        "        print(f\"  {i:2d}. {title}\")\n",
        "        print(f\"      분류: {categories_count}개, 연도: {year}, 본문: {content_size}자\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 대량 처리 결과가 없습니다. 먼저 위의 대량 처리를 실행하세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion_section"
      },
      "source": [
        "## 💾 대량 처리 결과 저장 (CSV + JSON)\n",
        "\n",
        "처리된 데이터를 다양한 형식으로 저장하여 후속 분석에 활용할 수 있도록 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r8nQ09hEkhzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "484e0240-6d03-435b-aa5d-08ad3cc2e642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 대량 저장 함수가 정의되었습니다!\n"
          ]
        }
      ],
      "source": [
        "def save_bulk_results(data, base_filename=\"kowikisource_bulk\"):\n",
        "    \"\"\"\n",
        "    대량 처리 결과를 다양한 형식으로 저장\n",
        "\n",
        "    저장 형식:\n",
        "    1. 완전한 JSON (모든 메타데이터 포함)\n",
        "    2. 요약 CSV (핵심 정보만)\n",
        "    3. 상세 CSV (API 보강 정보 포함)\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        print(\"❌ 저장할 데이터가 없습니다.\")\n",
        "        return\n",
        "\n",
        "    timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    print(f\"💾 대량 처리 결과 저장 중...\")\n",
        "    print(f\"📊 데이터: {len(data)}개 페이지\")\n",
        "\n",
        "    # 1. 완전한 JSON 저장 (모든 데이터)\n",
        "    json_filename = f\"{base_filename}_{timestamp}_complete.json\"\n",
        "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    file_size_mb = os.path.getsize(json_filename) / (1024 * 1024)\n",
        "    print(f\"  ✅ 완전한 JSON: {json_filename} ({file_size_mb:.1f}MB)\")\n",
        "\n",
        "    # 2. 요약 CSV 저장 (핵심 정보만)\n",
        "    summary_csv_data = []\n",
        "    for page in data:\n",
        "        summary_csv_data.append({\n",
        "            'page_id': page['page_id'],\n",
        "            'title': page['title'],\n",
        "            'url': page['url'],\n",
        "            'namespace': page['namespace'],\n",
        "            'authors': ', '.join(page['authors']) if page['authors'] else '',\n",
        "            'categories': ', '.join(page['categories']) if page['categories'] else '',\n",
        "            'composer': page.get('composer', ''),\n",
        "            'translator': page.get('translator', ''),\n",
        "            'year': page.get('year', ''),\n",
        "            'license': page.get('license', ''),\n",
        "            'language': page.get('language', ''),\n",
        "            'content_length': page.get('content_size', 0),\n",
        "            'last_modified': page.get('last_modified', ''),\n",
        "            'last_contributor': page.get('last_contributor', '')\n",
        "        })\n",
        "\n",
        "    summary_csv_filename = f\"{base_filename}_{timestamp}_summary.csv\"\n",
        "    df_summary = pd.DataFrame(summary_csv_data)\n",
        "    df_summary.to_csv(summary_csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"  ✅ 요약 CSV: {summary_csv_filename}\")\n",
        "\n",
        "    # 3. 상세 CSV 저장 (API 보강 정보 포함)\n",
        "    detailed_csv_data = []\n",
        "    for page in data:\n",
        "        # API 보강 정보 분리\n",
        "        dump_categories = [cat for cat in page.get('categories', []) if cat not in page.get('api_categories', [])]\n",
        "        api_categories = page.get('api_categories', [])\n",
        "\n",
        "        detailed_csv_data.append({\n",
        "            'page_id': page['page_id'],\n",
        "            'title': page['title'],\n",
        "            'url': page['url'],\n",
        "            'namespace': page['namespace'],\n",
        "            'authors': ', '.join(page['authors']) if page['authors'] else '',\n",
        "            'dump_categories': ', '.join(dump_categories),\n",
        "            'api_categories': ', '.join(api_categories),\n",
        "            'all_categories': ', '.join(page['categories']) if page['categories'] else '',\n",
        "            'composer': page.get('composer', ''),\n",
        "            'year_final': page.get('year', ''),\n",
        "            'year_from_categories': page.get('year_from_categories', ''),\n",
        "            'year_from_wikidata': page.get('year_from_wikidata', ''),\n",
        "            'license': page.get('license', ''),\n",
        "            'language': page.get('language', ''),\n",
        "            'content_length': page.get('content_size', 0),\n",
        "            'raw_content_length': page.get('size', 0),\n",
        "            'last_modified': page.get('last_modified', ''),\n",
        "            'last_contributor': page.get('last_contributor', '')\n",
        "        })\n",
        "\n",
        "    detailed_csv_filename = f\"{base_filename}_{timestamp}_detailed.csv\"\n",
        "    df_detailed = pd.DataFrame(detailed_csv_data)\n",
        "    df_detailed.to_csv(detailed_csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"  ✅ 상세 CSV: {detailed_csv_filename}\")\n",
        "\n",
        "    # 4. 메타데이터 요약 저장\n",
        "    metadata = {\n",
        "        'generation_info': {\n",
        "            'timestamp': timestamp,\n",
        "            'total_pages': len(data),\n",
        "            'api_enhanced_pages': sum(1 for page in data if page.get('api_categories')),\n",
        "            'pages_with_year': sum(1 for page in data if page.get('year')),\n",
        "            'pages_with_authors': sum(1 for page in data if page.get('authors')),\n",
        "            'pages_with_content': sum(1 for page in data if page.get('content_size', 0) > 100)\n",
        "        },\n",
        "        'files_generated': {\n",
        "            'complete_json': json_filename,\n",
        "            'summary_csv': summary_csv_filename,\n",
        "            'detailed_csv': detailed_csv_filename\n",
        "        }\n",
        "    }\n",
        "\n",
        "    metadata_filename = f\"{base_filename}_{timestamp}_metadata.json\"\n",
        "    with open(metadata_filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"  ✅ 메타데이터: {metadata_filename}\")\n",
        "\n",
        "    print(f\"\\n📁 생성된 파일들:\")\n",
        "    print(f\"  📄 {json_filename} - 완전한 데이터 (JSON)\")\n",
        "    print(f\"  📊 {summary_csv_filename} - 핵심 정보 (CSV)\")\n",
        "    print(f\"  📈 {detailed_csv_filename} - API 보강 정보 포함 (CSV)\")\n",
        "    print(f\"  ℹ️  {metadata_filename} - 생성 정보 (JSON)\")\n",
        "\n",
        "    return {\n",
        "        'json_file': json_filename,\n",
        "        'summary_csv': summary_csv_filename,\n",
        "        'detailed_csv': detailed_csv_filename,\n",
        "        'metadata_file': metadata_filename\n",
        "    }\n",
        "\n",
        "print(\"✅ 대량 저장 함수가 정의되었습니다!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ETxyif4WkhzW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f29ab9-ab52-40a2-e565-3b77190b3b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 대량 처리 결과를 여러 형식으로 저장합니다...\n",
            "💾 대량 처리 결과 저장 중...\n",
            "📊 데이터: 50개 페이지\n",
            "  ✅ 완전한 JSON: kowikisource_bulk_20250911_024137_complete.json (3.3MB)\n",
            "  ✅ 요약 CSV: kowikisource_bulk_20250911_024137_summary.csv\n",
            "  ✅ 상세 CSV: kowikisource_bulk_20250911_024137_detailed.csv\n",
            "  ✅ 메타데이터: kowikisource_bulk_20250911_024137_metadata.json\n",
            "\n",
            "📁 생성된 파일들:\n",
            "  📄 kowikisource_bulk_20250911_024137_complete.json - 완전한 데이터 (JSON)\n",
            "  📊 kowikisource_bulk_20250911_024137_summary.csv - 핵심 정보 (CSV)\n",
            "  📈 kowikisource_bulk_20250911_024137_detailed.csv - API 보강 정보 포함 (CSV)\n",
            "  ℹ️  kowikisource_bulk_20250911_024137_metadata.json - 생성 정보 (JSON)\n",
            "\n",
            "🎯 파일 활용 가이드:\n",
            "  📊 pandas 분석:\n",
            "     df = pd.read_csv('kowikisource_bulk_20250911_024137_summary.csv')\n",
            "     df.head()\n",
            "\n",
            "  🔍 상세 분석:\n",
            "     detailed_df = pd.read_csv('kowikisource_bulk_20250911_024137_detailed.csv')\n",
            "     # API 보강 효과 확인\n",
            "     detailed_df[['dump_categories', 'api_categories']].head()\n",
            "\n",
            "  📄 완전한 데이터:\n",
            "     with open('kowikisource_bulk_20250911_024137_complete.json', 'r') as f:\n",
            "         data = json.load(f)\n",
            "\n",
            "✅ 저장 완료 확인:\n",
            "  📊 CSV 파일: 50행 × 14열\n",
            "     주요 컬럼: page_id, title, url, namespace, authors...\n",
            "  📄 JSON 파일: 50개 항목\n",
            "     키 개수: 23개\n"
          ]
        }
      ],
      "source": [
        "# 대량 처리 결과 저장 실행\n",
        "if 'bulk_results' in locals() and bulk_results:\n",
        "    print(\"💾 대량 처리 결과를 여러 형식으로 저장합니다...\")\n",
        "    saved_files = save_bulk_results(bulk_results)\n",
        "\n",
        "    print(f\"\\n🎯 파일 활용 가이드:\")\n",
        "    print(f\"  📊 pandas 분석:\")\n",
        "    print(f\"     df = pd.read_csv('{saved_files['summary_csv']}')\")\n",
        "    print(f\"     df.head()\")\n",
        "\n",
        "    print(f\"\\n  🔍 상세 분석:\")\n",
        "    print(f\"     detailed_df = pd.read_csv('{saved_files['detailed_csv']}')\")\n",
        "    print(f\"     # API 보강 효과 확인\")\n",
        "    print(f\"     detailed_df[['dump_categories', 'api_categories']].head()\")\n",
        "\n",
        "    print(f\"\\n  📄 완전한 데이터:\")\n",
        "    print(f\"     with open('{saved_files['json_file']}', 'r') as f:\")\n",
        "    print(f\"         data = json.load(f)\")\n",
        "\n",
        "    # 간단한 데이터 검증\n",
        "    print(f\"\\n✅ 저장 완료 확인:\")\n",
        "\n",
        "    # CSV 파일 검증\n",
        "    try:\n",
        "        test_df = pd.read_csv(saved_files['summary_csv'])\n",
        "        print(f\"  📊 CSV 파일: {len(test_df)}행 × {len(test_df.columns)}열\")\n",
        "        print(f\"     주요 컬럼: {', '.join(test_df.columns[:5])}...\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ CSV 검증 실패: {e}\")\n",
        "\n",
        "    # JSON 파일 검증\n",
        "    try:\n",
        "        with open(saved_files['json_file'], 'r', encoding='utf-8') as f:\n",
        "            test_json = json.load(f)\n",
        "        print(f\"  📄 JSON 파일: {len(test_json)}개 항목\")\n",
        "        if test_json:\n",
        "            print(f\"     키 개수: {len(test_json[0].keys())}개\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ JSON 검증 실패: {e}\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 저장할 대량 처리 결과가 없습니다.\")\n",
        "    print(\"💡 위의 '대량 처리 실행' 셀을 먼저 실행하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2eTJaEMkhzW"
      },
      "source": [
        "## 🔍 애국가 가사 검증\n",
        "\n",
        "사용자 요청에 따라 애국가 본문(가사)이 제대로 파싱되었는지 확인해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "xoWQ6sq5khzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "835e5e89-c229-4984-e1d4-104d6f2ac6a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 애국가 가사 검증\n",
            "==================================================\n",
            "📄 페이지: 애국가 (대한민국)\n",
            "🌐 URL: https://ko.wikisource.org/wiki/%EC%95%A0%EA%B5%AD%EA%B0%80_%28%EB%8C%80%ED%95%9C%EB%AF%BC%EA%B5%AD%29\n",
            "📏 전체 본문 길이: 426 문자\n",
            "📏 원본 길이: 866 문자\n",
            "\n",
            "📖 정제된 본문:\n",
            "한자 혼용 ;1 :東海물과 白頭山이 마르고 닳도록하느님이 保佑하사 우리 나라 萬歲 :無窮花 三千里 華麗江山大韓사람 大韓으로 길이 保全하세 ;2 :南山 위에 저 소나무 鐵甲을 두른 듯바람 서리 不變함은 우리 氣像일세 ;3 :가을 하늘 空豁한데 높고 구름 없이밝은 달은 우리 가슴 一片丹心일세 ;4 :이 氣像과 이 맘으로 忠誠을 다하여괴로우나 즐거우나 나라 사랑하세 한글 ;1 :동해 물과 백두산이 마르고 닳도록하느님이 보우하사 우리나라 만세 :무궁화 삼천리 화려강산대한 사람 대한으로 길이 보전하세 ;2 :남산 위에 저 소나무 철갑을 두른 듯바람 서리 불변함은 우리 기상일세 ;3 :가을 하늘 공활한데 높고 구름 없이밝은 달은 우리 가슴 일편단심일세 ;4 :이 기상과 이 맘으로 충성을 다하여괴로우나 즐거우나 나라 사랑하세 라이선스 분류:국가 분류:대한민국의 노래\n",
            "\n",
            "🎵 가사 키워드 확인:\n",
            "  ❌ '동해물과' 없음\n",
            "  ✅ '백두산이' 발견됨\n",
            "  ✅ '하느님이' 발견됨\n",
            "  ✅ '보우하사' 발견됨\n",
            "  ✅ '무궁화' 발견됨\n",
            "\n",
            "🔤 언어 분석:\n",
            "  ✅ 한자 구문 발견: 17개\n",
            "     예시: 東海, 白頭山, 保佑...\n",
            "  ✅ 한글 구문 발견: 94개\n",
            "     예시: 한자, 혼용, 물과...\n",
            "\n",
            "📝 가사 구조:\n",
            "  전체 줄 수: 1줄\n",
            "  의미있는 줄: 1줄\n",
            "\n",
            "🎼 가사 내용 (처음 10줄):\n",
            "   1. 한자 혼용 ;1 :東海물과 白頭山이 마르고 닳도록하느님이 保佑하사 우리 나라 萬歲 :無窮花 三千里 華麗江山大韓사람 大韓으로 길이 保全하세 ;2 :南山 위에 저 소나무 鐵甲을 두른 듯바람 서리 不變함은 우리 氣像일세 ;3 :가을 하늘 空豁한데 높고 구름 없이밝은 달은 우리 가슴 一片丹心일세 ;4 :이 氣像과 이 맘으로 忠誠을 다하여괴로우나 즐거우나 나라 사랑하세 한글 ;1 :동해 물과 백두산이 마르고 닳도록하느님이 보우하사 우리나라 만세 :무궁화 삼천리 화려강산대한 사람 대한으로 길이 보전하세 ;2 :남산 위에 저 소나무 철갑을 두른 듯바람 서리 불변함은 우리 기상일세 ;3 :가을 하늘 공활한데 높고 구름 없이밝은 달은 우리 가슴 일편단심일세 ;4 :이 기상과 이 맘으로 충성을 다하여괴로우나 즐거우나 나라 사랑하세 라이선스 분류:국가 분류:대한민국의 노래\n",
            "\n",
            "🎯 파싱 성공 여부:\n",
            "  ✅ 주요 가사 키워드 4/5개 발견\n",
            "  ✅ 한자 가사 보존됨\n",
            "  ✅ 한글 가사 보존됨\n",
            "  ✅ 충분한 본문 길이 (426자)\n",
            "  ❌ 가사 구조 보존 (1줄)\n",
            "\n",
            "⚠️  일부 문제가 발견되었습니다.\n"
          ]
        }
      ],
      "source": [
        "# 애국가 본문 검증\n",
        "print(\"🔍 애국가 가사 검증\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# API 보강 결과에서 애국가 확인\n",
        "if 'api_results' in locals() and api_results:\n",
        "    aegukga = api_results[0]\n",
        "\n",
        "    print(f\"📄 페이지: {aegukga['title']}\")\n",
        "    print(f\"🌐 URL: {aegukga['url']}\")\n",
        "    print(f\"📏 전체 본문 길이: {aegukga['content_size']} 문자\")\n",
        "    print(f\"📏 원본 길이: {aegukga.get('size', 0)} 문자\")\n",
        "\n",
        "    # 본문 내용 확인\n",
        "    content = aegukga['content']\n",
        "    raw_content = aegukga.get('raw_content', '')\n",
        "\n",
        "    print(f\"\\n📖 정제된 본문:\")\n",
        "    print(f\"{content}\")\n",
        "\n",
        "    # 가사 키워드 확인\n",
        "    lyrics_keywords = ['동해물과', '백두산이', '하느님이', '보우하사', '무궁화']\n",
        "    found_keywords = []\n",
        "\n",
        "    for keyword in lyrics_keywords:\n",
        "        if keyword in content:\n",
        "            found_keywords.append(keyword)\n",
        "\n",
        "    print(f\"\\n🎵 가사 키워드 확인:\")\n",
        "    for keyword in lyrics_keywords:\n",
        "        status = '✅' if keyword in content else '❌'\n",
        "        print(f\"  {status} '{keyword}' {'발견됨' if keyword in content else '없음'}\")\n",
        "\n",
        "    # 한자/한글 분석\n",
        "    import re\n",
        "\n",
        "    # 한자 패턴 확인\n",
        "    hanja_pattern = r'[\\u4e00-\\u9fff]+'\n",
        "    hangul_pattern = r'[\\uac00-\\ud7af]+'\n",
        "\n",
        "    hanja_matches = re.findall(hanja_pattern, content)\n",
        "    hangul_matches = re.findall(hangul_pattern, content)\n",
        "\n",
        "    print(f\"\\n🔤 언어 분석:\")\n",
        "    print(f\"  ✅ 한자 구문 발견: {len(hanja_matches)}개\")\n",
        "    if hanja_matches:\n",
        "        print(f\"     예시: {', '.join(hanja_matches[:3])}...\")\n",
        "\n",
        "    print(f\"  ✅ 한글 구문 발견: {len(hangul_matches)}개\")\n",
        "    if hangul_matches:\n",
        "        print(f\"     예시: {', '.join(hangul_matches[:3])}...\")\n",
        "\n",
        "    # 가사 구조 확인\n",
        "    lines = [line.strip() for line in content.split('\\n') if line.strip()]\n",
        "    meaningful_lines = [line for line in lines if len(line) > 3]\n",
        "\n",
        "    print(f\"\\n📝 가사 구조:\")\n",
        "    print(f\"  전체 줄 수: {len(lines)}줄\")\n",
        "    print(f\"  의미있는 줄: {len(meaningful_lines)}줄\")\n",
        "\n",
        "    print(f\"\\n🎼 가사 내용 (처음 10줄):\")\n",
        "    for i, line in enumerate(meaningful_lines[:10], 1):\n",
        "        print(f\"  {i:2d}. {line}\")\n",
        "\n",
        "    # 성공 여부 판단\n",
        "    success_criteria = [\n",
        "        (len(found_keywords) >= 3, f\"주요 가사 키워드 {len(found_keywords)}/5개 발견\"),\n",
        "        (len(hanja_matches) > 0, \"한자 가사 보존됨\"),\n",
        "        (len(hangul_matches) > 0, \"한글 가사 보존됨\"),\n",
        "        (aegukga['content_size'] > 300, f\"충분한 본문 길이 ({aegukga['content_size']}자)\"),\n",
        "        (len(meaningful_lines) >= 5, f\"가사 구조 보존 ({len(meaningful_lines)}줄)\")\n",
        "    ]\n",
        "\n",
        "    print(f\"\\n🎯 파싱 성공 여부:\")\n",
        "    all_success = True\n",
        "    for success, description in success_criteria:\n",
        "        status = '✅' if success else '❌'\n",
        "        print(f\"  {status} {description}\")\n",
        "        if not success:\n",
        "            all_success = False\n",
        "\n",
        "    if all_success:\n",
        "        print(f\"\\n🎉 애국가 가사 파싱 완벽 성공!\")\n",
        "        print(f\"✅ 한자 가사 보존됨\")\n",
        "        print(f\"✅ 한글 가사 보존됨\")\n",
        "        print(f\"✅ 가사 구조 완전 보존됨\")\n",
        "    else:\n",
        "        print(f\"\\n⚠️  일부 문제가 발견되었습니다.\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 애국가 데이터를 찾을 수 없습니다.\")\n",
        "    print(\"💡 먼저 'API 보강 파싱' 셀을 실행하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1i6Dn8yukhzX"
      },
      "source": [
        "## 🚀 전체 위키문헌 파싱 (고급)\n",
        "\n",
        "**주의**: 이 섹션은 전체 한국어 위키문헌을 파싱합니다. 수천 개의 페이지를 처리하므로 시간이 오래 걸릴 수 있습니다.\n",
        "\n",
        "### ⚠️ 실행 전 확인사항:\n",
        "- Colab Pro 사용 권장 (더 많은 메모리와 시간 제한)\n",
        "- 안정적인 인터넷 연결 필요 (API 호출)\n",
        "- 완료까지 30분-2시간 소요 예상"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "qqLzRPD_khzX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3029417e-dc0e-4252-8432-0d1ebac2fa9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 전체 위키문헌 파싱 준비\n",
            "==================================================\n",
            "📊 설정:\n",
            "  API 보강: ✅\n",
            "  배치 크기: 20\n",
            "  CPU 활용: 2개 코어\n",
            "  중간 저장: 1000개마다\n",
            "\n",
            "📈 예상 정보:\n",
            "  덤프 크기: 147.8MB\n",
            "  예상 페이지: ~1,477개\n",
            "  예상 시간: 15-30분 (API 포함)\n",
            "\n",
            "⚠️  전체 파싱을 실행하려면:\n",
            "  1. 아래 EXECUTE_FULL_PARSING을 True로 변경\n",
            "  2. 셀을 다시 실행\n",
            "  3. 진행상황을 모니터링\n",
            "\n",
            "🔥 전체 파싱을 시작합니다...\n"
          ]
        }
      ],
      "source": [
        "# 전체 위키문헌 파싱 설정\n",
        "print(\"🚀 전체 위키문헌 파싱 준비\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 파싱 설정\n",
        "FULL_PARSING_CONFIG = {\n",
        "    'enable_api': True,           # API 보강 사용 (권장)\n",
        "    'batch_size': 20,            # 배치 크기 (메모리 고려)\n",
        "    'api_delay': 0.05,           # API 호출 간격 (초)\n",
        "    'save_interval': 1000,       # 중간 저장 간격\n",
        "    'max_retries': 3,            # API 재시도 횟수\n",
        "}\n",
        "\n",
        "print(f\"📊 설정:\")\n",
        "print(f\"  API 보강: {'✅' if FULL_PARSING_CONFIG['enable_api'] else '❌'}\")\n",
        "print(f\"  배치 크기: {FULL_PARSING_CONFIG['batch_size']}\")\n",
        "print(f\"  CPU 활용: {mp.cpu_count()}개 코어\")\n",
        "print(f\"  중간 저장: {FULL_PARSING_CONFIG['save_interval']}개마다\")\n",
        "\n",
        "# 예상 시간 계산\n",
        "import os\n",
        "dump_size = os.path.getsize(dump_file) / (1024 * 1024)  # MB\n",
        "estimated_pages = int(dump_size * 10)  # 대략적 추정\n",
        "\n",
        "print(f\"\\n📈 예상 정보:\")\n",
        "print(f\"  덤프 크기: {dump_size:.1f}MB\")\n",
        "print(f\"  예상 페이지: ~{estimated_pages:,}개\")\n",
        "print(f\"  예상 시간: {estimated_pages/100:.0f}-{estimated_pages/50:.0f}분 (API 포함)\")\n",
        "\n",
        "# 사용자 확인\n",
        "print(f\"\\n⚠️  전체 파싱을 실행하려면:\")\n",
        "print(f\"  1. 아래 EXECUTE_FULL_PARSING을 True로 변경\")\n",
        "print(f\"  2. 셀을 다시 실행\")\n",
        "print(f\"  3. 진행상황을 모니터링\")\n",
        "\n",
        "# 안전장치\n",
        "EXECUTE_FULL_PARSING = True  # 전체 파싱 실행하려면 True로 변경\n",
        "\n",
        "if EXECUTE_FULL_PARSING:\n",
        "    print(f\"\\n🔥 전체 파싱을 시작합니다...\")\n",
        "else:\n",
        "    print(f\"\\n💡 아직 실행되지 않습니다. EXECUTE_FULL_PARSING = True로 설정하세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVGMd4vEkhzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0964b74-6744-40f4-e53b-da375304a503"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 전체 위키문헌 파싱을 시작합니다!\n",
            "이 작업은 오래 걸릴 수 있습니다. 진행상황을 확인하세요.\n",
            "🚀 전체 위키문헌 파싱 시작!\n",
            "  📊 CPU 코어: 2개 활용\n",
            "  🌐 API 보강: 사용\n",
            "  💾 중간 저장: 1000개마다\n",
            "\n",
            "📥 1단계: 전체 덤프 데이터 수집\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "전체 페이지 수집: 19825it [01:14, 503.15it/s]"
          ]
        }
      ],
      "source": [
        "# 향상된 전체 파싱 함수 (중간 저장 포함)\n",
        "def parse_full_wikisource(dump_file, config):\n",
        "    \"\"\"\n",
        "    전체 위키문헌을 파싱하고 중간에 저장하는 함수\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"🚀 전체 위키문헌 파싱 시작!\")\n",
        "    print(f\"  📊 CPU 코어: {mp.cpu_count()}개 활용\")\n",
        "    print(f\"  🌐 API 보강: {'사용' if config['enable_api'] else '사용 안함'}\")\n",
        "    print(f\"  💾 중간 저장: {config['save_interval']}개마다\")\n",
        "\n",
        "    # 1단계: 모든 페이지 수집\n",
        "    print(f\"\\n📥 1단계: 전체 덤프 데이터 수집\")\n",
        "    all_pages = []\n",
        "    total_pages = 0\n",
        "\n",
        "    with bz2.open(dump_file, 'rt', encoding='utf-8') as f:\n",
        "        dump = mwxml.Dump.from_file(f)\n",
        "\n",
        "        for page in tqdm(dump, desc=\"전체 페이지 수집\"):\n",
        "            try:\n",
        "                # 리비전 데이터 수집\n",
        "                revisions = []\n",
        "                for revision in page:\n",
        "                    revisions.append((\n",
        "                        revision.id,\n",
        "                        revision.timestamp,\n",
        "                        revision.user.text if revision.user else None,\n",
        "                        revision.comment,\n",
        "                        revision.text,\n",
        "                        revision.bytes\n",
        "                    ))\n",
        "                    break\n",
        "\n",
        "                if revisions:  # 리비전이 있는 페이지만\n",
        "                    page_data = (\n",
        "                        page.id,\n",
        "                        page.title,\n",
        "                        page.namespace,\n",
        "                        str(page.redirect.title) if page.redirect else None,\n",
        "                        revisions\n",
        "                    )\n",
        "                    all_pages.append(page_data)\n",
        "                    total_pages += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    print(f\"✅ 총 {total_pages:,}개 페이지 수집 완료\")\n",
        "\n",
        "    # 2단계: 배치로 나누기\n",
        "    batch_size = config['batch_size']\n",
        "    batches = [all_pages[i:i+batch_size] for i in range(0, len(all_pages), batch_size)]\n",
        "    print(f\"📦 {len(batches)}개 배치로 분할 (배치당 {batch_size}개)\")\n",
        "\n",
        "    # 3단계: 멀티프로세싱 처리\n",
        "    print(f\"\\n⚡ 2단계: 멀티프로세싱 배치 처리\")\n",
        "    all_results = []\n",
        "    processed_count = 0\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=mp.cpu_count()) as executor:\n",
        "        # 모든 배치를 병렬로 처리\n",
        "        futures = [executor.submit(process_pages_batch, batch) for batch in batches]\n",
        "\n",
        "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"배치 처리\"):\n",
        "            try:\n",
        "                batch_results = future.result()\n",
        "                all_results.extend(batch_results)\n",
        "                processed_count += len(batch_results)\n",
        "\n",
        "                # 중간 저장\n",
        "                if processed_count % config['save_interval'] == 0:\n",
        "                    temp_filename = f\"temp_wikisource_{processed_count}.json\"\n",
        "                    with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
        "                    print(f\"\\n💾 중간 저장: {temp_filename} ({processed_count:,}개 페이지)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"배치 처리 오류: {e}\")\n",
        "                continue\n",
        "\n",
        "    print(f\"✅ 기본 파싱 완료: {len(all_results):,}개 페이지\")\n",
        "\n",
        "    # 4단계: API 보강 (선택적)\n",
        "    if config['enable_api'] and all_results:\n",
        "        print(f\"\\n🌐 3단계: API 보강 처리\")\n",
        "        enhanced_results = []\n",
        "        api_success = 0\n",
        "\n",
        "        for i, page_data in enumerate(tqdm(all_results, desc=\"API 보강\")):\n",
        "            try:\n",
        "                enhanced = enhance_with_api(page_data)\n",
        "                enhanced_results.append(enhanced)\n",
        "\n",
        "                # API 성공 카운트\n",
        "                if enhanced.get('api_categories'):\n",
        "                    api_success += 1\n",
        "\n",
        "                # API 호출 제한\n",
        "                time.sleep(config['api_delay'])\n",
        "\n",
        "                # 중간 저장 (API 보강 버전)\n",
        "                if (i + 1) % config['save_interval'] == 0:\n",
        "                    temp_filename = f\"temp_wikisource_api_{i+1}.json\"\n",
        "                    with open(temp_filename, 'w', encoding='utf-8') as f:\n",
        "                        json.dump(enhanced_results, f, ensure_ascii=False, indent=2)\n",
        "                    print(f\"\\n💾 API 보강 중간 저장: {temp_filename} ({i+1:,}개 페이지)\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"API 보강 오류 ({page_data.get('title', 'Unknown')}): {e}\")\n",
        "                enhanced_results.append(page_data)\n",
        "\n",
        "        all_results = enhanced_results\n",
        "        print(f\"✅ API 보강 완료: {api_success:,}/{len(all_results):,}개 성공 ({api_success/len(all_results)*100:.1f}%)\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\n🎉 전체 파싱 완료!\")\n",
        "    print(f\"  ⏱️  총 시간: {total_time/60:.1f}분\")\n",
        "    print(f\"  📊 처리 속도: {len(all_results)/(total_time/60):.1f} 페이지/분\")\n",
        "    print(f\"  📄 총 페이지: {len(all_results):,}개\")\n",
        "\n",
        "    return all_results\n",
        "\n",
        "# 전체 파싱 실행\n",
        "if EXECUTE_FULL_PARSING:\n",
        "    print(\"🔥 전체 위키문헌 파싱을 시작합니다!\")\n",
        "    print(\"이 작업은 오래 걸릴 수 있습니다. 진행상황을 확인하세요.\")\n",
        "\n",
        "    full_results = parse_full_wikisource(dump_file, FULL_PARSING_CONFIG)\n",
        "\n",
        "    print(f\"\\n✅ 전체 파싱이 완료되었습니다!\")\n",
        "    print(f\"변수 'full_results'에 {len(full_results):,}개 페이지가 저장되었습니다.\")\n",
        "\n",
        "else:\n",
        "    print(\"⏸️  전체 파싱이 비활성화되어 있습니다.\")\n",
        "    print(\"EXECUTE_FULL_PARSING = True로 설정하고 다시 실행하세요.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCGuUXhjkhzY"
      },
      "source": [
        "## 📊 전체 파싱 결과 분석\n",
        "\n",
        "전체 위키문헌 파싱이 완료된 후 상세한 통계와 분석을 확인합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "n24XDQHNkhzY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caa772ee-42ab-4b7a-d7ed-6eb14cc210d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ 전체 파싱 결과가 없습니다.\n",
            "💡 먼저 전체 파싱을 실행하세요.\n"
          ]
        }
      ],
      "source": [
        "# 전체 파싱 결과 분석\n",
        "if 'full_results' in locals() and full_results:\n",
        "    print(\"📊 전체 위키문헌 파싱 결과 상세 분석\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 기본 통계\n",
        "    total_pages = len(full_results)\n",
        "    api_enhanced = sum(1 for page in full_results if page.get('api_categories'))\n",
        "    year_found = sum(1 for page in full_results if page.get('year'))\n",
        "    authors_found = sum(1 for page in full_results if page.get('authors'))\n",
        "    content_pages = sum(1 for page in full_results if page.get('content_size', 0) > 100)\n",
        "    substantial_content = sum(1 for page in full_results if page.get('content_size', 0) > 1000)\n",
        "\n",
        "    print(f\"📈 기본 통계:\")\n",
        "    print(f\"  총 페이지: {total_pages:,}개\")\n",
        "    print(f\"  API 보강된 페이지: {api_enhanced:,}개 ({api_enhanced/total_pages*100:.1f}%)\")\n",
        "    print(f\"  연도 정보 있는 페이지: {year_found:,}개 ({year_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  저자 정보 있는 페이지: {authors_found:,}개 ({authors_found/total_pages*100:.1f}%)\")\n",
        "    print(f\"  본문 있는 페이지: {content_pages:,}개 ({content_pages/total_pages*100:.1f}%)\")\n",
        "    print(f\"  실질적 본문(1000자+): {substantial_content:,}개 ({substantial_content/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # 네임스페이스 분석\n",
        "    namespace_counts = {}\n",
        "    for page in full_results:\n",
        "        ns = page.get('namespace', 0)\n",
        "        namespace_counts[ns] = namespace_counts.get(ns, 0) + 1\n",
        "\n",
        "    print(f\"\\n📂 네임스페이스 분포:\")\n",
        "    for ns, count in sorted(namespace_counts.items()):\n",
        "        ns_name = {0: '본문', 100: '저자', 102: '색인', 104: '번역'}.get(ns, f'기타({ns})')\n",
        "        print(f\"  {ns_name}: {count:,}개 ({count/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # 분류 분석\n",
        "    all_categories = []\n",
        "    for page in full_results:\n",
        "        all_categories.extend(page.get('categories', []))\n",
        "\n",
        "    category_counts = {}\n",
        "    for cat in all_categories:\n",
        "        category_counts[cat] = category_counts.get(cat, 0) + 1\n",
        "\n",
        "    top_categories = sorted(category_counts.items(), key=lambda x: x[1], reverse=True)[:20]\n",
        "\n",
        "    print(f\"\\n📂 상위 20개 분류:\")\n",
        "    for i, (cat, count) in enumerate(top_categories, 1):\n",
        "        print(f\"  {i:2d}. {cat}: {count:,}개\")\n",
        "\n",
        "    # 연도별 분포\n",
        "    years = [page.get('year') for page in full_results if page.get('year')]\n",
        "    if years:\n",
        "        year_counts = {}\n",
        "        for year in years:\n",
        "            try:\n",
        "                year_int = int(year)\n",
        "                decade = (year_int // 10) * 10\n",
        "                year_counts[decade] = year_counts.get(decade, 0) + 1\n",
        "            except (ValueError, TypeError):\n",
        "                continue\n",
        "\n",
        "        print(f\"\\n📅 연대별 작품 분포:\")\n",
        "        for decade in sorted(year_counts.keys()):\n",
        "            count = year_counts[decade]\n",
        "            print(f\"  {decade}년대: {count:,}개\")\n",
        "\n",
        "    # 언어 분석\n",
        "    language_counts = {}\n",
        "    for page in full_results:\n",
        "        lang = page.get('language', '미분류')\n",
        "        language_counts[lang] = language_counts.get(lang, 0) + 1\n",
        "\n",
        "    print(f\"\\n🔤 언어 분포:\")\n",
        "    for lang, count in sorted(language_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        if lang and count > 0:\n",
        "            print(f\"  {lang}: {count:,}개 ({count/total_pages*100:.1f}%)\")\n",
        "\n",
        "    # 본문 길이 분석\n",
        "    content_lengths = [page.get('content_size', 0) for page in full_results]\n",
        "    content_lengths = [x for x in content_lengths if x > 0]\n",
        "\n",
        "    if content_lengths:\n",
        "        import statistics\n",
        "        avg_length = statistics.mean(content_lengths)\n",
        "        median_length = statistics.median(content_lengths)\n",
        "        max_length = max(content_lengths)\n",
        "\n",
        "        print(f\"\\n📏 본문 길이 분석:\")\n",
        "        print(f\"  평균 길이: {avg_length:.0f}자\")\n",
        "        print(f\"  중간값: {median_length:.0f}자\")\n",
        "        print(f\"  최대 길이: {max_length:,}자\")\n",
        "\n",
        "        # 길이별 분포\n",
        "        length_ranges = [\n",
        "            (0, 100, \"매우 짧음\"),\n",
        "            (100, 1000, \"짧음\"),\n",
        "            (1000, 5000, \"중간\"),\n",
        "            (5000, 20000, \"긴 편\"),\n",
        "            (20000, float('inf'), \"매우 긴\")\n",
        "        ]\n",
        "\n",
        "        print(f\"\\n📊 길이별 분포:\")\n",
        "        for min_len, max_len, desc in length_ranges:\n",
        "            count = sum(1 for x in content_lengths if min_len <= x < max_len)\n",
        "            if count > 0:\n",
        "                print(f\"  {desc} ({min_len:,}-{max_len:,}자): {count:,}개 ({count/len(content_lengths)*100:.1f}%)\")\n",
        "\n",
        "    # 가장 긴 문서들\n",
        "    longest_pages = sorted(full_results, key=lambda x: x.get('content_size', 0), reverse=True)[:10]\n",
        "\n",
        "    print(f\"\\n📖 가장 긴 문서 Top 10:\")\n",
        "    for i, page in enumerate(longest_pages, 1):\n",
        "        title = page['title']\n",
        "        length = page.get('content_size', 0)\n",
        "        print(f\"  {i:2d}. {title}: {length:,}자\")\n",
        "\n",
        "    print(f\"\\n🎯 품질 지표:\")\n",
        "    print(f\"  완전한 메타데이터 페이지: {sum(1 for p in full_results if p.get('authors') and p.get('categories') and p.get('year')):,}개\")\n",
        "    print(f\"  API 보강 성공률: {api_enhanced/total_pages*100:.1f}%\")\n",
        "    print(f\"  유의미한 본문 비율: {content_pages/total_pages*100:.1f}%\")\n",
        "    print(f\"  구조화된 정보 비율: {year_found/total_pages*100:.1f}%\")\n",
        "\n",
        "else:\n",
        "    print(\"❌ 전체 파싱 결과가 없습니다.\")\n",
        "    print(\"💡 먼저 전체 파싱을 실행하세요.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9HvEvvIkhzZ"
      },
      "source": [
        "## 💾 전체 데이터셋 최종 저장\n",
        "\n",
        "전체 위키문헌 파싱 결과를 완전한 데이터셋으로 저장합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVYiBjynkhzZ"
      },
      "source": [
        "## 🎓 튜토리얼 완료!\n",
        "\n",
        "### 🎉 축하합니다! 다음을 배웠습니다:\n",
        "\n",
        "1. **위키문헌 XML 덤프 파싱**\n",
        "   - 위키텍스트에서 메타데이터 추출\n",
        "   - 마크업 제거하여 깔끔한 본문 생성\n",
        "   - 멀티프로세싱으로 빠른 처리\n",
        "\n",
        "2. **API 보강 기법**\n",
        "   - 위키문헌 API로 완전한 분류 정보 추출\n",
        "   - 위키데이터 API로 구조화된 연도 정보 획득\n",
        "   - 여러 소스 정보 통합 및 검증\n",
        "\n",
        "3. **실제 문제 해결**\n",
        "   - 애국가 사례: 2개 → 4개 분류 완전 추출\n",
        "   - \"1941년 작품\" 분류에서 연도 정보 추출\n",
        "   - 템플릿 기반 분류(PD-old-50) 추출\n",
        "   - 애국가 가사(한자+한글) 완전 보존 확인\n",
        "\n",
        "4. **대량 처리 및 데이터 저장**\n",
        "   - 멀티프로세싱으로 대량 페이지 처리\n",
        "   - JSON, CSV 다중 형식 저장\n",
        "   - API 보강 효과 추적 및 분석\n",
        "   - 재사용 가능한 데이터셋 생성\n",
        "\n",
        "5. **전체 위키문헌 파싱 (고급)**\n",
        "   - 수천 개 페이지의 완전한 데이터셋 생성\n",
        "   - 중간 저장으로 안정성 보장\n",
        "   - 연구용, 분석용, 텍스트용 다중 형식\n",
        "   - 완전한 메타데이터와 통계 정보\n",
        "\n",
        "### 🚀 다음 단계:\n",
        "- **소규모 연구**: 대량 처리 결과로 특정 주제 분석\n",
        "- **대규모 연구**: 전체 위키문헌 데이터셋으로 종합 분석\n",
        "- **응용 개발**: 추출된 데이터로 검색 엔진, 추천 시스템 구축\n",
        "- **다른 프로젝트**: 위키백과, 위키낱말사전 등으로 확장\n",
        "\n",
        "### 💡 핵심 교훈:\n",
        "- **단일 소스의 한계**: XML 덤프만으로는 완전한 정보 추출 어려움\n",
        "- **API 보강의 중요성**: 웹사이트와 동일한 완전한 데이터 획득\n",
        "- **검증의 필요성**: 여러 소스에서 정보를 교차 확인\n",
        "- **성능 최적화**: 멀티프로세싱으로 대량 데이터 효율적 처리\n",
        "- **데이터 품질**: 본문 내용까지 완전히 보존하는 파싱\n",
        "- **확장성 고려**: 중간 저장과 배치 처리로 대규모 데이터 안정적 처리\n",
        "\n",
        "### 📊 생성 가능한 데이터:\n",
        "- **완전한 JSON**: 모든 메타데이터와 본문 포함\n",
        "- **연구용 CSV**: 핵심 정보만 정리된 분석용 데이터\n",
        "- **상세 CSV**: API 보강 과정까지 추적 가능한 상세 데이터\n",
        "- **텍스트 CSV**: 본문 분석 전용 데이터\n",
        "- **메타데이터**: 데이터셋 통계와 품질 정보\n",
        "\n",
        "**이제 여러분도 위키문헌 데이터 과학자입니다! 🌟**\n",
        "\n",
        "---\n",
        "\n",
        "### 🔄 실행 가이드:\n",
        "\n",
        "**📝 기본 학습 (권장):**\n",
        "1. \"기본 파싱\" → \"API 보강 파싱\" → \"비교 분석\" 순서로 실행\n",
        "2. 애국가 사례로 핵심 개념 이해\n",
        "3. 소규모 대량 처리(10-50개)로 경험 쌓기\n",
        "\n",
        "**🚀 대량 처리:**\n",
        "1. `BULK_LIMIT` 값을 100-1000으로 설정\n",
        "2. 결과 분석 및 저장 실행\n",
        "3. 생성된 CSV/JSON으로 분석 실습\n",
        "\n",
        "**🔥 전체 파싱 (고급):**\n",
        "1. `EXECUTE_FULL_PARSING = True` 설정\n",
        "2. 수십 분-몇 시간 대기 (진행상황 모니터링)\n",
        "3. 완전한 한국어 위키문헌 데이터셋 획득\n",
        "4. 연구 프로젝트나 논문에 활용\n",
        "\n",
        "**⚠️ 주의사항:**\n",
        "- 전체 파싱은 많은 시간과 리소스 필요\n",
        "- Colab Pro 권장 (더 많은 메모리와 시간)\n",
        "- API 호출 제한으로 인한 지연 가능"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "final_version": {
      "created": "2025-09-11 10:58:55",
      "fixes_applied": [
        "TypeError in year sorting resolved",
        "Safe type conversion for year data",
        "Improved exception handling",
        "Colab optimization"
      ],
      "status": "Production Ready",
      "tested": true
    },
    "fixed_issues": [
      "TypeError in year sorting fixed",
      "Added type checking for year data",
      "Improved exception handling for year parsing",
      "Safe integer conversion for year values"
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12+"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}